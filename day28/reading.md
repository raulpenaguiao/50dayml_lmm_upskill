# Day 28: Training minGPT on Shakespeare - Further Reading

This day focuses on practical language model training using minGPT on the Shakespeare dataset, demonstrating how to achieve good generation quality through proper hyperparameter tuning and training practices.

## References

1. **The Unreasonable Effectiveness of RNNs**
   - [Karpathy: RNN Effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
   - Classic blog showing character-level language models can generate coherent text, providing context for similar GPT experiments.

2. **Perplexity in Language Modeling**
   - [Understanding Perplexity](https://towardsdatascience.com/perplexity-in-language-models-5aab550d6ab)
   - Explanation of perplexity metric for evaluating language models with intuitive examples.

3. **Sampling Techniques for Text Generation**
   - [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)
   - Paper analyzing temperature, top-k, and nucleus sampling for improving text generation quality.

4. **Shakespeare Dataset and Text Corpora**
   - [Project Gutenberg](https://www.gutenberg.org/)
   - Large collection of public domain texts including Shakespeare for training language models.

5. **Checkpoint Saving and Model Evaluation**
   - [PyTorch: Saving and Loading Models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
   - Best practices for checkpoint management during training and model selection.

---

**Tip:** Start with reference #2 for evaluation metrics, #3 for generation quality, #1 for motivation, and #5 for checkpoint best practices.
