# Day 46: Multimodal Learning with Vision and Language - Further Reading

This day covers multimodal models that process both text and images, combining vision transformers with language models for understanding and generation across modalities.

## References

1. **CLIP: Connecting Text and Images**
   - [Radford et al.: CLIP](https://arxiv.org/abs/2103.00020)
   - Influential paper on learning aligned text-image representations through contrastive learning.

2. **Vision Transformers (ViT)**
   - [Dosovitskiy et al.: Vision Transformer](https://arxiv.org/abs/2010.11929)
   - Paper applying transformer architecture directly to image patches for vision tasks.

3. **Multimodal Foundation Models**
   - [LLaVA: Large Language and Vision Assistant](https://llava-vl.github.io/)
   - Open-source multimodal model combining vision encoder with language model.

4. **Video Understanding with Transformers**
   - [TimeSformer: Is Space-Time Attention All You Need?](https://arxiv.org/abs/2102.05095)
   - Extending transformers to video understanding through space-time attention.

5. **Hugging Face Multimodal Models**
   - [HF: Vision & Language Models](https://huggingface.co/docs/transformers/tasks/image_task_guide)
   - Guide to using pretrained multimodal models with the Transformers library.

---

**Tip:** Start with reference #5 for practical use, #1 for CLIP concepts, #3 for full multimodal systems, and #2 for vision transformers.
