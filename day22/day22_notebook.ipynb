{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 22: Transformer Decoder and Full Architecture\n",
    "\n",
    "**Goal:** Build a complete Transformer decoder and assemble the full encoder-decoder architecture.\n",
    "\n",
    "**Time estimate:** 4-5 hours\n",
    "\n",
    "**What you'll learn:**\n",
    "- Masked self-attention (causal attention)\n",
    "- Cross-attention (encoder-decoder attention)\n",
    "- Complete decoder layer\n",
    "- Full Transformer assembly\n",
    "- Sequence-to-sequence generation\n",
    "\n",
    "**Topics build on:**\n",
    "- Day 21: Transformer encoder\n",
    "- Day 17: Multi-head attention\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Encoder Components from Day 21\n",
    "\n",
    "We'll reuse the encoder components we built yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy implementations from Day 21 for convenience\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"Layer normalization from Day 21.\"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward from Day 21.\"\"\"\n",
    "    def __init__(self, d_model, d_ff=2048, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = F.relu if activation == 'relu' else F.gelu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention from Day 21.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('scale', torch.tensor(self.d_k ** 0.5))\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"Generalized to support cross-attention.\n",
    "        Args:\n",
    "            Q, K, V: Query, Key, Value tensors (may come from different sources)\n",
    "            mask: Optional attention mask\n",
    "        \"\"\"\n",
    "        batch_size = Q.shape[0]\n",
    "        seq_len_q = Q.shape[1]\n",
    "        seq_len_kv = K.shape[1]\n",
    "        \n",
    "        # Project\n",
    "        Q = self.W_q(Q)  # (batch, seq_len_q, d_model)\n",
    "        K = self.W_k(K)  # (batch, seq_len_kv, d_model)\n",
    "        V = self.W_v(V)  # (batch, seq_len_kv, d_model)\n",
    "        \n",
    "        # Reshape for multi-head\n",
    "        Q = Q.view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len_kv, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len_kv, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply to values\n",
    "        attended = torch.matmul(attn_weights, V)\n",
    "        attended = attended.transpose(1, 2).contiguous()\n",
    "        attended = attended.view(batch_size, seq_len_q, self.d_model)\n",
    "        \n",
    "        output = self.W_o(attended)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding from Day 21.\"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        positions = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        dimensions = torch.arange(d_model).unsqueeze(0)\n",
    "        angle_rates = 1 / (10000 ** (2 * (dimensions // 2) / d_model))\n",
    "        angle_rads = positions * angle_rates\n",
    "        angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        self.register_buffer('pe', angle_rads.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "print(\"✓ All encoder components imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Attention Masks\n",
    "\n",
    "### Causal Mask (Look-Ahead Mask)\n",
    "\n",
    "In autoregressive generation, we can't look at future tokens:\n",
    "\n",
    "```\n",
    "At position i, we can attend to: [0, 1, ..., i]\n",
    "We CANNOT attend to: [i+1, i+2, ..., seq_len-1]\n",
    "```\n",
    "\n",
    "Causal mask:\n",
    "$$\\text{mask}[i,j] = \\begin{cases}\n",
    "1 & \\text{if } j \\leq i \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Visualized as lower triangular matrix with 1s below diagonal, 0s above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len, device):\n",
    "    \"\"\"\n",
    "    Create causal (look-ahead) mask for decoder self-attention.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of sequence\n",
    "        device: Device to create mask on\n",
    "    \n",
    "    Returns:\n",
    "        mask: Shape (1, 1, seq_len, seq_len)\n",
    "              1 where attention is allowed, 0 where masked\n",
    "    \"\"\"\n",
    "    # Create lower triangular matrix (1 in lower triangle, 0 in upper)\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    \n",
    "    # Add batch and head dimensions for broadcasting\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\ndef create_padding_mask(x, pad_idx=0):\n",
    "    \"\"\"\n",
    "    Create padding mask for padding tokens.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tokens (batch, seq_len)\n",
    "        pad_idx: Padding token index\n",
    "    \n",
    "    Returns:\n",
    "        mask: Shape (batch, 1, 1, seq_len)\n",
    "              1 where token is real, 0 where padding\n",
    "    \"\"\"\n",
    "    mask = (x != pad_idx).unsqueeze(1).unsqueeze(2).float()\n",
    "    return mask\n",
    "\n",
    "\ndef combine_masks(causal_mask, padding_mask):\n",
    "    \"\"\"\n",
    "    Combine causal and padding masks.\n",
    "    \n",
    "    Args:\n",
    "        causal_mask: (1, 1, seq_len, seq_len) - lower triangular\n",
    "        padding_mask: (batch, 1, 1, seq_len) - 0s for padding\n",
    "    \n",
    "    Returns:\n",
    "        combined_mask: (batch, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Both must be 1 for attention to be allowed\n",
    "    return causal_mask * padding_mask\n",
    "\n",
    "\nprint(\"✓ Mask functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and visualize masks\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len, device)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Causal mask\n",
    "ax = axes[0]\n",
    "causal_viz = causal_mask[0, 0].cpu().numpy()\n",
    "im = ax.imshow(causal_viz, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "ax.set_title('Causal Mask\\n(Can only attend to past)', fontweight='bold', fontsize=11)\n",
    "ax.set_xlabel('Attend to (Key)', fontweight='bold')\n",
    "ax.set_ylabel('Attend from (Query)', fontweight='bold')\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "plt.colorbar(im, ax=ax, label='Allow attention')\n",
    "\n",
    "# Padding mask (example with 2 padding tokens at end)\n",
    "ax = axes[1]\n",
    "x_example = torch.tensor([[1, 2, 3, 4, 5, 0, 0, 0]])  # 0 is padding\n",
    "padding_mask = create_padding_mask(x_example, pad_idx=0)\n",
    "padding_viz = padding_mask[0, 0, 0].cpu().numpy()\n",
    "padding_viz_2d = np.tile(padding_viz, (seq_len, 1))\n",
    "im = ax.imshow(padding_viz_2d, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "ax.set_title('Padding Mask\\n(Skip padding tokens)', fontweight='bold', fontsize=11)\n",
    "ax.set_xlabel('Token position', fontweight='bold')\n",
    "ax.set_ylabel('All queries', fontweight='bold')\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks([])\n",
    "plt.colorbar(im, ax=ax, label='Is real token')\n",
    "\n",
    "# Combined mask\n",
    "ax = axes[2]\n",
    "combined = combine_masks(causal_mask, padding_mask)\n",
    "combined_viz = combined[0, 0].cpu().numpy()\n",
    "im = ax.imshow(combined_viz, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "ax.set_title('Combined Mask\\n(Causal + Padding)', fontweight='bold', fontsize=11)\n",
    "ax.set_xlabel('Attend to (Key)', fontweight='bold')\n",
    "ax.set_ylabel('Attend from (Query)', fontweight='bold')\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "plt.colorbar(im, ax=ax, label='Allow attention')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMask interpretation:\")\n",
    "print(\"- Green (1): Attention allowed\")\n",
    "print(\"- Red (0): Attention masked out\")\n",
    "print(\"- Lower triangular: Causal (can't look ahead)\")\n",
    "print(\"- Padding columns zeroed: Skip padding tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Decoder Layer\n",
    "\n",
    "### Decoder Layer Architecture\n",
    "\n",
    "A decoder layer has **3 sub-layers**:\n",
    "1. **Masked multi-head self-attention** (attends to previous decoder outputs)\n",
    "2. **Cross-attention** (attends to encoder output)\n",
    "3. **Feed-forward network** (position-wise)\n",
    "\n",
    "Each with residual connections and layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer decoder layer.\n",
    "    \n",
    "    Three sub-layers:\n",
    "    1. Masked self-attention (can't look ahead)\n",
    "    2. Cross-attention (to encoder output)\n",
    "    3. Feed-forward network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout rate\n",
    "            activation: Activation function ('relu' or 'gelu')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Three attention mechanisms\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, activation)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "        self.norm3 = LayerNormalization(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch, tgt_seq_len, d_model)\n",
    "            encoder_output: Encoder output (batch, src_seq_len, d_model)\n",
    "            self_attn_mask: Mask for self-attention (causal + padding)\n",
    "            cross_attn_mask: Mask for cross-attention (encoder padding)\n",
    "        \n",
    "        Returns:\n",
    "            output: Decoded representation\n",
    "            self_attn_weights: Self-attention weights\n",
    "            cross_attn_weights: Cross-attention weights\n",
    "        \"\"\"\n",
    "        # 1. Masked self-attention (can't look at future tokens)\n",
    "        x_norm = self.norm1(x)\n",
    "        self_attn_out, self_attn_weights = self.self_attention(\n",
    "            x_norm, x_norm, x_norm, mask=self_attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(self_attn_out)\n",
    "        \n",
    "        # 2. Cross-attention (attend to encoder output)\n",
    "        # Query from decoder, Key/Value from encoder\n",
    "        x_norm = self.norm2(x)\n",
    "        cross_attn_out, cross_attn_weights = self.cross_attention(\n",
    "            x_norm, encoder_output, encoder_output, mask=cross_attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(cross_attn_out)\n",
    "        \n",
    "        # 3. Feed-forward\n",
    "        x_norm = self.norm3(x)\n",
    "        ffn_out = self.ffn(x_norm)\n",
    "        x = x + self.dropout(ffn_out)\n",
    "        \n",
    "        return x, self_attn_weights, cross_attn_weights\n",
    "\n",
    "\n",
    "print(\"✓ Decoder layer implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "src_seq_len = 15\n",
    "tgt_seq_len = 12\n",
    "batch_size = 4\n",
    "\n",
    "# Create decoder layer\n",
    "decoder_layer = DecoderLayer(d_model, num_heads, d_ff, dropout=0.1)\n",
    "\n",
    "# Test data\n",
    "decoder_input = torch.randn(batch_size, tgt_seq_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_seq_len, d_model)\n",
    "\n",
    "# Create masks\n",
    "causal_mask = create_causal_mask(tgt_seq_len, device)\n",
    "tgt_padding_mask = torch.ones(batch_size, 1, 1, tgt_seq_len, device=device)\n",
    "self_attn_mask = combine_masks(causal_mask, tgt_padding_mask)\n",
    "\n",
    "src_padding_mask = torch.ones(batch_size, 1, 1, src_seq_len, device=device)\n",
    "cross_attn_mask = src_padding_mask\n",
    "\n",
    "# Forward pass\n",
    "output, self_attn_weights, cross_attn_weights = decoder_layer(\n",
    "    decoder_input, encoder_output,\n",
    "    self_attn_mask=self_attn_mask,\n",
    "    cross_attn_mask=cross_attn_mask\n",
    ")\n",
    "\n",
    "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Self-attention weights shape: {self_attn_weights.shape}\")\n",
    "print(f\"Cross-attention weights shape: {cross_attn_weights.shape}\")\n",
    "print(f\"\\nShapes correct:\")\n",
    "print(f\"  Output matches input: {output.shape == decoder_input.shape} ✓\")\n",
    "print(f\"  Self-attn: (batch, heads, tgt_len, tgt_len) ✓\")\n",
    "print(f\"  Cross-attn: (batch, heads, tgt_len, src_len) ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Complete Transformer Decoder"
   ]
  },
  {
   "cell_type": "code">
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer decoder.\n",
    "    \n",
    "    Stacks multiple decoder layers with embeddings and positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff=2048,\n",
    "                 max_seq_len=1000, dropout=0.1, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of target vocabulary\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of decoder layers\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            max_seq_len: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "            activation: Activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Stack of decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = LayerNormalization(d_model)\n",
    "        \n",
    "        # Output projection to vocabulary\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, cross_attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Target token IDs (batch, tgt_seq_len)\n",
    "            encoder_output: Encoder output (batch, src_seq_len, d_model)\n",
    "            self_attn_mask: Mask for decoder self-attention\n",
    "            cross_attn_mask: Mask for cross-attention\n",
    "        \n",
    "        Returns:\n",
    "            logits: Output logits (batch, tgt_seq_len, vocab_size)\n",
    "            all_attn: List of attention weights from all layers\n",
    "        \"\"\"\n",
    "        # Embedding and position encoding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        all_self_attn = []\n",
    "        all_cross_attn = []\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            x, self_attn, cross_attn = layer(\n",
    "                x, encoder_output,\n",
    "                self_attn_mask=self_attn_mask,\n",
    "                cross_attn_mask=cross_attn_mask\n",
    "            )\n",
    "            all_self_attn.append(self_attn)\n",
    "            all_cross_attn.append(cross_attn)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(x)  # (batch, tgt_seq_len, vocab_size)\n",
    "        \n",
    "        return logits, all_self_attn, all_cross_attn\n",
    "\n",
    "\n",
    "print(\"✓ Transformer decoder implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Complete Transformer (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer with encoder and decoder.\n",
    "    \n",
    "    Used for sequence-to-sequence tasks:\n",
    "    - Machine translation\n",
    "    - Summarization\n",
    "    - Q&A\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, \n",
    "                 num_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 d_ff=2048, max_seq_len=1000, dropout=0.1, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab_size: Source language vocabulary size\n",
    "            tgt_vocab_size: Target language vocabulary size\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_encoder_layers: Number of encoder layers\n",
    "            num_decoder_layers: Number of decoder layers\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            max_seq_len: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "            activation: Activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Encoder\n",
    "        from day21_components import TransformerEncoder  # Reuse from Day 21\n",
    "        self.encoder = self._build_encoder(\n",
    "            src_vocab_size, d_model, num_heads, num_encoder_layers, d_ff,\n",
    "            max_seq_len, dropout, activation\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = TransformerDecoder(\n",
    "            tgt_vocab_size, d_model, num_heads, num_decoder_layers,\n",
    "            d_ff, max_seq_len, dropout, activation\n",
    "        )\n",
    "    \n",
    "    def _build_encoder(self, src_vocab_size, d_model, num_heads, num_layers,\n",
    "                       d_ff, max_seq_len, dropout, activation):\n",
    "        \"\"\"Build encoder from components.\"\"\"\n",
    "        encoder = nn.ModuleDict()\n",
    "        encoder['embedding'] = nn.Embedding(src_vocab_size, d_model)\n",
    "        encoder['pos_encoding'] = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Encoder layers\n",
    "        encoder_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            encoder_layers.append(\n",
    "                self._build_encoder_layer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            )\n",
    "        encoder['layers'] = encoder_layers\n",
    "        encoder['final_norm'] = LayerNormalization(d_model)\n",
    "        encoder['dropout'] = nn.Dropout(dropout)\n",
    "        \n",
    "        return encoder\n",
    "    \n",
    "    def _build_encoder_layer(self, d_model, num_heads, d_ff, dropout, activation):\n",
    "        \"\"\"Build single encoder layer.\"\"\"\n",
    "        from day21_components import EncoderLayer\n",
    "        return EncoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"\n",
    "        Encode source sequence.\n",
    "        \n",
    "        Args:\n",
    "            src: Source token IDs (batch, src_seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            encoder_output: (batch, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        enc = self.encoder\n",
    "        x = enc['embedding'](src) * np.sqrt(self.d_model)\n",
    "        x = enc['pos_encoding'](x)\n",
    "        x = enc['dropout'](x)\n",
    "        \n",
    "        for layer in enc['layers']:\n",
    "            x, _ = layer(x)\n",
    "        \n",
    "        x = enc['final_norm'](x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        \"\"\"\n",
    "        Decode target sequence given encoder output.\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target token IDs (batch, tgt_seq_len)\n",
    "            encoder_output: Encoder output (batch, src_seq_len, d_model)\n",
    "            tgt_mask: Causal mask for target\n",
    "            src_mask: Padding mask for source\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        logits, _, _ = self.decoder(tgt, encoder_output, tgt_mask, src_mask)\n",
    "        return logits\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through encoder-decoder.\n",
    "        \n",
    "        Args:\n",
    "            src: Source tokens (batch, src_seq_len)\n",
    "            tgt: Target tokens (batch, tgt_seq_len)\n",
    "            src_mask: Encoder mask\n",
    "            tgt_mask: Decoder mask\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        encoder_output = self.encode(src)\n",
    "        logits = self.decode(tgt, encoder_output, tgt_mask, src_mask)\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"✓ Complete Transformer implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Simpler Implementation (Without External Dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified complete Transformer.\n",
    "    \n",
    "    All components defined together for clarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256,\n",
    "                 num_heads=4, num_layers=2, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Encoder\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.encoder_pos_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            self._make_encoder_layer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.encoder_norm = LayerNormalization(d_model)\n",
    "        \n",
    "        # Decoder\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.decoder_pos_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            self._make_decoder_layer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_norm = LayerNormalization(d_model)\n",
    "        \n",
    "        # Output\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _make_encoder_layer(self, d_model, num_heads, d_ff, dropout):\n",
    "        return nn.ModuleDict({\n",
    "            'attention': MultiHeadAttention(d_model, num_heads, dropout),\n",
    "            'ffn': FeedForwardNetwork(d_model, d_ff),\n",
    "            'norm1': LayerNormalization(d_model),\n",
    "            'norm2': LayerNormalization(d_model),\n",
    "            'dropout': nn.Dropout(dropout),\n",
    "        })\n",
    "    \n",
    "    def _make_decoder_layer(self, d_model, num_heads, d_ff, dropout):\n",
    "        return nn.ModuleDict({\n",
    "            'self_attention': MultiHeadAttention(d_model, num_heads, dropout),\n",
    "            'cross_attention': MultiHeadAttention(d_model, num_heads, dropout),\n",
    "            'ffn': FeedForwardNetwork(d_model, d_ff),\n",
    "            'norm1': LayerNormalization(d_model),\n",
    "            'norm2': LayerNormalization(d_model),\n",
    "            'norm3': LayerNormalization(d_model),\n",
    "            'dropout': nn.Dropout(dropout),\n",
    "        })\n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"Encode source.\"\"\"\n",
    "        x = self.src_embedding(src) * np.sqrt(self.d_model)\n",
    "        x = self.encoder_pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.encoder_layers:\n",
    "            # Self-attention\n",
    "            x_norm = layer['norm1'](x)\n",
    "            attn_out, _ = layer['attention'](x_norm, x_norm, x_norm)\n",
    "            x = x + layer['dropout'](attn_out)\n",
    "            \n",
    "            # FFN\n",
    "            x_norm = layer['norm2'](x)\n",
    "            ffn_out = layer['ffn'](x_norm)\n",
    "            x = x + layer['dropout'](ffn_out)\n",
    "        \n",
    "        return self.encoder_norm(x)\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        \"\"\"Decode target given encoder output.\"\"\"\n",
    "        x = self.tgt_embedding(tgt) * np.sqrt(self.d_model)\n",
    "        x = self.decoder_pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            # Self-attention (masked)\n",
    "            x_norm = layer['norm1'](x)\n",
    "            self_attn_out, _ = layer['self_attention'](\n",
    "                x_norm, x_norm, x_norm, mask=tgt_mask\n",
    "            )\n",
    "            x = x + layer['dropout'](self_attn_out)\n",
    "            \n",
    "            # Cross-attention\n",
    "            x_norm = layer['norm2'](x)\n",
    "            cross_attn_out, _ = layer['cross_attention'](\n",
    "                x_norm, encoder_output, encoder_output, mask=src_mask\n",
    "            )\n",
    "            x = x + layer['dropout'](cross_attn_out)\n",
    "            \n",
    "            # FFN\n",
    "            x_norm = layer['norm3'](x)\n",
    "            ffn_out = layer['ffn'](x_norm)\n",
    "            x = x + layer['dropout'](ffn_out)\n",
    "        \n",
    "        x = self.decoder_norm(x)\n",
    "        return self.output_projection(x)\n",
    "    \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_mask=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        encoder_output = self.encode(src)\n",
    "        logits = self.decode(tgt, encoder_output, tgt_mask, src_mask)\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"✓ SimpleTransformer implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Complete Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1200\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "src_seq_len = 20\n",
    "tgt_seq_len = 18\n",
    "batch_size = 8\n",
    "\n",
    "# Create model\n",
    "transformer = SimpleTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Transformer created:\")\n",
    "print(f\"  Source vocab: {src_vocab_size}\")\n",
    "print(f\"  Target vocab: {tgt_vocab_size}\")\n",
    "print(f\"  Model dim: {d_model}\")\n",
    "print(f\"  Heads: {num_heads}\")\n",
    "print(f\"  Layers: {num_layers}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len)).to(device)\n",
    "tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len)).to(device)\n",
    "\n",
    "print(f\"Source shape: {src.shape}\")\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "\n",
    "# Create masks\n",
    "tgt_mask = create_causal_mask(tgt_seq_len, device)\n",
    "tgt_padding_mask = torch.ones(batch_size, 1, 1, tgt_seq_len, device=device)\n",
    "tgt_mask = combine_masks(tgt_mask, tgt_padding_mask)\n",
    "\n",
    "src_padding_mask = torch.ones(batch_size, 1, 1, src_seq_len, device=device)\n",
    "\n",
    "# Forward pass\n",
    "logits = transformer(src, tgt, tgt_mask=tgt_mask, src_mask=src_padding_mask)\n",
    "\n",
    "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
    "print(f\"Expected: (batch={batch_size}, tgt_seq_len={tgt_seq_len}, tgt_vocab={tgt_vocab_size})\")\n",
    "print(f\"\\n✓ Forward pass successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualization and Analysis\n",
    "\n",
    "### Visualize Cross-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example for visualization\n",
    "# Source: \"The cat sat\" → Target: \"El gato se sentó\" (Spanish)\n",
    "src_vocab = {0: '<pad>', 1: 'The', 2: 'cat', 3: 'sat'}\n",
    "tgt_vocab = {0: '<pad>', 1: 'El', 2: 'gato', 3: 'se', 4: 'sentó'}\n",
    "\n",
    "src_example = torch.tensor([[1, 2, 3]], dtype=torch.long)  # \"The cat sat\"\n",
    "tgt_example = torch.tensor([[1, 2, 3, 4]], dtype=torch.long)  # \"El gato se sentó\"\n",
    "\n",
    "src_words = ['The', 'cat', 'sat']\n",
    "tgt_words = ['El', 'gato', 'se', 'sentó']\n",
    "\n",
    "# Create small transformer\n",
    "small_transformer = SimpleTransformer(\n",
    "    src_vocab_size=5, tgt_vocab_size=6,\n",
    "    d_model=64, num_heads=4, num_layers=1,\n",
    "    d_ff=256, dropout=0.1\n",
    ")\n",
    "\n",
    "# Get encoder output\n",
    "with torch.no_grad():\n",
    "    encoder_output = small_transformer.encode(src_example)\n",
    "\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"\\nNote: In a real model, cross-attention learns alignment.\")\n",
    "print(f\"Example alignments:\")\n",
    "print(f\"  'El' → 'The' (article alignment)\")\n",
    "print(f\"  'gato' → 'cat' (noun alignment)\")\n",
    "print(f\"  'se sentó' → 'sat' (verb alignment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Key Concepts Summary\n",
    "\n",
    "### Decoder vs Encoder\n",
    "\n",
    "| Aspect | Encoder | Decoder |\n",
    "|--------|---------|----------|\n",
    "| Self-attention | Unrestricted | **Masked (causal)** |\n",
    "| Cross-attention | None | Attends to encoder |\n",
    "| Can look ahead | ✓ Yes | ✗ No |\n",
    "| Task | Understand input | Generate output |\n",
    "\n",
    "### Causal Masking (Look-Ahead Mask)\n",
    "\n",
    "At position `i`, model can only attend to positions `[0...i]`:\n",
    "- Prevents information leak from future tokens\n",
    "- Crucial for autoregressive generation\n",
    "- Implemented as lower triangular mask\n",
    "\n",
    "### Cross-Attention\n",
    "\n",
    "Query from decoder, Key/Value from encoder:\n",
    "- Decoder queries \"what information do I need?\"\n",
    "- Encoder keys \"what information can I provide?\"\n",
    "- Learns alignment between source and target\n",
    "\n",
    "### Information Flow\n",
    "\n",
    "```\n",
    "Source tokens\n",
    "    ↓\n",
    "Encoder Stack\n",
    "    ↓\n",
    "Encoder Output (context)\n",
    "    ↓ (cross-attention)\n",
    "Decoder Stack ← Target tokens (self-attention)\n",
    "    ↓\n",
    "Target token probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Decoding Strategies\n",
    "\n",
    "### Greedy Decoding\n",
    "\n",
    "Generate one token at a time:\n",
    "```python\n",
    "output = [<start>]\n",
    "while output[-1] != <end>:\n",
    "    logits = model(src, output)\n",
    "    next_token = argmax(logits[-1])\n",
    "    output.append(next_token)\n",
    "```\n",
    "\n",
    "### Beam Search\n",
    "\n",
    "Keep top-k hypotheses, expand all, keep top-k again.\n",
    "\n",
    "### Sampling\n",
    "\n",
    "Sample from probability distribution (temperature-controlled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, max_len, start_idx, end_idx, device):\n",
    "    \"\"\"\n",
    "    Generate sequence greedily, token by token.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained transformer\n",
    "        src: Source sequence (1, src_len)\n",
    "        max_len: Maximum generation length\n",
    "        start_idx: Start token index\n",
    "        end_idx: End token index\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        output: Generated sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get encoder output (once, reuse for all decoder steps)\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.encode(src)\n",
    "    \n",
    "    # Start with <start> token\n",
    "    output = torch.tensor([[start_idx]], device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1):\n",
    "            # Create causal mask for current output length\n",
    "            tgt_len = output.size(1)\n",
    "            tgt_mask = create_causal_mask(tgt_len, device)\n",
    "            \n",
    "            # Decode\n",
    "            logits = model.decode(output, encoder_output, tgt_mask=tgt_mask)\n",
    "            \n",
    "            # Get logits for next token (last position)\n",
    "            next_logits = logits[0, -1, :]  # (vocab_size,)\n",
    "            \n",
    "            # Greedy: select argmax\n",
    "            next_token = next_logits.argmax().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # Append to output\n",
    "            output = torch.cat([output, next_token], dim=1)\n",
    "            \n",
    "            # Stop if <end> token\n",
    "            if next_token.item() == end_idx:\n",
    "                break\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"✓ Greedy decoding function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Training Transformer Models\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Cross-entropy loss between predicted and target distributions.\n",
    "\n",
    "### Key Training Techniques\n",
    "\n",
    "1. **Warmup:** Linear increase of learning rate for first steps\n",
    "2. **Label smoothing:** Smooth target distribution (helps generalization)\n",
    "3. **Gradient clipping:** Clip gradients to prevent explosion\n",
    "4. **Learning rate schedule:** Cosine annealing or step decay\n",
    "5. **Dropout:** 0.1-0.3 typical\n",
    "6. **Weight initialization:** Xavier/He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Encoder-Only vs Decoder-Only vs Full\n",
    "\n",
    "### Encoder-Only (BERT, RoBERTa)\n",
    "- Bidirectional context\n",
    "- No masked self-attention\n",
    "- Good for classification, tagging\n",
    "- Can't generate text directly\n",
    "\n",
    "### Decoder-Only (GPT, Llama)\n",
    "- Causal self-attention only\n",
    "- No cross-attention\n",
    "- Can generate text\n",
    "- Unified model for all tasks\n",
    "\n",
    "### Encoder-Decoder (T5, mBART)\n",
    "- Best of both worlds\n",
    "- Encoder processes input fully\n",
    "- Decoder generates output\n",
    "- Best for seq2seq tasks\n",
    "\n",
    "**Modern trend:** Decoder-only models (GPT) dominate because:\n",
    "- Unified architecture\n",
    "- Can handle any task\n",
    "- Scales better\n",
    "- In-context learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Exercises\n",
    "\n",
    "### Basic\n",
    "1. **Test different sequence lengths:** How does performance change with longer sequences?\n",
    "2. **Visualize attention:** Plot cross-attention patterns for word alignment\n",
    "3. **Mask exploration:** Test what happens without causal mask\n",
    "\n",
    "### Intermediate\n",
    "1. **Implement beam search:** Keep top-k hypotheses\n",
    "2. **Add label smoothing:** Smooth target distributions\n",
    "3. **Learning rate warmup:** Implement warmup schedule\n",
    "4. **Gradient analysis:** Check gradient flow through layers\n",
    "\n",
    "### Advanced\n",
    "1. **Train on real translation:** Use small parallel corpus\n",
    "2. **Implement BLEU score:** Evaluate translation quality\n",
    "3. **Optimize attention:** Use sparse or linear attention\n",
    "4. **Multi-head analysis:** Understand what different heads learn\n",
    "5. **Positional bias:** Try learned positions instead of sinusoidal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Coming Up\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "✅ Complete Transformer decoder from scratch\n",
    "✅ Full encoder-decoder Transformer\n",
    "✅ Causal masking for autoregressive generation\n",
    "✅ Cross-attention for seq2seq\n",
    "✅ Greedy decoding\n",
    "\n",
    "### Day 23: Tokenization\n",
    "- Byte-Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- SentencePiece\n",
    "- Subword tokenization\n",
    "\n",
    "### Days 24-30: GPT and Fine-Tuning\n",
    "- Day 24: GPT architecture\n",
    "- Day 25: Training GPT from scratch\n",
    "- Day 26: BERT and masked language modeling\n",
    "- Day 27-30: minGPT implementation\n",
    "\n",
    "### Why Transformers Matter\n",
    "\n",
    "The Transformer architecture is **the foundation of modern NLP**:\n",
    "- All major language models use it\n",
    "- Extended to vision (Vision Transformers)\n",
    "- Extended to multimodal (CLIP, DALL-E)\n",
    "- Simple yet powerful\n",
    "- Highly parallelizable\n",
    "\n",
    "Understanding transformers deeply is crucial for working in modern AI!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_name_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
