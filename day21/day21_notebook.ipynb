{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 21: Transformer Encoder Architecture\n",
    "\n",
    "**Goal:** Build a complete Transformer encoder from scratch, understanding all components.\n",
    "\n",
    "**Time estimate:** 4-5 hours\n",
    "\n",
    "**What you'll learn:**\n",
    "- Layer normalization and why it's crucial\n",
    "- Encoder layer architecture\n",
    "- Feed-forward networks in Transformers\n",
    "- Residual connections and their role\n",
    "- Stacking encoder layers\n",
    "- Complete encoder with embeddings\n",
    "\n",
    "**Topics build on:**\n",
    "- Day 17: Self-attention and multi-head attention\n",
    "- Day 19-20: Language modeling and generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Layer Normalization\n",
    "\n",
    "### Why Layer Normalization?\n",
    "\n",
    "Batch normalization normalizes across the batch dimension:\n",
    "$$\\text{BatchNorm}(x) = \\gamma \\odot \\frac{x - \\mathbb{E}_{batch}[x]}{\\sqrt{\\text{Var}_{batch}[x] + \\epsilon}} + \\beta$$\n",
    "\n",
    "**Problem for Transformers:**\n",
    "- Batch size matters (affects statistics)\n",
    "- RNNs: batch_size=1 at inference time (batch norm breaks)\n",
    "- Variable sequence lengths (batch norm expects fixed sizes)\n",
    "\n",
    "Layer normalization normalizes across the feature dimension:\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mathbb{E}_{features}[x]}{\\sqrt{\\text{Var}_{features}[x] + \\epsilon}} + \\beta$$\n",
    "\n",
    "**Advantages:**\n",
    "✅ Independent of batch size\n",
    "✅ Works with variable sequence lengths\n",
    "✅ No train/test discrepancy\n",
    "✅ Better for Transformers (empirically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization: normalize features independently of batch.\n",
    "    \n",
    "    Normalizes across feature dimension (last dimension).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of features to normalize\n",
    "            eps: Small value for numerical stability\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Learnable scale (gamma) and shift (beta)\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (..., d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Normalized tensor same shape as input\n",
    "        \"\"\"\n",
    "        # Compute mean and variance across feature dimension\n",
    "        # For (..., d_model): compute mean/var along last dimension\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # (..., 1)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # (..., 1)\n",
    "        \n",
    "        # Normalize\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)  # (..., d_model)\n",
    "        \n",
    "        # Scale and shift\n",
    "        return self.gamma * x_norm + self.beta\n",
    "\n",
    "\n",
    "print(\"✓ Layer normalization implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "batch_size = 4\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input mean (before norm): {x.mean(dim=-1)[0, 0].item():.4f}\")\n",
    "print(f\"Input std (before norm): {x.std(dim=-1)[0, 0].item():.4f}\")\n",
    "\n",
    "# Apply layer norm\n",
    "ln = LayerNormalization(d_model)\n",
    "x_norm = ln(x)\n",
    "\n",
    "print(f\"\\nAfter layer norm:\")\n",
    "print(f\"Output shape: {x_norm.shape}\")\n",
    "print(f\"Output mean (after norm): {x_norm.mean(dim=-1)[0, 0].item():.4f}\")\n",
    "print(f\"Output std (after norm): {x_norm.std(dim=-1)[0, 0].item():.4f}\")\n",
    "print(f\"\\nNote: Mean ≈ 0, Std ≈ 1 ✓\")\n",
    "\n",
    "# Compare with PyTorch's LayerNorm\n",
    "pytorch_ln = nn.LayerNorm(d_model)\n",
    "pytorch_output = pytorch_ln(x)\n",
    "\n",
    "print(f\"\\nComparison with PyTorch LayerNorm:\")\n",
    "max_diff = (x_norm - pytorch_output).abs().max().item()\n",
    "print(f\"Max difference: {max_diff:.6f}\")\n",
    "print(f\"Outputs match: {max_diff < 1e-5} ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feed-Forward Network\n",
    "\n",
    "### Position-wise Feed-Forward Network (FFN)\n",
    "\n",
    "Applied to each position independently:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Or with GELU activation:\n",
    "$$\\text{FFN}(x) = \\text{GELU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "**Key properties:**\n",
    "- Applied to each token independently\n",
    "- Shared parameters across positions\n",
    "- Expansion: $d_{model} \\to d_{ff} \\to d_{model}$ (typically $d_{ff} = 4 \\times d_{model}$)\n",
    "- Adds non-linearity and capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network.\n",
    "    \n",
    "    Two linear layers with activation in between.\n",
    "    Applied independently to each position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=2048, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Input/output dimension\n",
    "            d_ff: Hidden layer dimension (typically 4 * d_model)\n",
    "            activation: 'relu' or 'gelu'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'gelu':\n",
    "            self.activation = F.gelu\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (..., d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor same shape as input\n",
    "        \"\"\"\n",
    "        # First linear layer + activation: (..., d_model) -> (..., d_ff)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Second linear layer: (..., d_ff) -> (..., d_model)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"✓ Feed-forward network implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "d_ff = 256\n",
    "\n",
    "ffn = FeedForwardNetwork(d_model, d_ff, activation='relu')\n",
    "\n",
    "# Test data\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Shapes match: {output.shape == x.shape} ✓\")\n",
    "\n",
    "# Count parameters\n",
    "params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"\\nFFN parameters: {params:,}\")\n",
    "print(f\"  Linear1: {d_model} × {d_ff} = {d_model * d_ff:,}\")\n",
    "print(f\"  Linear2: {d_ff} × {d_model} = {d_ff * d_model:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Residual Connections\n",
    "\n",
    "### Add & Norm Pattern\n",
    "\n",
    "Residual connections (skip connections) are crucial:\n",
    "$$y = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "Or pre-norm variant:\n",
    "$$y = x + \\text{Sublayer}(\\text{LayerNorm}(x))$$\n",
    "\n",
    "**Benefits:**\n",
    "✅ Gradient flow: Can skip over deep layers\n",
    "✅ Identity shortcut: If sublayer learns little, output ≈ input\n",
    "✅ Training stability: Easier to train very deep networks\n",
    "✅ Feature preservation: Input features propagate forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual connection with layer normalization.\n",
    "    \n",
    "    Implements: output = LayerNorm(input + Sublayer(input))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, sublayer, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension\n",
    "            sublayer: The sublayer to apply (attention or FFN)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(d_model)\n",
    "        self.sublayer = sublayer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (..., d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor same shape\n",
    "        \"\"\"\n",
    "        # Apply sublayer, add residual, apply dropout\n",
    "        return self.norm(x + self.dropout(self.sublayer(x)))\n",
    "\n",
    "\n",
    "print(\"✓ Residual connection implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Head Self-Attention (Review)\n",
    "\n",
    "Let's reimplement the multi-head attention for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('scale', torch.tensor(self.d_k ** 0.5))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch, seq_len, d_model)\n",
    "            mask: Optional mask for attention\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Project to Q, K, V: (batch, seq_len, d_model)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape for multi-head: (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (batch, num_heads, seq_len, seq_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = torch.nan_to_num(attn_weights, 0.0)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended = torch.matmul(attn_weights, V)  # (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended = attended.transpose(1, 2).contiguous()  # (batch, seq_len, num_heads, d_k)\n",
    "        attended = attended.view(batch_size, seq_len, d_model)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(attended)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "print(\"✓ Multi-head attention implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Encoder Layer\n",
    "\n",
    "### Complete Encoder Layer\n",
    "\n",
    "Combines:\n",
    "1. Multi-head self-attention\n",
    "2. Add & Norm\n",
    "3. Feed-forward network\n",
    "4. Add & Norm\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input (batch, seq_len, d_model)\n",
    "  ↓\n",
    "Layer Norm\n",
    "  ↓\n",
    "Multi-Head Attention\n",
    "  ↓\n",
    "Dropout\n",
    "  ↓\n",
    "Add (residual) → [intermediate]\n",
    "  ↓\n",
    "Layer Norm\n",
    "  ↓\n",
    "Feed-Forward\n",
    "  ↓\n",
    "Dropout\n",
    "  ↓\n",
    "Add (residual) → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer encoder layer.\n",
    "    \n",
    "    Combines:\n",
    "    - Multi-head self-attention\n",
    "    - Feed-forward network\n",
    "    - Layer normalization and residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            dropout: Dropout rate\n",
    "            activation: Activation function ('relu' or 'gelu')\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, activation)\n",
    "        \n",
    "        # Layer normalization (pre-norm variant)\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Encoded representation (batch, seq_len, d_model)\n",
    "            attn_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output, attn_weights = self.attention(x_norm, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "print(\"✓ Encoder layer implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "d_ff = 1024\n",
    "seq_len = 20\n",
    "batch_size = 4\n",
    "\n",
    "# Create encoder layer\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff, dropout=0.1)\n",
    "\n",
    "# Test data\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = encoder_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nShapes correct: {output.shape == x.shape} ✓\")\n",
    "\n",
    "# Count parameters\n",
    "params = sum(p.numel() for p in encoder_layer.parameters())\n",
    "print(f\"\\nEncoder layer parameters: {params:,}\")\n",
    "\n",
    "# Breakdown\n",
    "attn_params = sum(p.numel() for p in encoder_layer.attention.parameters())\n",
    "ffn_params = sum(p.numel() for p in encoder_layer.ffn.parameters())\n",
    "norm_params = sum(p.numel() for p in encoder_layer.norm1.parameters()) + \\\n",
    "              sum(p.numel() for p in encoder_layer.norm2.parameters())\n",
    "\n",
    "print(f\"  Attention: {attn_params:,}\")\n",
    "print(f\"  FFN: {ffn_params:,}\")\n",
    "print(f\"  Normalization: {norm_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Positional Encodings (Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(max_seq_len).unsqueeze(1)  # (max_seq_len, 1)\n",
    "        dimensions = torch.arange(d_model).unsqueeze(0)  # (1, d_model)\n",
    "        \n",
    "        # Compute angle rates\n",
    "        angle_rates = 1 / (10000 ** (2 * (dimensions // 2) / d_model))\n",
    "        angle_rads = positions * angle_rates  # (max_seq_len, d_model)\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd\n",
    "        angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        # Register as buffer\n",
    "        self.register_buffer('pe', angle_rads.unsqueeze(0))  # (1, max_seq_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x + positional encoding\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "print(\"✓ Positional encoding implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Complete Transformer Encoder\n",
    "\n",
    "### Stack Multiple Encoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer encoder.\n",
    "    \n",
    "    Stacks multiple encoder layers with embeddings and positional encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff=2048,\n",
    "                 max_seq_len=1000, dropout=0.1, activation='relu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of encoder layers\n",
    "            d_ff: Feed-forward hidden dimension\n",
    "            max_seq_len: Maximum sequence length\n",
    "            dropout: Dropout rate\n",
    "            activation: Activation function\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout, activation)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.final_norm = LayerNormalization(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token IDs (batch, seq_len)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Encoded representations (batch, seq_len, d_model)\n",
    "            all_attn_weights: Attention weights from all layers\n",
    "        \"\"\"\n",
    "        # Embedding and position encoding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)  # Scale embeddings\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        all_attn_weights = []\n",
    "        for layer in self.encoder_layers:\n",
    "            x, attn_weights = layer(x, mask)\n",
    "            all_attn_weights.append(attn_weights)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        return x, all_attn_weights\n",
    "\n",
    "\n",
    "print(\"✓ Transformer encoder implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Complete Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "vocab_size = 5000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "d_ff = 1024\n",
    "seq_len = 20\n",
    "batch_size = 8\n",
    "\n",
    "# Create encoder\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Encoder architecture:\")\n",
    "print(encoder)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "x = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Sample input (token IDs): {x[0]}\")\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = encoder(x)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Number of layers: {len(attn_weights)}\")\n",
    "print(f\"Attention weights per layer shape: {attn_weights[0].shape}\")\n",
    "print(f\"  (batch={batch_size}, num_heads={num_heads}, seq_len={seq_len}, seq_len={seq_len})\")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"\\n✓ Output shape correct: {output.shape == (batch_size, seq_len, d_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Visualization and Analysis\n",
    "\n",
    "### Analyze Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example for visualization\n",
    "# Simple sentence: \"The cat sat on the mat\"\n",
    "vocab = {\n",
    "    '<pad>': 0, 'the': 1, 'cat': 2, 'sat': 3,\n",
    "    'on': 4, 'mat': 5\n",
    "}\n",
    "tokens = torch.tensor([[1, 2, 3, 4, 1, 5]], dtype=torch.long)  # \"The cat sat on the mat\"\n",
    "word_labels = ['The', 'cat', 'sat', 'on', 'the', 'mat']\n",
    "\n",
    "# Create small encoder for visualization\n",
    "small_encoder = TransformerEncoder(\n",
    "    vocab_size=6,\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_ff=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Get attention weights\n",
    "output, attn_weights_list = small_encoder(tokens)\n",
    "\n",
    "print(f\"Encoder output shape: {output.shape}\")\n",
    "print(f\"Number of layers: {len(attn_weights_list)}\")\n",
    "\n",
    "# Visualize attention from first layer, first head\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Layer 1\n",
    "layer_1_attn = attn_weights_list[0][0].detach()  # (num_heads, seq_len, seq_len)\n",
    "head_0_attn = layer_1_attn[0].cpu().numpy()  # First head\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(head_0_attn, cmap='viridis', aspect='auto')\n",
    "ax.set_xticks(range(len(word_labels)))\n",
    "ax.set_yticks(range(len(word_labels)))\n",
    "ax.set_xticklabels(word_labels, rotation=45, ha='right')\n",
    "ax.set_yticklabels(word_labels)\n",
    "ax.set_title('Layer 1, Head 1 Attention\\n(What each word attends to)', fontweight='bold')\n",
    "ax.set_xlabel('Key (attend to)', fontweight='bold')\n",
    "ax.set_ylabel('Query (attend from)', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "\n",
    "# Layer 2\n",
    "layer_2_attn = attn_weights_list[1][0].detach()\n",
    "head_0_attn = layer_2_attn[0].cpu().numpy()\n",
    "\n",
    "ax = axes[1]\n",
    "im = ax.imshow(head_0_attn, cmap='viridis', aspect='auto')\n",
    "ax.set_xticks(range(len(word_labels)))\n",
    "ax.set_yticks(range(len(word_labels)))\n",
    "ax.set_xticklabels(word_labels, rotation=45, ha='right')\n",
    "ax.set_yticklabels(word_labels)\n",
    "ax.set_title('Layer 2, Head 1 Attention\\n(Refined attention after first layer)', fontweight='bold')\n",
    "ax.set_xlabel('Key (attend to)', fontweight='bold')\n",
    "ax.set_ylabel('Query (attend from)', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Diagonal: All words attend to themselves (expected)\")\n",
    "print(\"- 'The' attends to article and nouns\")\n",
    "print(\"- 'sat' (verb) attends to subject and object\")\n",
    "print(\"- Layer 2 shows refined, task-specific patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize All Heads in a Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all 4 heads from layer 1\n",
    "layer_1_attn = attn_weights_list[0][0].detach().cpu().numpy()  # (4, 6, 6)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    head_attn = layer_1_attn[head_idx]\n",
    "    \n",
    "    im = ax.imshow(head_attn, cmap='Blues', aspect='auto')\n",
    "    ax.set_xticks(range(len(word_labels)))\n",
    "    ax.set_yticks(range(len(word_labels)))\n",
    "    ax.set_xticklabels(word_labels, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(word_labels, fontsize=9)\n",
    "    ax.set_title(f'Head {head_idx + 1}', fontweight='bold')\n",
    "    \n",
    "    if head_idx % 2 == 0:\n",
    "        ax.set_ylabel('Query', fontweight='bold')\n",
    "    if head_idx >= 2:\n",
    "        ax.set_xlabel('Key', fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "plt.suptitle('Layer 1: All Attention Heads', fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight:\")\n",
    "print(\"Each head learns different attention patterns:\")\n",
    "print(\"- Some heads might focus on adjacent words\")\n",
    "print(\"- Others might focus on syntactic roles\")\n",
    "print(\"- Multi-head attention = ensemble of different perspectives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Architectural Variants\n",
    "\n",
    "### Pre-Norm vs Post-Norm\n",
    "\n",
    "**Post-Norm (Original, what we implemented):**\n",
    "$$y = \\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
    "\n",
    "**Pre-Norm (More stable):**\n",
    "$$y = x + \\text{Sublayer}(\\text{LayerNorm}(x))$$\n",
    "\n",
    "Pre-norm is increasingly popular - better gradient flow, more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayerPreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer with pre-normalization (more stable).\n    \n",
    "    Apply normalization before sublayers instead of after.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1, activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, activation)\n",
    "        \n",
    "        # Layer norms come BEFORE sublayers (pre-norm)\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply norm FIRST, then sublayer\n",
    "        attn_output, attn_weights = self.attention(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(self.norm2(x))\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "print(\"✓ Pre-norm encoder layer implemented\")\n",
    "print(\"\\nComparison:\")\n",
    "print(\"Post-Norm: Better generalization, original paper\")\n",
    "print(\"Pre-Norm:  Better gradient flow, more stable training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Concepts Summary\n",
    "\n",
    "### Layer Normalization\n",
    "- Normalizes features **across feature dimension**\n",
    "- Independent of batch size (better for Transformers)\n",
    "- Learnable scale and shift parameters\n",
    "\n",
    "### Feed-Forward Network\n",
    "- Position-wise: Applied independently to each token\n",
    "- Two-layer MLP: $d_{model} \\to d_{ff} \\to d_{model}$\n",
    "- Typical expansion: $d_{ff} = 4 \\times d_{model}$\n",
    "- Adds non-linearity and capacity\n",
    "\n",
    "### Residual Connections\n",
    "- $y = x + \\text{Sublayer}(x)$ (skip connection)\n",
    "- Crucial for training deep networks\n",
    "- Preserves input features\n",
    "- Improves gradient flow\n",
    "\n",
    "### Encoder Layer Architecture\n",
    "- Multi-head attention\n",
    "- Residual connection\n",
    "- Feed-forward network\n",
    "- Residual connection\n",
    "- Total: ~6 × d_model²/num_heads parameters\n",
    "\n",
    "### Multi-Head Attention\n",
    "- $h$ parallel attention mechanisms\n",
    "- Each head: dimension $d_model / h$\n",
    "- Concatenate outputs\n",
    "- Learn different relationship types\n",
    "\n",
    "### Information Flow in Encoder\n",
    "1. **Embedding & Position Encoding:** Add position information\n",
    "2. **Layer 1:** Immediate context, local patterns\n",
    "3. **Layer 2:** Larger context, higher-order patterns\n",
    "4. **Layer N:** Global context, semantic information\n",
    "\n",
    "Deeper layers see more of the sequence through attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Practical Considerations\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "For a single encoder layer:\n",
    "- Multi-head attention: **O(seq_len² × d_model)**\n",
    "- Feed-forward: **O(seq_len × d_ff × d_model)**\n",
    "- Total: **O(seq_len² × d_model + seq_len × d_ff × d_model)**\n",
    "\n",
    "With $d_{ff} = 4 \\times d_{model}$, dominant term is **O(seq_len² × d_model)**\n",
    "\n",
    "### Memory Complexity\n",
    "\n",
    "- Attention weights: O(seq_len²) - can be bottleneck for long sequences\n",
    "- Activation checkpointing: Trade computation for memory\n",
    "- Sparse attention: Only attend to subset of positions\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "1. **Gradient clipping:** Important for stability\n",
    "2. **Warmup:** Linear warmup of learning rate helps\n",
    "3. **Weight initialization:** Xavier/He initialization standard\n",
    "4. **Dropout:** 0.1-0.2 typical\n",
    "5. **Learning rate:** Usually 1e-4 to 1e-3\n",
    "6. **Label smoothing:** 0.1 helps with generalization\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "- **Unstable training:** Check gradient clipping, learning rate\n",
    "- **Poor convergence:** Try warmup, adjust learning rate schedule\n",
    "- **Overfitting:** Increase dropout, add regularization\n",
    "- **Memory issues:** Use gradient checkpointing, reduce sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Exercises\n",
    "\n",
    "### Basic\n",
    "1. **Modify hidden dimension:** Try d_ff = 2×d_model vs 8×d_model. How does it affect performance?\n",
    "2. **Change activation:** Replace ReLU with GELU. What's the difference?\n",
    "3. **Analyze layer outputs:** Print norms of outputs at each layer.\n",
    "\n",
    "### Intermediate\n",
    "1. **Implement pre-norm variant:** Compare training stability with post-norm\n",
    "2. **Add layer-wise visualization:** Track how representations change across layers\n",
    "3. **Gradient analysis:** Check gradients at each layer during backprop\n",
    "4. **Residual importance:** Remove residual connections, see what breaks\n",
    "\n",
    "### Advanced\n",
    "1. **Efficient attention:** Implement sparse or linear attention\n",
    "2. **Relative position bias:** Replace absolute positions with relative\n",
    "3. **Adaptive computation:** Skip layers for easy examples\n",
    "4. **Layer sharing:** Share weights across encoder layers (ALBERT style)\n",
    "5. **Profile computation:** Measure FLOPs and memory usage by component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Coming Up\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "✅ Complete Transformer encoder from scratch\n",
    "✅ All component implementations (attention, FFN, norm)\n",
    "✅ Understanding of architectural design choices\n",
    "✅ Visualization and analysis tools\n",
    "\n",
    "### Day 22: Transformer Decoder\n",
    "- Masked self-attention (causal attention)\n",
    "- Cross-attention (encoder-decoder attention)\n",
    "- Sequence generation\n",
    "- Complete Seq2Seq Transformer\n",
    "\n",
    "### Days 23-30: Complete Transformer-Based Models\n",
    "- Day 23: Tokenization strategies (BPE, WordPiece)\n",
    "- Day 24-25: GPT architecture and training\n",
    "- Day 26: BERT and bidirectional models\n",
    "- Day 27-30: minGPT implementation and training\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Understanding encoders is crucial because:\n",
    "- **BERT and RoBERTa:** Encoder-only models\n",
    "- **GPT and Llama:** Decoder-only, but use encoder knowledge\n",
    "- **T5 and mBART:** Full encoder-decoder\n",
    "- **Vision Transformers:** Use encoder architecture\n",
    "- **Multimodal models:** Often have encoder-decoder structure\n",
    "\n",
    "The encoder is the **fundamental building block** of modern deep learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_name_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
