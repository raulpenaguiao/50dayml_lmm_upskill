# Day 26: BERT and Masked Language Modeling - Further Reading

This day covers BERT (Bidirectional Encoder Representations from Transformers), which uses masked language modeling to learn powerful bidirectional text representations through self-supervised learning.

## References

1. **BERT: Pre-training of Deep Bidirectional Transformers**
   - [Devlin et al.: BERT Paper](https://arxiv.org/abs/1810.04805)
   - Seminal paper introducing BERT with masked language modeling (MLM) and next sentence prediction (NSP) pretraining objectives.

2. **How BERT Works - Visual Explanation**
   - [Jay Alammar: BERT Explained](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
   - Clear visual guide to BERT architecture, pretraining, and fine-tuning with practical examples.

3. **RoBERTa: A Robustly Optimized BERT Pretraining Approach**
   - [Liu et al.: RoBERTa](https://arxiv.org/abs/1907.11692)
   - Paper improving upon BERT's pretraining with better hyperparameters, dynamic masking, and more training data.

4. **Hugging Face BERT Fine-tuning**
   - [Hugging Face: BERT Fine-tuning Guide](https://huggingface.co/docs/transformers/training)
   - Practical guide to using pretrained BERT models and fine-tuning them on downstream NLP tasks.

5. **Understanding BERT's Attention**
   - [Attention is Not Only a Weight](https://www.aclweb.org/anthology/D19-1143.pdf)
   - Analysis of what BERT's attention patterns learn and how to interpret them for understanding model decisions.

---

**Tip:** Start with reference #2 for visual understanding, #1 for theory, #3 for improvements, and #4 for practical implementation.
