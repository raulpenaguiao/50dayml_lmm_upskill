{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 18: Text Classification with Deep Learning\n",
    "\n",
    "**Goal:** Build practical text classification systems using various architectures (CNN, RNN, Attention, Transformers).\n",
    "\n",
    "**Time estimate:** 4-5 hours\n",
    "\n",
    "**What you'll learn:**\n",
    "- CNN for text (1D convolutions)\n",
    "- LSTM/GRU classifiers with attention\n",
    "- Transformer-based classification\n",
    "- Comparing different architectures\n",
    "- Error analysis and debugging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Classification Problem\n",
    "\n",
    "### Classic Problem: Sentiment Analysis\n",
    "\n",
    "**Input:** Text (e.g., movie review)  \n",
    "**Output:** Label (positive, negative, or neutral)\n",
    "\n",
    "Example:\n",
    "```\n",
    "\"This movie was absolutely incredible! Best film I've ever seen.\"\n",
    "→ [Positive]\n",
    "\n",
    "\"Terrible waste of time. Boring plot and bad acting.\"\n",
    "→ [Negative]\n",
    "```\n",
    "\n",
    "### Other Text Classification Tasks\n",
    "\n",
    "- **Intent detection:** \"Book a flight\" → [BOOKING]\n",
    "- **Topic classification:** Article → [SPORTS/POLITICS/TECH]\n",
    "- **Spam detection:** Email → [SPAM/NOT_SPAM]\n",
    "- **Language identification:** Text → [EN/FR/DE]\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Text → [Tokenize & Embed] → [Process] → [Classify] → Label\n",
    "                                ↓\n",
    "                    CNN / RNN / Transformer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset Preparation\n",
    "\n",
    "### Load and Preprocess Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple sentiment analysis dataset\n",
    "# In practice, you'd use AG News, IMDb, SST, etc.\n",
    "\n",
    "sample_data = [\n",
    "    (\"This movie is absolutely fantastic! Best film I've seen all year.\", 1),  # Positive\n",
    "    (\"Terrible waste of time. Boring and predictable plot.\", 0),  # Negative\n",
    "    (\"Pretty good movie, some interesting scenes.\", 1),  # Positive\n",
    "    (\"Not worth watching. Poor acting and weak storyline.\", 0),  # Negative\n",
    "    (\"Outstanding performance! Highly recommend to everyone.\", 1),  # Positive\n",
    "    (\"Disappointing. I expected much better from this director.\", 0),  # Negative\n",
    "    (\"Great cinematography and excellent soundtrack!\", 1),  # Positive\n",
    "    (\"Awful. One of the worst films I've ever seen.\", 0),  # Negative\n",
    "    (\"Decent movie, good entertainment value.\", 1),  # Positive\n",
    "    (\"Hated it. Complete disaster of a film.\", 0),  # Negative\n",
    "    (\"Amazing story and brilliant actors!\", 1),  # Positive\n",
    "    (\"Absolutely dreadful. Save your money.\", 0),  # Negative\n",
    "    (\"Loved every minute of it!\", 1),  # Positive\n",
    "    (\"Couldn't finish watching. Too bad.\", 0),  # Negative\n",
    "    (\"Wonderful experience at the cinema!\", 1),  # Positive\n",
    "    (\"Regret watching this movie so much.\", 0),  # Negative\n",
    "]\n",
    "\n",
    "texts, labels = zip(*sample_data)\n",
    "print(f\"Dataset size: {len(texts)} samples\")\n",
    "print(f\"Class distribution: {Counter(labels)}\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample samples:\")\n",
    "for text, label in sample_data[:3]:\n",
    "    label_str = \"Positive\" if label == 1 else \"Negative\"\n",
    "    print(f\"  [{label_str}] {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Build and manage vocabulary for text data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=1):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}  # Special tokens\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.word_freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from texts.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of tokenized sentences\n",
    "        \"\"\"\n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                self.word_freq[word] += 1\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        idx = 2\n",
    "        for word, freq in self.word_freq.most_common():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Convert tokens to indices.\n",
    "        \n",
    "        Args:\n",
    "            text: List of tokens\n",
    "        \n",
    "        Returns:\n",
    "            List of indices\n",
    "        \"\"\"\n",
    "        return [self.word2idx.get(word, self.word2idx['<UNK>']) for word in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"\n",
    "        Convert indices back to tokens.\n",
    "        \"\"\"\n",
    "        return [self.idx2word.get(idx, '<UNK>') for idx in indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\ndef preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Basic text preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "    \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s!?.]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n# Preprocess all texts\ntokenized_texts = [preprocess_text(text) for text in texts]\n\n# Build vocabulary\nvocab = Vocabulary(min_freq=1)\nvocab.build(tokenized_texts)\n\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(f\"\\nSample tokenization:\")\nfor text, tokens in zip(texts[:2], tokenized_texts[:2]):\n",
    "    print(f\"  Original: {text}\")\n",
    "    print(f\"  Tokens: {tokens}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for text classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, vocab, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess text\n",
    "        text = preprocess_text(self.texts[idx])\n",
    "        \n",
    "        # Encode to indices\n",
    "        indices = self.vocab.encode(text)\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indices) < self.max_len:\n",
    "            indices = indices + [0] * (self.max_len - len(indices))  # Pad\n",
    "        else:\n",
    "            indices = indices[:self.max_len]  # Truncate\n",
    "        \n",
    "        return {\n",
    "            'text': torch.tensor(indices, dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n# Create dataset and split\ndataset = TextDataset(texts, labels, vocab, max_len=64)\n",
    "\n# 60-20-20 split\ntrain_size = int(0.6 * len(dataset))\nval_size = int(0.2 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\n\ntrain_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\nprint(f\"Train set: {len(train_dataset)} samples\")\n",
    "print(f\"Val set: {len(val_dataset)} samples\")\n",
    "print(f\"Test set: {len(test_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders\nbatch_size = 4\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"\\nDataloader ready (batch_size={batch_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: CNN for Text Classification\n",
    "\n",
    "### Why CNN for Text?\n",
    "\n",
    "1D convolutions detect **local n-gram patterns**\n",
    "```\n",
    "\"This movie is absolutely fantastic\"\n",
    "     ↓ Conv3, Conv4, Conv5\n",
    "  [absolutely fantastic] (2-word phrase)\n",
    "  [movie is absolutely] (3-word phrase)\n",
    "```\n",
    "\n",
    "- **Filter size = n-gram size** (3, 4, 5)\n",
    "- **Multiple filters per size** detect different patterns\n",
    "- **Max pooling** selects most important feature\n",
    "\n",
    "### Architecture (Kim 2014)\n",
    "\n",
    "```\n",
    "Input (seq_len × d_embed)\n",
    "    ↓\n",
    "Conv1d filters: [3, 4, 5]\n",
    "    ↓\n",
    "ReLU activation\n",
    "    ↓\n",
    "Max pooling over time\n",
    "    ↓\n",
    "Concatenate pooled features\n",
    "    ↓\n",
    "Dropout\n",
    "    ↓\n",
    "Linear → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based text classifier (Kim 2014 architecture).\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        embedding_dim: Word embedding dimension\n",
    "        filter_sizes: Sizes of convolution filters (e.g., [3, 4, 5])\n",
    "        num_filters: Number of filters per size\n",
    "        num_classes: Number of output classes\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, filter_sizes, \n",
    "                 num_filters, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolution layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # Linear layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits, shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Embed: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Permute for Conv1d: (batch_size, embedding_dim, seq_len)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply convolutions and pooling\n",
    "        # Each conv outputs: (batch_size, num_filters, seq_len - kernel_size + 1)\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            # Convolution\n",
    "            conv_out = F.relu(conv(embedded))\n",
    "            # Max pooling over time\n",
    "            pooled = F.max_pool1d(conv_out, conv_out.shape[2])\n",
    "            # Remove time dimension: (batch_size, num_filters)\n",
    "            pooled = pooled.squeeze(2)\n",
    "            conv_outputs.append(pooled)\n",
    "        \n",
    "        # Concatenate: (batch_size, num_filters * len(filter_sizes))\n",
    "        concatenated = torch.cat(conv_outputs, dim=1)\n",
    "        \n",
    "        # Dropout\n",
    "        dropped = self.dropout(concatenated)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fc(dropped)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"✓ CNN classifier implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RNN Classifier with Attention\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input\n",
    "  ↓\n",
    "Embedding\n",
    "  ↓\n",
    "LSTM/GRU (bidirectional)\n",
    "  ↓\n",
    "Attention (weighted sum)\n",
    "  ↓\n",
    "Dropout\n",
    "  ↓\n",
    "Linear → Output\n",
    "```\n",
    "\n",
    "### Attention Mechanism for Classification\n",
    "\n",
    "Instead of using final hidden state, use **weighted combination** of all states:\n",
    "\n",
    "$$\\text{context} = \\sum_i \\alpha_i h_i$$\n",
    "\n",
    "where $\\alpha_i$ are learned attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple attention mechanism.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, lstm_output, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lstm_output: (batch_size, seq_len, hidden_dim)\n",
    "            mask: Optional mask for padding\n",
    "        \n",
    "        Returns:\n",
    "            context: (batch_size, hidden_dim)\n",
    "            weights: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # Compute attention scores: (batch_size, seq_len, 1)\n",
    "        scores = self.attention(lstm_output)\n",
    "        scores = scores.squeeze(-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        weights = torch.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
    "        weights = torch.nan_to_num(weights, 0.0)\n",
    "        \n",
    "        # Weighted sum: (batch_size, seq_len, hidden_dim)\n",
    "        # Expand weights: (batch_size, seq_len, 1)\n",
    "        context = torch.sum(\n",
    "            lstm_output * weights.unsqueeze(-1),\n",
    "            dim=1\n",
    "        )  # (batch_size, hidden_dim)\n",
    "        \n",
    "        return context, weights\n",
    "\n",
    "\nclass RNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN-based text classifier with attention.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Vocabulary size\n",
    "        embedding_dim: Word embedding dimension\n",
    "        hidden_dim: RNN hidden dimension\n",
    "        num_classes: Number of output classes\n",
    "        num_layers: Number of RNN layers\n",
    "        bidirectional: Use bidirectional RNN\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 num_classes, num_layers=1, bidirectional=True, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "            bidirectional=bidirectional, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # RNN output dimension\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        self.attention = Attention(rnn_output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(rnn_output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits, shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Embed: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # RNN: (batch_size, seq_len, hidden_dim * num_directions)\n",
    "        rnn_output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        # Attention\n",
    "        context, attn_weights = self.attention(rnn_output)\n",
    "        \n",
    "        # Dropout\n",
    "        context = self.dropout(context)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fc(context)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"✓ RNN classifier with attention implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Transformer Classifier\n",
    "\n",
    "### Using Transformer for Classification\n",
    "\n",
    "```\n",
    "Input → Embedding → Positional Encoding\n",
    "           ↓\n",
    "        Transformer Blocks\n",
    "           ↓\n",
    "        [CLS] Token pooling\n",
    "           ↓\n",
    "        Linear → Output\n",
    "```\n",
    "\n",
    "Strategy: Use [CLS] token from Day 17, or pool all representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse components from Day 17\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        positions = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        dimensions = torch.arange(d_model).unsqueeze(0)\n",
    "        angle_rates = 1 / (10000 ** (2 * (dimensions // 2) / d_model))\n",
    "        angle_rads = positions * angle_rates\n",
    "        angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "        self.register_buffer('pe', angle_rads.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\nclass TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based text classifier.\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Vocabulary size\n",
    "        embedding_dim: Word embedding dimension\n",
    "        num_heads: Number of attention heads\n",
    "        num_layers: Number of transformer blocks\n",
    "        d_ff: Feed-forward hidden dimension\n",
    "        num_classes: Number of output classes\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, num_layers,\n",
    "                 d_ff, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_dim)\n",
    "        \n",
    "        # Simplified transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Classification logits, shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.pos_encoding(embedded)\n",
    "        \n",
    "        # Transformer\n",
    "        # Create padding mask\n",
    "        src_key_padding_mask = (x == 0)  # Mask padding tokens\n",
    "        transformer_output = self.transformer(\n",
    "            embedded, src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Pool: Use mean of all tokens\n",
    "        # Create attention mask for pooling (ignore padding)\n",
    "        lengths = (x != 0).sum(dim=1, keepdim=True).float()  # (batch_size, 1)\n",
    "        pooled = transformer_output.sum(dim=1) / lengths.clamp(min=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fc(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"✓ Transformer classifier implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training and Evaluation\n",
    "\n",
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        texts = batch['text'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(texts)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(data_loader), correct / total\n",
    "\n",
    "\ndef evaluate(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate on validation/test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            texts = batch['text'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(texts)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return total_loss / len(data_loader), accuracy, all_preds, all_labels\n",
    "\n",
    "\ndef train_model(model, train_loader, val_loader, num_epochs, learning_rate, device, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Complete training pipeline.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n",
    "    return history\n",
    "\n",
    "print(\"✓ Training functions implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\nembedding_dim = 100\nnum_epochs = 30\nlearning_rate = 0.001\nnum_classes = 2\n\n# Initialize models\nprint(\"Initializing models...\\n\")\n",
    "\n# CNN Classifier\ncnn_model = CNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    num_filters=50,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "print(f\"CNN parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "\n# RNN Classifier\nrnn_model = RNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=64,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=1,\n",
    "    bidirectional=True\n",
    ").to(device)\n",
    "print(f\"RNN parameters: {sum(p.numel() for p in rnn_model.parameters()):,}\")\n",
    "\n# Transformer Classifier\ntransformer_model = TransformerClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    d_ff=256,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "print(f\"Transformer parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training CNN Classifier\")\nprint(\"=\"*80)\ncnn_history = train_model(\n",
    "    cnn_model, train_loader, val_loader, num_epochs, learning_rate, device, \"cnn\"\n",
    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training RNN Classifier\")\nprint(\"=\"*80)\nrnn_history = train_model(\n",
    "    rnn_model, train_loader, val_loader, num_epochs, learning_rate, device, \"rnn\"\n",
    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transformer\nprint(\"\\n\" + \"=\"*80)\nprint(\"Training Transformer Classifier\")\nprint(\"=\"*80)\ntransformer_history = train_model(\n",
    "    transformer_model, train_loader, val_loader, num_epochs, learning_rate, device, \"transformer\"\n",
    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparison and Analysis\n",
    "\n",
    "### Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Validation Loss\n",
    "ax = axes[0]\n",
    "ax.plot(epochs, cnn_history['val_loss'], 'o-', label='CNN', linewidth=2)\n",
    "ax.plot(epochs, rnn_history['val_loss'], 's-', label='RNN', linewidth=2)\n",
    "ax.plot(epochs, transformer_history['val_loss'], '^-', label='Transformer', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Validation Loss Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(epochs, cnn_history['val_acc'], 'o-', label='CNN', linewidth=2)\n",
    "ax.plot(epochs, rnn_history['val_acc'], 's-', label='RNN', linewidth=2)\n",
    "ax.plot(epochs, transformer_history['val_acc'], '^-', label='Transformer', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Validation Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate all models\nmodels = {\n",
    "    'CNN': cnn_model,\n",
    "    'RNN': rnn_model,\n",
    "    'Transformer': transformer_model\n",
    "}\n",
    "\nresults = {}\n",
    "predictions = {}\n",
    "\nfor name, model in models.items():\n",
    "    test_loss, test_acc, preds, labels = evaluate(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    results[name] = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'precision': precision_score(labels, preds, zero_division=0),\n",
    "        'recall': recall_score(labels, preds, zero_division=0),\n",
    "        'f1': f1_score(labels, preds, zero_division=0)\n",
    "    }\n",
    "    predictions[name] = (preds, labels)\n",
    "\n# Create results table\nresults_df = pd.DataFrame(results).T\nprint(\"\\nTest Set Results:\")\nprint(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    preds, labels = predictions[name]\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    ax.set_title(f'{name}\\n(Acc: {results[name][\"test_acc\"]:.3f})',\n",
    "                fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\nplt.suptitle('Confusion Matrices on Test Set', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDetailed Classification Report:\\n\")\n",
    "for name, model in models.items():\n",
    "    preds, labels = predictions[name]\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"{name} Classifier\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(classification_report(labels, preds, \n",
    "                               target_names=['Negative', 'Positive']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Error Analysis\n",
    "\n",
    "### Analyze Misclassified Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misclassified_examples(model, data_loader, device, vocab, tokenizer, num_examples=3):\n",
    "    \"\"\"\n",
    "    Get misclassified examples from test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    misclassified = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            texts = batch['text'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(texts)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            \n",
    "            # Find errors\n",
    "            errors = preds != labels\n",
    "            \n",
    "            for i in range(texts.size(0)):\n",
    "                if errors[i]:\n",
    "                    # Get probabilities\n",
    "                    probs = torch.softmax(logits[i], dim=0)\n",
    "                    \n",
    "                    misclassified.append({\n",
    "                        'text_indices': texts[i],\n",
    "                        'true_label': labels[i].item(),\n",
    "                        'pred_label': preds[i].item(),\n",
    "                        'confidence': probs[preds[i]].item()\n",
    "                    })\n",
    "    \n",
    "    # Sort by confidence\n",
    "    misclassified.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    return misclassified[:num_examples]\n",
    "\n",
    "\n# Analyze errors for one model\nprint(\"\\nMisclassified Examples (CNN):\")\nprint(\"=\"*80)\n",
    "\nmisclassified = get_misclassified_examples(cnn_model, test_loader, device, vocab, preprocess_text, num_examples=5)\n",
    "\nfor i, example in enumerate(misclassified):\n",
    "    # Decode\n",
    "    text_indices = example['text_indices'].cpu().numpy()\n",
    "    text = ' '.join([vocab.idx2word.get(idx, '<UNK>') for idx in text_indices if idx != 0])\n",
    "    \n",
    "    true_label = 'Positive' if example['true_label'] == 1 else 'Negative'\n",
    "    pred_label = 'Positive' if example['pred_label'] == 1 else 'Negative'\n",
    "    confidence = example['confidence']\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Text: {text}\")\n",
    "    print(f\"  True label: {true_label}\")\n",
    "    print(f\"  Predicted: {pred_label} (confidence: {confidence:.3f})\")\n",
    "    print(f\"  Analysis: {'✗ Wrong classification' if true_label != pred_label else '✓ Correct'}\")"
   ]
  },
  {
   "cell_type": {"execution_count": null, "metadata": {}, "outputs": [], "source_code": "## Part 9: Key Insights and Takeaways\n\n### Architecture Comparison\n\n**CNN:**\n- ✅ Fast, efficient\n- ✅ Good for local patterns (n-grams)\n- ❌ Limited long-range dependencies\n- Best for: Short, focused texts\n\n**RNN/LSTM:**\n- ✅ Captures sequential dependencies\n- ✅ Attention provides interpretability\n- ⚠️ Slower due to sequential nature\n- Best for: Longer documents, need for interpretability\n\n**Transformer:**\n- ✅ Parallelizable, fast\n- ✅ Strong long-range dependencies\n- ✅ Attention visualization\n- ⚠️ Quadratic memory with sequence length\n- Best for: Large datasets, GPUs available\n\n### When to Use Each\n\n- **CNN:** Fast prototyping, inference-heavy applications\n- **RNN:** Interpretability needed, medium-length sequences\n- **Transformer:** Best accuracy, large-scale training\n\n### Common Pitfalls\n\n1. **Class Imbalance:** Use weighted loss if classes imbalanced\n2. **Overfitting:** Use dropout, early stopping\n3. **Sequence Length:** Pad/truncate consistently\n4. **Vocabulary Coverage:** Handle unknown words with <UNK> token\n5. **Preprocessing:** Tokenization strategy matters\n\n---"}, "metadata": {}, "cell_type": "markdown"}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Exercises\n",
    "\n",
    "### Basic\n",
    "1. **Modify architecture:** Change embedding dimension, filter sizes, number of heads\n",
    "2. **Adjust preprocessing:** Try different tokenization strategies\n",
    "3. **Class weights:** Handle imbalanced data with weighted loss\n",
    "\n",
    "### Intermediate\n",
    "1. **Multi-class classification:** Extend to 3+ classes (positive/neutral/negative)\n",
    "2. **Hierarchical model:** Process sentences then documents\n",
    "3. **Ensemble methods:** Combine CNN+RNN+Transformer predictions\n",
    "\n",
    "### Advanced\n",
    "1. **Fine-tune BERT:** Use pretrained model from Hugging Face\n",
    "2. **Active learning:** Select uncertain examples for annotation\n",
    "3. **Interpretability:** Implement LIME/SHAP for explanations\n",
    "4. **Multi-task learning:** Jointly learn sentiment + emotion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Summary\n",
    "\n",
    "You've implemented three major text classification architectures and compared their performance:\n",
    "\n",
    "**CNN:**\n",
    "- 1D convolutions with multiple filter sizes\n",
    "- Max pooling over time\n",
    "- Fast and efficient\n",
    "\n",
    "**RNN with Attention:**\n",
    "- LSTM/GRU for sequence processing\n",
    "- Attention mechanism for weighted aggregation\n",
    "- Interpretable attention weights\n",
    "\n",
    "**Transformer:**\n",
    "- Multi-head self-attention\n",
    "- Positional encodings\n",
    "- Parallel processing\n",
    "\n",
    "**Next Steps:**\n",
    "- Day 19: Character-level models for text generation\n",
    "- Day 20: Project 2 - Build a complete RNN text generation system\n",
    "- Days 21+: Advanced transformers and LLMs\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
