{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 20: Project 2 - Character-Level RNN for Text Generation\n",
    "\n",
    "**Goal:** Build a portfolio-quality character-level text generation model.\n",
    "\n",
    "**Time estimate:** 4-5 hours\n",
    "\n",
    "**What you'll create:**\n",
    "- A trained character-level LSTM\n",
    "- Multiple generation strategies (temperature, sampling)\n",
    "- Comprehensive evaluation metrics\n",
    "- Professional visualizations and documentation\n",
    "\n",
    "**Deliverables:**\n",
    "- Working, trained model\n",
    "- Generation examples with analysis\n",
    "- Training documentation\n",
    "- Portfolio-ready code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "\n",
    "### Step 1: Download and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_shakespeare():\n",
    "    \"\"\"\n",
    "    Download Shakespeare dataset or use local fallback.\n",
    "    \"\"\"\n",
    "    filepath = \"shakespeare.txt\"\n",
    "    \n",
    "    # Try to download from Karpathy's repo\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        try:\n",
    "            print(f\"Downloading Shakespeare dataset...\")\n",
    "            urllib.request.urlretrieve(url, filepath)\n",
    "            print(f\"âœ“ Downloaded to {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            print(f\"Using sample data instead...\")\n",
    "            # Create small sample\n",
    "            sample = \"\"\"ROMEO AND JULIET\n",
    "\n",
    "ACT I\n",
    "\n",
    "SCENE I. A public place.\n",
    "\n",
    "Enter SAMPSON and GREGORY, of the house of Capulet, armed with swords and bucklers.\n",
    "\n",
    "SAMPSON: Gregory, on my word, we'll not carry coals.\n",
    "GREGORY: No, for then we should be colliers.\n",
    "SAMPSON: I mean, an we be in choler, we'll draw.\n",
    "GREGORY: Ay, while you live, draw your neck out of collar.\n",
    "SAMPSON: I strike quickly, being moved.\n",
    "GREGORY: But thou art not quickly moved to strike.\n",
    "SAMPSON: A dog of the house of Capulet moves me.\n",
    "GREGORY: To move is to stir; and to stir is to move; but here's the point--\n",
    "SAMPSON: Draw, if you dare. It is the law of our side if I say ay.\n",
    "GREGORY: Nay, I'll not stir for no man's pleasure, I.\n",
    "SAMPSON: This is a sorry sight.\n",
    "GREGORY: True, cousin.\n",
    "SAMPSON: Draw thy tool! here comes of the house of the Montagues.\n",
    "GREGORY: My naked weapon is out: quarrel, I will back thee.\n",
    "SAMPSON: How! turn thy back and run?\n",
    "GREGORY: Fear me not.\n",
    "SAMPSON: No, marry; I fear thee not.\n",
    "GREGORY: Let us take the law of our sides; let them begin.\n",
    "SAMPSON: I will frown as I pass by, and let them take it as they list.\n",
    "GREGORY: Nay, as they dare. I will bite my thumb at them; which is a disgrace to them, if they bear it.\n",
    "\n",
    "Enter, at a distance, ABRAHAM and BALTHASAR.\n",
    "\n",
    "SAMPSON: Do you bite your thumb at us, sir?\n",
    "ABRAHAM: Do you bite your thumb at us, sir?\n",
    "SAMPSON: Is the law of our side, if I say ay?\n",
    "GREGORY: No.\n",
    "SAMPSON: No, sir, I do not bite my thumb at you, sir; but I bite my thumb, sir.\n",
    "GREGORY: Do you quarrel, sir?\n",
    "ABRAHAM: Quarrel sir! no, sir.\n",
    "SAMPSON: If you do, sir, I am for you: I serve as good a man as yourself.\n",
    "ABRAHAM: No better.\n",
    "SAMPSON: Well, sir.\n",
    "\n",
    "Enter BENVOLIO.\n",
    "\n",
    "GREGORY: Say 'better': here comes one of my master's kinsmen.\n",
    "SAMPSON: Yes, better, sir.\n",
    "ABRAHAM: You lie.\n",
    "SAMPSON: Draw, if you dare.\n",
    "GREGORY: Nay, I am done.\n",
    "\n",
    "They fight; enter, on one side, BENVOLIO; on the other, TYBALT.\n",
    "\n",
    "BENVOLIO: Part, fools!\n",
    "Put up your swords; you know not what you do.\n",
    "\n",
    "TYBALT: What, drawn, and talk of peace! I hate the word,\n",
    "As I hate hell, all Montagues, and thee:\n",
    "Have at thee, coward!\n",
    "\n",
    "They fight.\n",
    "\n",
    "Enter, several of both houses, who join the fray; then enter Citizens, with clubs.\n",
    "\n",
    "First Citizen: Clubs, bills, and partisans! Strike! beat them down!\n",
    "Down with the Capulets! Down with the Montagues!\n",
    "\n",
    "Enter CAPULET in his gown, and LADY CAPULET.\n",
    "\n",
    "CAPULET: What noise is this? Give me my long sword, ho!\n",
    "LADY CAPULET: A crutch, a crutch! Why call you for a sword?\n",
    "\n",
    "Enter MONTAGUE and LADY MONTAGUE.\n",
    "\n",
    "MONTAGUE: Thou villain Capulet,--Hold me not, let me go.\n",
    "LADY MONTAGUE: Thou shalt not stir a foot to seek a foe.\n",
    "\n",
    "Enter PRINCE, with attendants.\n",
    "\n",
    "PRINCE: Rebellious subjects, enemies to peace,\n",
    "Profaners of this neighbour-stain'd steel,--\n",
    "Will they not hear? What, ho! you men, you beasts,\n",
    "That quench the fire of your pernicious rage\n",
    "With purple fountains issuing from your veins,\n",
    "On pain of torture, from those bloody hands\n",
    "Throw your mistemper'd weapons to the ground,\n",
    "And hear the sentence of your moved prince.\n",
    "Three civil brawls, bred of an airy word,\n",
    "By thee, old Capulet, and Montague,\n",
    "Have thrice disturb'd the quiet of our streets,\n",
    "And made Verona's ancient citizens\n",
    "Cast by their grave beseeming ornaments,\n",
    "To wield old partisans, in hands as old\n",
    "Curst now with age against the butt'ry peace,\n",
    "Of our ill-govern'd youth. Let me be bold\n",
    "Which old dissension, hot, against the peace.\n",
    "BENVOLIO: And yet I do not let you know:\n",
    "I have been in your service and have not\n",
    "Done any thing at any time against your grace.\n",
    "MONTAGUE: See what a scourge is laid upon your hate,\n",
    "That heaven finds means to kill your joys with love.\n",
    "And I for winking at your discords too\n",
    "Have lost a brace of kinsmen: all are paid.\n",
    "\n",
    "PRINCE: Where are the vile beginners of this fray?\n",
    "BENVOLIO: O noble prince, I can discover all\n",
    "The unlucky manage of this fatal brawl:\n",
    "There lies the man, slain by young Romeo,\n",
    "That slew thy kinsman, brave Mercutio.\n",
    "LADY CAPULET: Tybalt, my cousin! O my liege, I beg\n",
    "For justice, which thou, prince, must give; Romeo\n",
    "Slew Tybalt; Romeo must not live.\n",
    "PRINCE: And for that offence\n",
    "Immediately we do exile him hence:\n",
    "I have an interest in your hate's proceeding,\n",
    "My blood for your rude brawls doth lie a-bleeding;\n",
    "But I'll amerce you with so strong a fine\n",
    "That you shall all repent the loss of mine:\n",
    "I will be deaf to pleading and excuses;\n",
    "Nor tears nor prayers shall purchase out abuses:\n",
    "Therefore use none: let Romeo hence in haste,\n",
    "Else, when he's found, that hour is his last.\n",
    "Bear hence this body and attend our will:\n",
    "Mercy but murders, pardoning those that kill.\n",
    "\n",
    "Exeunt.\n",
    "\"\"\"\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                f.write(sample * 20)  # Repeat to make it bigger\n",
    "            print(f\"âœ“ Created sample {filepath}\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# Download dataset\n",
    "data_path = download_shakespeare()\n",
    "\n",
    "# Load text\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"  Total characters: {len(text):,}\")\n",
    "print(f\"  Unique characters: {len(set(text))}\")\n",
    "print(f\"\\nFirst 200 characters:\")\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Character Vocabulary and Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"\\nCharacters (as list):\")\n",
    "print(repr(''.join(chars)))\n",
    "\n",
    "# Encode entire text\n",
    "data = torch.tensor([char_to_idx[char] for char in text], dtype=torch.long)\n",
    "print(f\"\\nEncoded data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Character-level language modeling dataset.\n",
    "    Creates sequences of fixed length where target is next character.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, seq_len):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Tensor of character indices\n",
    "            seq_len: Sequence length (context window)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + self.seq_len]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Configuration\n",
    "seq_len = 64\n",
    "batch_size = 64\n",
    "\n",
    "# Create train/val split (90/10)\n",
    "split_idx = int(0.9 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "val_data = data[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CharacterLMDataset(train_data, seq_len)\n",
    "val_dataset = CharacterLMDataset(val_data, seq_len)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} sequences\")\n",
    "print(f\"Val dataset: {len(val_dataset):,} sequences\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Train batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Example batch\n",
    "x_sample, y_sample = next(iter(train_loader))\n",
    "print(f\"\\nExample batch:\")\n",
    "print(f\"  Input shape: {x_sample.shape}\")\n",
    "print(f\"  Target shape: {y_sample.shape}\")\n",
    "print(f\"  First sequence: {''.join([idx_to_char[i.item()] for i in x_sample[0]])}\")\n",
    "print(f\"  First target: '{idx_to_char[y_sample[0].item()]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer LSTM for character-level language modeling.\n",
    "    \n",
    "    Architecture:\n",
    "    Input (batch, seq_len) â†’ Embedding â†’ LSTM â†’ Linear â†’ Output (batch, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=512, \n",
    "                 num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Predictions (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embedding: (batch, seq_len, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # LSTM: (batch, seq_len, hidden_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Output: (batch, seq_len, vocab_size)\n",
    "        logits = self.fc(lstm_out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = CharacterLevelLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training Loop\n",
    "\n",
    "### Define Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # Compute loss (use all predictions, not just last)\n",
    "        loss = criterion(logits.reshape(-1, model.vocab_size), y.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (important for RNNs to prevent exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation set.\n",
    "    \n",
    "    Returns:\n",
    "        loss: Cross-entropy loss\n",
    "        perplexity: exp(loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.reshape(-1, model.vocab_size), y.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "print(\"âœ“ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "patience = 5  # Early stopping\n",
    "\n",
    "# Initialize optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "perplexities = []\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Epoch':<8} {'Train Loss':<15} {'Val Loss':<15} {'Perplexity':<15} {'LR':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, perplexity = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"{epoch+1:<8} {train_loss:<15.4f} {val_loss:<15.4f} {perplexity:<15.4f} {current_lr:<10.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'perplexity': perplexity,\n",
    "        }, 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "end_time = datetime.now()\n",
    "training_time = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"  Total time: {training_time:.1f} minutes\")\n",
    "print(f\"  Final validation perplexity: {perplexities[-1]:.2f}\")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"  Loaded best model from epoch {checkpoint['epoch'] + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0, 0]\n",
    "ax.plot(train_losses, label='Train Loss', linewidth=2.5, marker='o', markersize=4)\n",
    "ax.plot(val_losses, label='Val Loss', linewidth=2.5, marker='s', markersize=4)\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "ax = axes[0, 1]\n",
    "ax.plot(perplexities, linewidth=2.5, marker='o', markersize=4, color='orange')\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Validation Perplexity', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale loss\n",
    "ax = axes[1, 0]\n",
    "ax.semilogy(train_losses, label='Train Loss', linewidth=2.5, marker='o', markersize=4)\n",
    "ax.semilogy(val_losses, label='Val Loss', linewidth=2.5, marker='s', markersize=4)\n",
    "ax.set_xlabel('Epoch', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Loss (log scale)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Loss Convergence (Log Scale)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Training statistics\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "stats_text = f\"\"\"\n",
    "TRAINING STATISTICS\n",
    "\n",
    "Model Configuration:\n",
    "  Vocab Size: {vocab_size}\n",
    "  Embedding Dim: 128\n",
    "  Hidden Dim: 512\n",
    "  Num Layers: 2\n",
    "  Dropout: 0.2\n",
    "\n",
    "Training Details:\n",
    "  Epochs: {len(train_losses)}\n",
    "  Batch Size: {batch_size}\n",
    "  Sequence Length: {seq_len}\n",
    "  Total Parameters: {total_params:,}\n",
    "\n",
    "Results:\n",
    "  Best Val Loss: {best_val_loss:.4f}\n",
    "  Final Perplexity: {perplexities[-1]:.2f}\n",
    "  Training Time: {training_time:.1f} min\n",
    "\"\"\"\n",
    "ax.text(0.1, 0.5, stats_text, fontsize=10, family='monospace',\n",
    "        verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_progress.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training progress saved to training_progress.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Text Generation\n",
    "\n",
    "### Sampling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Sample next token from logits.\n",
    "    \n",
    "    Args:\n",
    "        logits: Unnormalized scores (vocab_size,)\n",
    "        temperature: Sampling temperature (1.0 = no change)\n",
    "        top_k: Keep only top-k tokens\n",
    "        top_p: Keep tokens with cumulative prob >= top_p\n",
    "    \n",
    "    Returns:\n",
    "        token_idx: Sampled token index\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    logits = logits / (temperature + 1e-8)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Top-k sampling\n",
    "    if top_k is not None and top_k > 0:\n",
    "        topk_probs, topk_indices = torch.topk(probs, min(top_k, len(probs)))\n",
    "        probs = torch.zeros_like(probs)\n",
    "        probs[topk_indices] = topk_probs\n",
    "        probs = probs / probs.sum()\n",
    "    \n",
    "    # Nucleus (top-p) sampling\n",
    "    if top_p is not None and top_p < 1.0:\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumsum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumsum > top_p\n",
    "        sorted_indices_to_remove[0] = False  # Keep at least one\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        probs = torch.zeros_like(probs)\n",
    "        probs[sorted_indices] = sorted_probs\n",
    "        probs = probs / probs.sum()\n",
    "    \n",
    "    # Sample\n",
    "    token_idx = torch.multinomial(probs, num_samples=1)\n",
    "    return token_idx.item()\n",
    "\n",
    "\n",
    "def generate_text(model, start_text, max_length, char_to_idx, idx_to_char, device,\n",
    "                  temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Generate text starting from start_text.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        start_text: Starting sequence (string)\n",
    "        max_length: Max characters to generate\n",
    "        char_to_idx, idx_to_char: Character mappings\n",
    "        device: Device to run on\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k sampling\n",
    "        top_p: Nucleus sampling\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: Full generated text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    current_text = start_text\n",
    "    context = torch.tensor([char_to_idx[c] for c in start_text], \n",
    "                           dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Keep only last seq_len tokens as context\n",
    "            context_trimmed = context[-seq_len:].unsqueeze(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(context_trimmed)\n",
    "            next_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Sample next token\n",
    "            next_idx = sample_next_token(next_logits, temperature=temperature,\n",
    "                                        top_k=top_k, top_p=top_p)\n",
    "            \n",
    "            # Append to context and text\n",
    "            context = torch.cat([context, torch.tensor([next_idx]).to(device)])\n",
    "            current_text += idx_to_char[next_idx]\n",
    "    \n",
    "    return current_text\n",
    "\n",
    "\n",
    "print(\"âœ“ Generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Samples with Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for generation\n",
    "start_prompts = [\"ROMEO:\", \"JULIET:\", \"The\", \"What\"]\n",
    "max_gen_length = 300\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEXT GENERATION SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "generated_samples = {}\n",
    "\n",
    "# Temperature variations\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TEMPERATURE VARIATIONS (Temperature = Diversity)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    generated = generate_text(model, start_prompts[0], max_gen_length,\n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             temperature=temp)\n",
    "    generated_samples[f'temp_{temp}'] = generated\n",
    "    print(generated[:250])\n",
    "    print(\"...\")\n",
    "\n",
    "# Top-k variations\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP-K SAMPLING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    print(f\"\\nTop-K = {k}:\")\n",
    "    generated = generate_text(model, start_prompts[1], max_gen_length,\n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             top_k=k)\n",
    "    generated_samples[f'topk_{k}'] = generated\n",
    "    print(generated[:250])\n",
    "    print(\"...\")\n",
    "\n",
    "# Top-p variations\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"NUCLEUS (TOP-P) SAMPLING\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for p in [0.5, 0.9, 0.95]:\n",
    "    print(f\"\\nTop-P = {p}:\")\n",
    "    generated = generate_text(model, start_prompts[2], max_gen_length,\n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             top_p=p)\n",
    "    generated_samples[f'topp_{p}'] = generated\n",
    "    print(generated[:250])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate from Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATION FROM DIFFERENT PROMPTS (Temperature=1.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in start_prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 80)\n",
    "    generated = generate_text(model, prompt, 500,\n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             temperature=1.0, top_p=0.9)\n",
    "    generated_samples[f'prompt_{prompt}'] = generated\n",
    "    # Print first 400 characters\n",
    "    print(generated[:400])\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation and Analysis\n",
    "\n",
    "### Text Diversity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_diversity(text, vocab_size):\n",
    "    \"\"\"\n",
    "    Analyze generated text for diversity and quality.\n",
    "    \"\"\"\n",
    "    # Character diversity\n",
    "    unique_chars = len(set(text))\n",
    "    char_entropy = -sum((text.count(c) / len(text)) * np.log(text.count(c) / len(text))\n",
    "                        for c in set(text))\n",
    "    \n",
    "    # Token repetition (bigrams)\n",
    "    bigrams = [text[i:i+2] for i in range(len(text)-1)]\n",
    "    unique_bigrams = len(set(bigrams))\n",
    "    bigram_diversity = unique_bigrams / len(bigrams) if bigrams else 0\n",
    "    \n",
    "    # Token repetition (words)\n",
    "    words = text.split()\n",
    "    unique_words = len(set(words))\n",
    "    word_diversity = unique_words / len(words) if words else 0\n",
    "    \n",
    "    return {\n",
    "        'unique_chars': unique_chars,\n",
    "        'char_entropy': char_entropy,\n",
    "        'bigram_diversity': bigram_diversity,\n",
    "        'word_diversity': word_diversity,\n",
    "        'text_length': len(text),\n",
    "    }\n",
    "\n",
    "\n",
    "# Analyze all samples\n",
    "analysis_results = []\n",
    "\n",
    "for name, text in generated_samples.items():\n",
    "    stats = analyze_text_diversity(text, vocab_size)\n",
    "    stats['Method'] = name\n",
    "    analysis_results.append(stats)\n",
    "\n",
    "import pandas as pd\n",
    "analysis_df = pd.DataFrame(analysis_results)\n",
    "\n",
    "# Display sorted by entropy\n",
    "print(\"\\nText Diversity Analysis:\")\n",
    "print(analysis_df[['Method', 'unique_chars', 'char_entropy', 'bigram_diversity', 'word_diversity']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sampling Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for temperature comparison\n",
    "temp_results = [r for r in analysis_results if 'temp_' in r['Method']]\n",
    "temp_names = [r['Method'].replace('temp_', '') for r in temp_results]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Entropy by temperature\n",
    "ax = axes[0]\n",
    "entropies = [r['char_entropy'] for r in temp_results]\n",
    "ax.bar(temp_names, entropies, color=['blue', 'green', 'red'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Character Entropy', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Temperature', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Entropy vs Temperature', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bigram diversity\n",
    "ax = axes[1]\n",
    "bigram_divs = [r['bigram_diversity'] for r in temp_results]\n",
    "ax.bar(temp_names, bigram_divs, color=['blue', 'green', 'red'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Bigram Diversity', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Temperature', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Bigram Diversity vs Temperature', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Unique characters\n",
    "ax = axes[2]\n",
    "unique_chars = [r['unique_chars'] for r in temp_results]\n",
    "ax.bar(temp_names, unique_chars, color=['blue', 'green', 'red'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Unique Characters', fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel('Temperature', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Character Variety vs Temperature', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim([0, vocab_size * 1.1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('generation_analysis.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Analysis saved to generation_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Summary and Documentation\n",
    "\n",
    "### Save Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model configuration and metadata\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_dim': 128,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.2,\n",
    "    'seq_len': seq_len,\n",
    "    'batch_size': batch_size,\n",
    "    'num_epochs': len(train_losses),\n",
    "    'best_val_loss': float(best_val_loss),\n",
    "    'final_perplexity': float(perplexities[-1]),\n",
    "    'training_time_minutes': training_time,\n",
    "    'chars': ''.join(chars),\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Save character mappings\n",
    "torch.save({\n",
    "    'char_to_idx': char_to_idx,\n",
    "    'idx_to_char': idx_to_char,\n",
    "}, 'char_mappings.pt')\n",
    "\n",
    "print(\"âœ“ Model configuration saved to model_config.json\")\n",
    "print(\"âœ“ Character mappings saved to char_mappings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   CHARACTER-LEVEL TEXT GENERATION PROJECT                     â•‘\n",
    "â•‘                          FINAL SUMMARY & RESULTS                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“Š DATASET STATISTICS\n",
    "  Total characters: {len(text):,}\n",
    "  Unique characters (vocab): {vocab_size}\n",
    "  Train sequences: {len(train_dataset):,}\n",
    "  Validation sequences: {len(val_dataset):,}\n",
    "  Train/Val split: 90/10\n",
    "\n",
    "ğŸ—ï¸  MODEL ARCHITECTURE\n",
    "  Type: Multi-layer LSTM\n",
    "  Embedding dimension: 128\n",
    "  Hidden dimension: 512\n",
    "  Number of layers: 2\n",
    "  Dropout: 0.2\n",
    "  Total parameters: {total_params:,}\n",
    "  Trainable parameters: {trainable_params:,}\n",
    "\n",
    "ğŸ“ˆ TRAINING RESULTS\n",
    "  Training epochs: {len(train_losses)}\n",
    "  Best validation loss: {best_val_loss:.4f}\n",
    "  Final perplexity: {perplexities[-1]:.2f}\n",
    "  Training time: {training_time:.1f} minutes\n",
    "  Optimizer: Adam (lr=0.001)\n",
    "  Learning rate schedule: StepLR (step_size=10, gamma=0.5)\n",
    "  Gradient clipping: norm=1.0\n",
    "  Batch size: {batch_size}\n",
    "\n",
    "ğŸ¯ GENERATION STRATEGIES TESTED\n",
    "  âœ“ Temperature sampling (0.5, 1.0, 1.5)\n",
    "  âœ“ Top-K sampling (5, 10, 20)\n",
    "  âœ“ Nucleus (top-p) sampling (0.5, 0.9, 0.95)\n",
    "  âœ“ Multiple seed texts\n",
    "\n",
    "ğŸ“ DELIVERABLES\n",
    "  âœ“ Trained model checkpoint: best_model.pt\n",
    "  âœ“ Model configuration: model_config.json\n",
    "  âœ“ Character mappings: char_mappings.pt\n",
    "  âœ“ Training visualization: training_progress.png\n",
    "  âœ“ Analysis visualization: generation_analysis.png\n",
    "  âœ“ Generated samples: 10+ examples with various strategies\n",
    "\n",
    "ğŸ” KEY OBSERVATIONS\n",
    "  â€¢ Lower temperature (0.5) â†’ More focused, less diverse\n",
    "  â€¢ Higher temperature (1.5) â†’ More diverse, sometimes incoherent\n",
    "  â€¢ Top-p sampling â†’ Better balance of coherence and diversity\n",
    "  â€¢ Model successfully learned character-level patterns\n",
    "  â€¢ Generated text shows realistic structure (dialogue, punctuation)\n",
    "\n",
    "ğŸ’¡ WHAT YOU LEARNED\n",
    "  âœ“ Character-level language modeling\n",
    "  âœ“ LSTM architecture and training\n",
    "  âœ“ Multiple text generation strategies\n",
    "  âœ“ Model evaluation (perplexity, diversity metrics)\n",
    "  âœ“ Professional code organization and documentation\n",
    "  âœ“ End-to-end NLP project pipeline\n",
    "\n",
    "ğŸš€ NEXT STEPS\n",
    "  â€¢ Fine-tune on different datasets\n",
    "  â€¢ Try word-level or subword tokenization\n",
    "  â€¢ Experiment with Transformer architecture (Day 17+)\n",
    "  â€¢ Build interactive web interface for generation\n",
    "  â€¢ Compare with GPT-2/GPT-3 (Days 24-26)\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                     PROJECT SUCCESSFULLY COMPLETED! ğŸ‰                       â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open('PROJECT_SUMMARY.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\nâœ“ Summary saved to PROJECT_SUMMARY.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Advanced Features (Optional)\n",
    "\n",
    "### Interactive Generation with Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_generate(model, char_to_idx, idx_to_char, device, seq_len):\n",
    "    \"\"\"\n",
    "    Interactive text generation interface.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERACTIVE TEXT GENERATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nEnter your custom prompts and generation parameters.\")\n",
    "    print(\"(Type 'quit' to exit)\\n\")\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"Enter prompt: \").strip()\n",
    "        if prompt.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if not prompt:\n",
    "            print(\"Prompt cannot be empty.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            length = int(input(\"Generation length (default 300): \") or \"300\")\n",
    "            temp = float(input(\"Temperature (default 1.0): \") or \"1.0\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input.\")\n",
    "            continue\n",
    "        \n",
    "        generated = generate_text(model, prompt, length, char_to_idx, idx_to_char,\n",
    "                                 device, temperature=temp, top_p=0.9)\n",
    "        \n",
    "        print(\"\\nGenerated text:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(generated)\n",
    "        print(\"-\" * 80)\n",
    "        print()\n",
    "\n",
    "# Uncomment to run interactive mode:\n",
    "# interactive_generate(model, char_to_idx, idx_to_char, device, seq_len)\n",
    "\n",
    "print(\"âœ“ Interactive generation function defined\")\n",
    "print(\"  Uncomment and run: interactive_generate(model, char_to_idx, idx_to_char, device, seq_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Sampling Strategies Side by Side"
   ]
  },
  {
   "cell_type": {"text/plain": "code"},
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate same prompt with all strategies for comparison\n",
    "comparison_prompt = \"ROMEO:\"\n",
    "gen_length = 200\n",
    "\n",
    "strategies = [\n",
    "    ('Greedy (T=0.1)', {'temperature': 0.1}),\n",
    "    ('Conservative (T=0.5)', {'temperature': 0.5}),\n",
    "    ('Balanced (T=1.0)', {'temperature': 1.0}),\n",
    "    ('Creative (T=1.5)', {'temperature': 1.5}),\n",
    "    ('Top-K=10', {'top_k': 10}),\n",
    "    ('Top-P=0.9', {'top_p': 0.9}),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"COMPARISON: All strategies with same prompt: '{comparison_prompt}'\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, kwargs in strategies:\n",
    "    text = generate_text(model, comparison_prompt, gen_length,\n",
    "                        char_to_idx, idx_to_char, device, **kwargs)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Conclusion and Key Takeaways\n",
    "\n",
    "### What You've Built\n",
    "\n",
    "âœ… **Complete text generation system** from data to deployment  \n",
    "âœ… **Multi-layer LSTM** trained on character-level sequences  \n",
    "âœ… **Multiple generation strategies** for controlling output quality/diversity  \n",
    "âœ… **Professional evaluation** with metrics and visualizations  \n",
    "âœ… **Production-ready code** with documentation  \n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Temperature Controls Creativity**\n",
    "   - Low (0.1-0.5): Focused, repetitive\n",
    "   - Medium (1.0): Balanced\n",
    "   - High (1.5+): Diverse, sometimes incoherent\n",
    "\n",
    "2. **Sampling Strategies Matter**\n",
    "   - Greedy: Fast but boring\n",
    "   - Top-K: Reasonable, but not perfect\n",
    "   - Top-P: Excellent balance (industry standard)\n",
    "\n",
    "3. **Perplexity Measures Uncertainty**\n",
    "   - Lower is better\n",
    "   - Baseline: vocab_size (random)\n",
    "   - Good models: 50-200\n",
    "\n",
    "4. **Character vs Word Level**\n",
    "   - Character: Handles everything, longer sequences\n",
    "   - Word: Faster, semantic awareness, needs preprocessing\n",
    "   - Modern: Subword (BPE, SentencePiece) - best of both\n",
    "\n",
    "### Architecture Insights\n",
    "\n",
    "- **LSTM vs GRU**: LSTMs more common for language modeling\n",
    "- **Gradient clipping**: Essential for RNNs to prevent explosion\n",
    "- **Dropout**: Helps prevent overfitting\n",
    "- **Layer normalization**: Increasingly used in modern architectures\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- Code completion (GitHub Copilot)\n",
    "- Chatbots and dialogue systems\n",
    "- Creative writing and story generation\n",
    "- Machine translation\n",
    "- Question answering systems\n",
    "\n",
    "### Portfolio Value\n",
    "\n",
    "This project demonstrates:\n",
    "- Deep learning implementation from scratch\n",
    "- NLP fundamentals\n",
    "- Problem-solving and debugging\n",
    "- Professional code organization\n",
    "- Clear communication of results\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Phase 2!** ğŸ‰\n",
    "\n",
    "You've progressed from basic neural networks (Phase 1) to understanding and implementing:\n",
    "- Recurrent architectures\n",
    "- Sequence modeling\n",
    "- Attention mechanisms\n",
    "- Transformers\n",
    "- Complete NLP systems\n",
    "\n",
    "Next up: **Phase 3 (Days 21-30)** focuses on implementing Transformers and training GPT from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_name_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
