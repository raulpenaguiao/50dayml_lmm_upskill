{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Model Evaluation and Metrics\n",
    "\n",
    "**Time:** 3 hours\n",
    "\n",
    "**Mathematical Prerequisites:**\n",
    "- Probability theory (conditional probability, Bayes theorem)\n",
    "- Statistics (hypothesis testing, confidence intervals)\n",
    "- Linear algebra (for multi-class metrics)\n",
    "- Information theory basics (for some metrics)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Accuracy alone is **insufficient** for evaluating ML models. Today we explore:\n",
    "1. Comprehensive classification metrics (precision, recall, F1, etc.)\n",
    "2. Multi-class evaluation strategies\n",
    "3. Visual evaluation tools (ROC curves, PR curves, calibration plots)\n",
    "4. Statistical model comparison\n",
    "5. Systematic error analysis\n",
    "\n",
    "**Goal:** Build a complete evaluation framework for any classification task\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Theory - Why Accuracy is Not Enough\n",
    "\n",
    "### 1.1 The Accuracy Paradox\n",
    "\n",
    "Consider a cancer detection problem:\n",
    "- 1% of patients have cancer\n",
    "- Model predicts \"no cancer\" for everyone\n",
    "- **Accuracy: 99%!** But **completely useless**\n",
    "\n",
    "**Problem:** Accuracy treats all errors equally and ignores class imbalance.\n",
    "\n",
    "### 1.2 Confusion Matrix\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "```\n",
    "                Predicted\n",
    "              Pos      Neg\n",
    "Actual Pos    TP       FN\n",
    "       Neg    FP       TN\n",
    "```\n",
    "\n",
    "- **True Positive (TP):** Correctly predicted positive\n",
    "- **True Negative (TN):** Correctly predicted negative\n",
    "- **False Positive (FP):** Type I error (predicted positive, actually negative)\n",
    "- **False Negative (FN):** Type II error (predicted negative, actually positive)\n",
    "\n",
    "### 1.3 Key Metrics\n",
    "\n",
    "**Accuracy:**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Precision (Positive Predictive Value):**\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP} = P(\\text{actual pos} | \\text{predicted pos})\n",
    "$$\n",
    "*\"Of all positive predictions, how many were correct?\"*\n",
    "\n",
    "**Recall (Sensitivity, True Positive Rate):**\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN} = P(\\text{predicted pos} | \\text{actual pos})\n",
    "$$\n",
    "*\"Of all actual positives, how many did we find?\"*\n",
    "\n",
    "**Specificity (True Negative Rate):**\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "**F1 Score (Harmonic Mean of Precision and Recall):**\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}\n",
    "$$\n",
    "\n",
    "**F-Beta Score (Weighted Harmonic Mean):**\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\beta^2 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "- $\\beta > 1$: Favor recall (e.g., cancer detection)\n",
    "- $\\beta < 1$: Favor precision (e.g., spam detection)\n",
    "\n",
    "### 1.4 Multi-Class Averaging\n",
    "\n",
    "For $C$ classes:\n",
    "\n",
    "**Macro Average:**\n",
    "$$\n",
    "\\text{Macro-F1} = \\frac{1}{C} \\sum_{i=1}^C F1_i\n",
    "$$\n",
    "*Treats all classes equally (good for imbalanced datasets)*\n",
    "\n",
    "**Micro Average:**\n",
    "$$\n",
    "\\text{Micro-F1} = \\frac{2 \\sum_{i=1}^C TP_i}{2 \\sum_{i=1}^C TP_i + \\sum_{i=1}^C FP_i + \\sum_{i=1}^C FN_i}\n",
    "$$\n",
    "*Treats all samples equally (dominated by frequent classes)*\n",
    "\n",
    "**Weighted Average:**\n",
    "$$\n",
    "\\text{Weighted-F1} = \\sum_{i=1}^C \\frac{n_i}{N} F1_i\n",
    "$$\n",
    "where $n_i$ is the number of samples in class $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    cohen_kappa_score, matthews_corrcoef,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.calibration import calibration_curve\n",
    "from scipy import stats\n",
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Models and Data\n",
    "\n",
    "We'll use CIFAR-10 and evaluate multiple models from Day 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Multiple Models for Comparison\n",
    "\n",
    "We'll train 3 different models to compare their evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, num_epochs=5):\n",
    "    \"\"\"Quick training function for demonstration.\"\"\"\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    \n",
    "    # Load train data\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Create model\n",
    "    if model_name == 'ResNet18':\n",
    "        model = models.resnet18(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif model_name == 'ResNet34':\n",
    "        model = models.resnet34(pretrained=True)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif model_name == 'MobileNetV2':\n",
    "        model = models.mobilenet_v2(pretrained=True)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Freeze backbone (feature extraction for speed)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    if model_name in ['ResNet18', 'ResNet34']:\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            if i % 200 == 199:\n",
    "                print(f'Epoch {epoch+1}, Batch {i+1}: Loss={running_loss/200:.3f}, Acc={100.*correct/total:.2f}%')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train models\n",
    "model1 = train_model('ResNet18', num_epochs=5)\n",
    "model2 = train_model('ResNet34', num_epochs=5)\n",
    "model3 = train_model('MobileNetV2', num_epochs=5)\n",
    "\n",
    "models_dict = {\n",
    "    'ResNet18': model1,\n",
    "    'ResNet34': model2,\n",
    "    'MobileNetV2': model3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_probs(model, data_loader):\n",
    "    \"\"\"Get predictions and class probabilities.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# Get predictions for all models\n",
    "predictions_dict = {}\n",
    "probabilities_dict = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"Getting predictions for {name}...\")\n",
    "    preds, labels, probs = get_predictions_and_probs(model, test_loader)\n",
    "    predictions_dict[name] = preds\n",
    "    probabilities_dict[name] = probs\n",
    "\n",
    "# Store ground truth labels (same for all models)\n",
    "true_labels = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comprehensive Metrics Analysis\n",
    "\n",
    "### 3.1 Basic Metrics for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive classification metrics.\"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'Macro Precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'Macro Recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'Macro F1': f1_score(y_true, y_pred, average='macro'),\n",
    "        'Weighted Precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'Weighted Recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'Weighted F1': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'Cohen Kappa': cohen_kappa_score(y_true, y_pred),\n",
    "        'Matthews Corr Coef': matthews_corrcoef(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics for all models\n",
    "all_metrics = {}\n",
    "for name, preds in predictions_dict.items():\n",
    "    all_metrics[name] = calculate_all_metrics(true_labels, preds)\n",
    "\n",
    "# Display as DataFrame\n",
    "df_metrics = pd.DataFrame(all_metrics).T\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE METRICS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(df_metrics.to_string())\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Understanding Different Metrics\n",
    "\n",
    "**Balanced Accuracy:**\n",
    "$$\n",
    "\\text{Balanced Acc} = \\frac{1}{C} \\sum_{i=1}^C \\frac{TP_i}{TP_i + FN_i}\n",
    "$$\n",
    "Average of per-class recall. Better than accuracy for imbalanced data.\n",
    "\n",
    "**Cohen's Kappa:**\n",
    "$$\n",
    "\\kappa = \\frac{p_o - p_e}{1 - p_e}\n",
    "$$\n",
    "where $p_o$ is observed agreement and $p_e$ is expected agreement by chance.\n",
    "- $\\kappa = 1$: Perfect agreement\n",
    "- $\\kappa = 0$: Random agreement\n",
    "- $\\kappa < 0$: Worse than random\n",
    "\n",
    "**Matthews Correlation Coefficient (MCC):**\n",
    "$$\n",
    "MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "$$\n",
    "Balanced metric even for imbalanced classes. Range: [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Select key metrics to visualize\n",
    "key_metrics = ['Accuracy', 'Macro F1', 'Weighted F1', 'Balanced Accuracy']\n",
    "\n",
    "for idx, metric in enumerate(key_metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = [all_metrics[name][metric] for name in models_dict.keys()]\n",
    "    bars = ax.bar(models_dict.keys(), values, alpha=0.7, edgecolor='black')\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title(metric, fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Comparison: Key Metrics', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Per-Class Analysis\n",
    "\n",
    "### 4.1 Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report for best model\n",
    "best_model_name = max(all_metrics.keys(), key=lambda k: all_metrics[k]['Accuracy'])\n",
    "best_preds = predictions_dict[best_model_name]\n",
    "\n",
    "print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(true_labels, best_preds, target_names=classes, digits=4))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Per-Class Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "precision_per_class = precision_score(true_labels, best_preds, average=None)\n",
    "recall_per_class = recall_score(true_labels, best_preds, average=None)\n",
    "f1_per_class = f1_score(true_labels, best_preds, average=None)\n",
    "\n",
    "# Create DataFrame\n",
    "df_per_class = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Precision': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(df_per_class.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Grouped bar chart\n",
    "x = np.arange(len(classes))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x - width, precision_per_class, width, label='Precision', alpha=0.8)\n",
    "ax1.bar(x, recall_per_class, width, label='Recall', alpha=0.8)\n",
    "ax1.bar(x + width, f1_per_class, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Class', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title(f'Per-Class Metrics - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(classes, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Heatmap\n",
    "metrics_array = np.array([precision_per_class, recall_per_class, f1_per_class])\n",
    "sns.heatmap(metrics_array, annot=True, fmt='.3f', cmap='YlGnBu',\n",
    "            xticklabels=classes, yticklabels=['Precision', 'Recall', 'F1'],\n",
    "            ax=ax2, cbar_kws={'label': 'Score'})\n",
    "ax2.set_title(f'Metrics Heatmap - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "best_class_idx = f1_per_class.argmax()\n",
    "worst_class_idx = f1_per_class.argmin()\n",
    "\n",
    "print(f\"\\nBest performing class: {classes[best_class_idx]} (F1={f1_per_class[best_class_idx]:.4f})\")\n",
    "print(f\"Worst performing class: {classes[worst_class_idx]} (F1={f1_per_class[worst_class_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Confusion Matrix Analysis\n",
    "\n",
    "### 5.1 Confusion Matrices for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, (name, preds) in enumerate(predictions_dict.items()):\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    \n",
    "    # Normalize by row (true labels)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                ax=axes[idx], cbar_kws={'label': 'Proportion'})\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
    "    axes[idx].set_ylabel('True Label', fontsize=11)\n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {all_metrics[name][\"Accuracy\"]:.4f}', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Normalized Confusion Matrices (Row-Normalized)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Common Misclassification Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confusion patterns for best model\n",
    "cm = confusion_matrix(true_labels, best_preds)\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "\n",
    "# Find top 10 confusion pairs\n",
    "print(f\"\\nTop 10 Most Common Misclassifications ({best_model_name}):\\n\")\n",
    "print(f\"{'Rank':<5} {'True Class':<15} {'Predicted As':<15} {'Count':<8} {'% of True Class'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "flat_indices = np.argsort(cm_no_diag.ravel())[::-1]\n",
    "for i, flat_idx in enumerate(flat_indices[:10]):\n",
    "    true_idx, pred_idx = np.unravel_index(flat_idx, cm.shape)\n",
    "    count = cm_no_diag[true_idx, pred_idx]\n",
    "    total_in_class = cm[true_idx, :].sum()\n",
    "    percentage = 100 * count / total_in_class\n",
    "    print(f\"{i+1:<5} {classes[true_idx]:<15} {classes[pred_idx]:<15} {count:<8} {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ROC Curves and AUC\n",
    "\n",
    "### 6.1 Theory: ROC and AUC\n",
    "\n",
    "**ROC (Receiver Operating Characteristic) Curve:**\n",
    "- Plot of True Positive Rate vs False Positive Rate at various thresholds\n",
    "- TPR = Recall = $\\frac{TP}{TP+FN}$\n",
    "- FPR = $\\frac{FP}{FP+TN}$\n",
    "\n",
    "**AUC (Area Under Curve):**\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC = 0.5: Random classifier\n",
    "- AUC < 0.5: Worse than random\n",
    "\n",
    "**Multi-class ROC:**\n",
    "- One-vs-Rest (OvR): Each class vs all others\n",
    "- Macro-average: Average of per-class AUC\n",
    "- Micro-average: Aggregate all classes\n",
    "\n",
    "### 6.2 ROC Curves for Multi-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_roc(y_true, y_probs, class_names, model_name):\n",
    "    \"\"\"Plot ROC curves for multi-class classification.\"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    \n",
    "    # Binarize labels for OvR\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    \n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve and AUC\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_probs.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Compute macro-average ROC curve and AUC\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # Plot 1: Per-class ROC curves\n",
    "    colors = cycle(plt.cm.tab10.colors)\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax1.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC={roc_auc[i]:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC=0.5)')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax1.set_title(f'Per-Class ROC Curves - {model_name}', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='lower right', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Micro and Macro average\n",
    "    ax2.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "            label=f'Micro-average (AUC={roc_auc[\"micro\"]:.3f})',\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "    ax2.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "            label=f'Macro-average (AUC={roc_auc[\"macro\"]:.3f})',\n",
    "            color='navy', linestyle=':', linewidth=4)\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC=0.5)')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax2.set_title(f'Average ROC Curves - {model_name}', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='lower right', fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC for best model\n",
    "best_probs = probabilities_dict[best_model_name]\n",
    "roc_auc_scores = plot_multiclass_roc(true_labels, best_probs, classes, best_model_name)\n",
    "\n",
    "print(f\"\\nMacro-average AUC: {roc_auc_scores['macro']:.4f}\")\n",
    "print(f\"Micro-average AUC: {roc_auc_scores['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Precision-Recall Curves\n",
    "\n",
    "### 7.1 Theory: Precision-Recall vs ROC\n",
    "\n",
    "**When to use PR curves instead of ROC:**\n",
    "- Imbalanced datasets (few positive samples)\n",
    "- Care more about positive class\n",
    "- ROC can be overly optimistic for imbalanced data\n",
    "\n",
    "**Average Precision (AP):**\n",
    "$$\n",
    "AP = \\sum_n (R_n - R_{n-1}) P_n\n",
    "$$\n",
    "Approximates area under PR curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curves(y_true, y_probs, class_names, model_name):\n",
    "    \"\"\"Plot Precision-Recall curves for multi-class.\"\"\"\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    n_classes = len(class_names)\n",
    "    y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    \n",
    "    # Compute PR curve for each class\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    avg_precision = dict()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_probs[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_true_bin[:, i], y_probs[:, i])\n",
    "    \n",
    "    # Micro-average\n",
    "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "        y_true_bin.ravel(), y_probs.ravel()\n",
    "    )\n",
    "    avg_precision[\"micro\"] = average_precision_score(y_true_bin, y_probs, average=\"micro\")\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # Per-class curves\n",
    "    colors = cycle(plt.cm.tab10.colors)\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax1.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AP={avg_precision[i]:.3f})')\n",
    "    \n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('Recall', fontsize=12)\n",
    "    ax1.set_ylabel('Precision', fontsize=12)\n",
    "    ax1.set_title(f'Per-Class Precision-Recall Curves - {model_name}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='lower left', fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Micro-average\n",
    "    ax2.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "            label=f'Micro-average (AP={avg_precision[\"micro\"]:.3f})',\n",
    "            color='deeppink', linestyle=':', linewidth=4)\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall', fontsize=12)\n",
    "    ax2.set_ylabel('Precision', fontsize=12)\n",
    "    ax2.set_title(f'Micro-Average Precision-Recall - {model_name}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='lower left', fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "# Plot PR curves\n",
    "ap_scores = plot_precision_recall_curves(true_labels, best_probs, classes, best_model_name)\n",
    "\n",
    "print(f\"\\nMean Average Precision (mAP): {np.mean([ap_scores[i] for i in range(10)]):.4f}\")\n",
    "print(f\"Micro-average AP: {ap_scores['micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Calibration Analysis\n",
    "\n",
    "### 8.1 Theory: Probability Calibration\n",
    "\n",
    "A well-calibrated model:\n",
    "- If it predicts 70% probability, the event should occur 70% of the time\n",
    "\n",
    "**Calibration Plot:**\n",
    "- X-axis: Predicted probability\n",
    "- Y-axis: Observed frequency\n",
    "- Perfect calibration: y = x line\n",
    "\n",
    "**Expected Calibration Error (ECE):**\n",
    "$$\n",
    "ECE = \\sum_{m=1}^M \\frac{|B_m|}{N} |\\text{acc}(B_m) - \\text{conf}(B_m)|\n",
    "$$\n",
    "where $B_m$ are bins of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curve(y_true, y_probs, model_name, n_bins=10):\n",
    "    \"\"\"Plot calibration curve for multi-class model.\"\"\"\n",
    "    # Get predicted probabilities for predicted class\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "    y_conf = np.max(y_probs, axis=1)\n",
    "    \n",
    "    # Check if prediction is correct\n",
    "    y_correct = (y_pred == y_true).astype(int)\n",
    "    \n",
    "    # Compute calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_correct, y_conf, n_bins=n_bins, strategy='uniform')\n",
    "    \n",
    "    # Compute ECE\n",
    "    ece = np.mean(np.abs(prob_true - prob_pred))\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Calibration curve\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=2, label='Perfect Calibration')\n",
    "    ax1.plot(prob_pred, prob_true, 's-', lw=2, markersize=10, \n",
    "            label=f'{model_name}\\nECE={ece:.4f}')\n",
    "    ax1.set_xlabel('Mean Predicted Probability', fontsize=12)\n",
    "    ax1.set_ylabel('Fraction of Positives', fontsize=12)\n",
    "    ax1.set_title('Calibration Curve', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='lower right', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Confidence histogram\n",
    "    ax2.hist(y_conf, bins=n_bins, edgecolor='black', alpha=0.7)\n",
    "    ax2.set_xlabel('Predicted Confidence', fontsize=12)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('Distribution of Predicted Confidences', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.suptitle(f'Model Calibration Analysis - {model_name}', \n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ece\n",
    "\n",
    "# Plot calibration for all models\n",
    "ece_scores = {}\n",
    "for name, probs in probabilities_dict.items():\n",
    "    print(f\"\\nCalibration for {name}:\")\n",
    "    ece = plot_calibration_curve(true_labels, probs, name)\n",
    "    ece_scores[name] = ece\n",
    "    print(f\"Expected Calibration Error: {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Statistical Model Comparison\n",
    "\n",
    "### 9.1 McNemar's Test\n",
    "\n",
    "Tests if two models have significantly different error rates.\n",
    "\n",
    "**Contingency Table:**\n",
    "```\n",
    "              Model 2 Correct  Model 2 Wrong\n",
    "Model 1 Correct      a              b\n",
    "Model 1 Wrong        c              d\n",
    "```\n",
    "\n",
    "**Test Statistic:**\n",
    "$$\n",
    "\\chi^2 = \\frac{(b - c)^2}{b + c}\n",
    "$$\n",
    "\n",
    "**Null Hypothesis:** Models have same error rate\n",
    "\n",
    "**Decision:** If p < 0.05, models are significantly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "def mcnemar_test(y_true, y_pred1, y_pred2, model1_name, model2_name):\n",
    "    \"\"\"Perform McNemar's test between two models.\"\"\"\n",
    "    # Create contingency table\n",
    "    correct1 = (y_pred1 == y_true)\n",
    "    correct2 = (y_pred2 == y_true)\n",
    "    \n",
    "    a = np.sum(correct1 & correct2)   # Both correct\n",
    "    b = np.sum(correct1 & ~correct2)  # Model 1 correct, Model 2 wrong\n",
    "    c = np.sum(~correct1 & correct2)  # Model 1 wrong, Model 2 correct\n",
    "    d = np.sum(~correct1 & ~correct2) # Both wrong\n",
    "    \n",
    "    contingency_table = np.array([[a, b], [c, d]])\n",
    "    \n",
    "    # McNemar's test\n",
    "    result = mcnemar(contingency_table, exact=False, correction=True)\n",
    "    \n",
    "    return {\n",
    "        'contingency_table': contingency_table,\n",
    "        'statistic': result.statistic,\n",
    "        'p_value': result.pvalue\n",
    "    }\n",
    "\n",
    "# Compare all pairs of models\n",
    "model_names = list(predictions_dict.keys())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"McNemar's Test for Pairwise Model Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(i+1, len(model_names)):\n",
    "        name1, name2 = model_names[i], model_names[j]\n",
    "        result = mcnemar_test(true_labels, predictions_dict[name1], \n",
    "                             predictions_dict[name2], name1, name2)\n",
    "        \n",
    "        print(f\"\\n{name1} vs {name2}:\")\n",
    "        print(f\"Contingency Table:\")\n",
    "        print(f\"                {name2} Correct  {name2} Wrong\")\n",
    "        print(f\"{name1} Correct  {result['contingency_table'][0,0]:>12}  {result['contingency_table'][0,1]:>12}\")\n",
    "        print(f\"{name1} Wrong    {result['contingency_table'][1,0]:>12}  {result['contingency_table'][1,1]:>12}\")\n",
    "        print(f\"\\nChi-square statistic: {result['statistic']:.4f}\")\n",
    "        print(f\"p-value: {result['p_value']:.4f}\")\n",
    "        \n",
    "        if result['p_value'] < 0.05:\n",
    "            print(\"✓ Significant difference (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"✗ No significant difference (p >= 0.05)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Confidence Intervals for Accuracy\n",
    "\n",
    "Using Wilson score interval (better than normal approximation for extreme probabilities):\n",
    "\n",
    "$$\n",
    "CI = \\frac{\\hat{p} + \\frac{z^2}{2n} \\pm z\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z^2}{4n^2}}}{1 + \\frac{z^2}{n}}\n",
    "$$\n",
    "\n",
    "where $\\hat{p}$ is observed accuracy, $n$ is sample size, $z$ is z-score (1.96 for 95% CI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def wilson_confidence_interval(n_correct, n_total, confidence=0.95):\n",
    "    \"\"\"Compute Wilson score confidence interval.\"\"\"\n",
    "    z = norm.ppf(1 - (1 - confidence) / 2)\n",
    "    p_hat = n_correct / n_total\n",
    "    \n",
    "    denominator = 1 + z**2 / n_total\n",
    "    center = p_hat + z**2 / (2 * n_total)\n",
    "    spread = z * np.sqrt(p_hat * (1 - p_hat) / n_total + z**2 / (4 * n_total**2))\n",
    "    \n",
    "    lower = (center - spread) / denominator\n",
    "    upper = (center + spread) / denominator\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "# Compute confidence intervals for all models\n",
    "n_total = len(true_labels)\n",
    "ci_data = []\n",
    "\n",
    "for name, preds in predictions_dict.items():\n",
    "    n_correct = np.sum(preds == true_labels)\n",
    "    accuracy = n_correct / n_total\n",
    "    lower, upper = wilson_confidence_interval(n_correct, n_total)\n",
    "    \n",
    "    ci_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'CI_Lower': lower,\n",
    "        'CI_Upper': upper,\n",
    "        'CI_Width': upper - lower\n",
    "    })\n",
    "\n",
    "df_ci = pd.DataFrame(ci_data)\n",
    "\n",
    "print(\"\\n95% Confidence Intervals for Accuracy (Wilson Score):\")\n",
    "print(\"=\"*70)\n",
    "for _, row in df_ci.iterrows():\n",
    "    print(f\"{row['Model']:<15}: {row['Accuracy']:.4f} [{row['CI_Lower']:.4f}, {row['CI_Upper']:.4f}] (width={row['CI_Width']:.4f})\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "y_pos = np.arange(len(df_ci))\n",
    "errors = np.array([[df_ci['Accuracy'].values - df_ci['CI_Lower'].values],\n",
    "                   [df_ci['CI_Upper'].values - df_ci['Accuracy'].values]])\n",
    "\n",
    "ax.errorbar(df_ci['Accuracy'], y_pos, xerr=errors, fmt='o', markersize=10,\n",
    "           capsize=5, capthick=2, elinewidth=2)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(df_ci['Model'])\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Model Accuracy with 95% Confidence Intervals (Wilson Score)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.set_xlim([df_ci['CI_Lower'].min() - 0.01, df_ci['CI_Upper'].max() + 0.01])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Comprehensive Evaluation Dashboard\n",
    "\n",
    "Create a final summary dashboard with all key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary_data = []\n",
    "\n",
    "for name in models_dict.keys():\n",
    "    preds = predictions_dict[name]\n",
    "    probs = probabilities_dict[name]\n",
    "    \n",
    "    # Basic metrics\n",
    "    acc = accuracy_score(true_labels, preds)\n",
    "    \n",
    "    # Multi-class metrics\n",
    "    macro_f1 = f1_score(true_labels, preds, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, preds, average='weighted')\n",
    "    \n",
    "    # AUC (one-vs-rest)\n",
    "    try:\n",
    "        macro_auc = roc_auc_score(true_labels, probs, average='macro', multi_class='ovr')\n",
    "    except:\n",
    "        macro_auc = 0\n",
    "    \n",
    "    # Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(true_labels, preds)\n",
    "    \n",
    "    # ECE\n",
    "    ece = ece_scores[name]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'Macro-F1': macro_f1,\n",
    "        'Weighted-F1': weighted_f1,\n",
    "        'Macro-AUC': macro_auc,\n",
    "        'Cohen-Kappa': kappa,\n",
    "        'ECE': ece\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE EVALUATION DASHBOARD\")\n",
    "print(\"=\"*100)\n",
    "print(df_summary.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Radar chart for comparison\n",
    "from math import pi\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Metrics to plot (normalize ECE by inverting: 1-ECE)\n",
    "metrics = ['Accuracy', 'Macro-F1', 'Weighted-F1', 'Macro-AUC', 'Cohen-Kappa']\n",
    "num_vars = len(metrics)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot for each model\n",
    "for idx, row in df_summary.iterrows():\n",
    "    values = [row[m] for m in metrics]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'])\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics, size=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Comparison Radar Chart', size=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Key Takeaways and Best Practices\n",
    "\n",
    "### 11.1 Metric Selection Guidelines\n",
    "\n",
    "**Use Case → Metric Mapping:**\n",
    "\n",
    "| Use Case | Primary Metric | Why |\n",
    "|----------|----------------|-----|\n",
    "| Balanced classes | Accuracy | Simple, interpretable |\n",
    "| Imbalanced classes | Macro-F1, Balanced Accuracy | Treats all classes equally |\n",
    "| Cost-sensitive (FP costly) | Precision | Minimize false positives |\n",
    "| Cost-sensitive (FN costly) | Recall | Minimize false negatives |\n",
    "| Ranking quality | AUC-ROC | Threshold-independent |\n",
    "| Imbalanced ranking | AUC-PR | Focus on positive class |\n",
    "| Probability calibration | ECE, Brier Score | Need well-calibrated probabilities |\n",
    "| Multi-label | Hamming Loss, Subset Accuracy | Handle label correlations |\n",
    "\n",
    "### 11.2 Common Pitfalls\n",
    "\n",
    "1. **Using only accuracy on imbalanced data**\n",
    "   - Solution: Use F1, balanced accuracy, or class weights\n",
    "\n",
    "2. **Ignoring confidence calibration**\n",
    "   - Solution: Plot calibration curves, apply calibration methods\n",
    "\n",
    "3. **Not using confidence intervals**\n",
    "   - Solution: Always report CIs, especially for small datasets\n",
    "\n",
    "4. **Comparing models without statistical tests**\n",
    "   - Solution: Use McNemar's test, paired t-tests, etc.\n",
    "\n",
    "5. **Forgetting about computational cost**\n",
    "   - Solution: Include training time, inference time in evaluation\n",
    "\n",
    "### 11.3 Mathematical Insights\n",
    "\n",
    "**Precision-Recall Trade-off:**\n",
    "$$\n",
    "\\text{Precision} \\uparrow \\implies \\text{Recall} \\downarrow\n",
    "$$\n",
    "Controlled by classification threshold. F1 balances this trade-off.\n",
    "\n",
    "**Why Harmonic Mean for F1?**\n",
    "- Arithmetic mean: $(P + R)/2$ - too lenient\n",
    "- Geometric mean: $\\sqrt{PR}$ - still too lenient\n",
    "- Harmonic mean: $\\frac{2PR}{P+R}$ - penalizes imbalance\n",
    "\n",
    "**Example:** P=0.9, R=0.1\n",
    "- Arithmetic: 0.5 (misleading!)\n",
    "- Harmonic: 0.18 (more realistic)\n",
    "\n",
    "### 11.4 Reporting Checklist\n",
    "\n",
    "When reporting model performance, always include:\n",
    "\n",
    "✅ **Basic Metrics:**\n",
    "- Accuracy (with confidence interval)\n",
    "- Macro and weighted F1\n",
    "- Per-class precision/recall\n",
    "\n",
    "✅ **Visual Analysis:**\n",
    "- Confusion matrix (normalized)\n",
    "- ROC curves (if applicable)\n",
    "- Calibration plot\n",
    "\n",
    "✅ **Statistical Tests:**\n",
    "- Confidence intervals\n",
    "- Comparison with baseline (McNemar's test)\n",
    "\n",
    "✅ **Error Analysis:**\n",
    "- Most common misclassifications\n",
    "- Failure modes\n",
    "- Examples of errors\n",
    "\n",
    "✅ **Computational Cost:**\n",
    "- Training time\n",
    "- Inference time\n",
    "- Model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've built a comprehensive model evaluation framework. You now understand:\n",
    "\n",
    "✅ Why accuracy is insufficient (accuracy paradox)  \n",
    "✅ Comprehensive classification metrics (precision, recall, F1, etc.)  \n",
    "✅ Multi-class averaging strategies (macro, micro, weighted)  \n",
    "✅ ROC curves and AUC interpretation  \n",
    "✅ Precision-Recall curves for imbalanced data  \n",
    "✅ Probability calibration analysis  \n",
    "✅ Statistical model comparison (McNemar's test)  \n",
    "✅ Confidence intervals for metrics  \n",
    "✅ Systematic error analysis  \n",
    "✅ Best practices for metric selection and reporting  \n",
    "\n",
    "**Key Insight:** No single metric tells the whole story. Always use multiple complementary metrics and statistical tests.\n",
    "\n",
    "**Time spent:** ~3 hours\n",
    "\n",
    "**Next:** Day 8 - Hyperparameter Tuning and Experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
