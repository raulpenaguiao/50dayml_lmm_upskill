# Day 07: Model Evaluation and Metrics - Further Reading

This day covers comprehensive evaluation metrics and techniques for assessing model performance. Choosing the right metric is critical for solving real-world problems correctly.

## References

1. **Metrics for Evaluating Machine Learning Models**
   - [Scikit-learn Metrics Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)
   - Official reference for classification metrics with detailed explanations of accuracy, precision, recall, F1, ROC-AUC, and more.

2. **Precision and Recall: Understanding the Tradeoff**
   - [Precision vs Recall: An Interactive Guide](https://towardsdatascience.com/precision-vs-recall-c23e9b63ce77)
   - Intuitive explanation of precision-recall tradeoffs with practical examples showing when to use each metric.

3. **Receiver Operating Characteristic (ROC) Curves**
   - [ROC Curves and AUC Explained](https://towardsdatascience.com/roc-auc-explained-auc-explained-82eb31e36dfd)
   - Comprehensive guide to ROC curves, AUC scores, and their interpretation for binary and multiclass classification.

4. **Confusion Matrix and Beyond: Classification Metrics Guide**
   - [Papers With Code: Classification Metrics](https://paperswithcode.com/methods/category/metric)
   - Reference implementations and explanations of different classification metrics with visual examples.

5. **Cross-Validation and Model Selection**
   - [Scikit-learn Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)
   - Practical guide to k-fold cross-validation, stratified splits, and proper model selection techniques.

---

**Tip:** Start with reference #1 for metric definitions, then #2 for practical intuition, and #5 for proper evaluation methodology.
