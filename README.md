
# üöÄ ML/LLM 50-Day Upskill Course

> **‚ö†Ô∏è IMPORTANT:** This course is designed for people with **graduate-level mathematics** (Master's degree or equivalent). If you don't have strong foundations in linear algebra, calculus, probability, and optimization, please start with foundational courses first.

## üí° Idea of the Course
This course is designed to **upskill software engineers with strong mathematical backgrounds into ML/LLM engineers** within 50 days (3-4 hours per day).

**Target Audience:**
- Software engineers with MS/PhD in quantitative fields (Math, Physics, CS, Engineering, Statistics)
- Researchers transitioning to ML engineering
- Quantitative professionals (finance, physics) moving into AI
- Anyone with graduate-level math who wants hands-on ML implementation skills

Through a mix of theory and hands-on projects, participants will leverage their mathematical foundation to quickly master machine learning, deep learning, and large language models (LLMs).

By the end, you will have completed **5 portfolio-ready projects** that demonstrate your skills and prepare you for an ML/LLM Engineer role.

---

## üìã Prerequisites (REQUIRED)

### Mathematical Background (Graduate Level)
You must have strong understanding of:
- ‚úÖ **Linear Algebra**: Matrix operations, eigenvalues/eigenvectors, SVD, vector spaces, norms
- ‚úÖ **Calculus**: Multivariable calculus, partial derivatives, chain rule, gradients, Jacobians
- ‚úÖ **Probability & Statistics**: Probability distributions, expectation, variance, Bayes' theorem, maximum likelihood
- ‚úÖ **Optimization**: Gradient descent, convex optimization, Lagrange multipliers
- ‚úÖ **Numerical Methods**: Understanding of numerical stability, floating-point arithmetic

**Expected Level:** Master's degree in Mathematics, Physics, Engineering, Computer Science, Statistics, or equivalent quantitative field.

### Programming Skills
- ‚úÖ **Python**: Proficient in Python 3.x (NumPy, object-oriented programming)
- ‚úÖ **Software Engineering**: Version control (Git), virtual environments, debugging
- ‚úÖ **Command Line**: Comfortable with terminal/bash

### Nice to Have (but not required)
- Basic machine learning concepts
- Experience with scientific computing (MATLAB, R, Julia)
- Familiarity with Linux/Unix systems

---

## ‚è±Ô∏è Time Commitment
- **Realistic estimate**: 3-4 hours per day √ó 50 days = 150-200 hours total
- Original "2 hours/day" is underestimated for most people
- Some days (projects, complex topics) may require 5-6 hours
- Budget extra time for debugging and experimentation

**Recommendation:** Block out 4 hours/day to complete comfortably  

---

## üóÇÔ∏è Outline of the Course

The course is structured into **5 phases**, each with specific goals and a capstone project.  

### **Phase 1: Foundations of ML & Deep Learning (Days 1‚Äì10)**
- Python, PyTorch, and ML tooling setup  
- Classical ML (regression, gradient descent)  
- Neural networks from scratch

**Project 1:** Implement a simple neural network in PyTorch for MNIST classification  

---

### **Phase 2: Deep Learning & NLP Basics (Days 11‚Äì20)**
- CNNs for image classification  
- Introduction to NLP (tokenization, embeddings)  
- RNNs, GRUs, LSTMs for sequence modeling  
- Attention mechanism and intro to Transformers  

**Project 2:** Character-level RNN for text generation  

---

### **Phase 3: Transformers & LLM Foundations (Days 21‚Äì30)**
- Self-attention, multi-head attention  
- Transformer blocks and training  
- Tokenization strategies (BPE, WordPiece)  
- Training a small GPT-like model  

**Project 3:** Karpathy‚Äôs *‚ÄúminGPT - Little Shakespeare‚Äù* (train and generate text with a GPT from scratch)  

---

### **Phase 4: Modern ML/LLM Engineering (Days 31‚Äì40)**
- Hugging Face Transformers library  
- Fine-tuning pretrained models on custom datasets  
- Evaluation metrics (BLEU, ROUGE, perplexity, accuracy, F1)  
- Model serving with FastAPI & Docker  

**Project 4:** Fine-tuned Transformer (BERT/DistilBERT) for text classification  

---

### **Phase 5: Advanced Topics & Capstone Project (Days 41‚Äì50)**
- Retrieval-Augmented Generation (RAG)  
- Prompt engineering & evaluation  
- Training optimization techniques  
- MLOps basics (MLflow, model versioning, deployment)  

**Project 5 (Capstone):** End-to-end ML/LLM application (choose one):  
- RAG-powered chatbot with Hugging Face + FAISS  
- Document classification pipeline with deployment  
- Small-scale fine-tuned LLM with LoRA  

---

## üèÜ Skills Developed
- PyTorch for deep learning  
- Transformer architecture & LLMs  
- Hugging Face ecosystem  
- Fine-tuning & LoRA techniques  
- MLOps fundamentals (tracking, serving, Docker)  
- Building end-to-end ML/LLM applications  

---

## üìÅ Repository Structure
```
.
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ day01/
‚îÇ   ‚îú‚îÄ‚îÄ day01_syllabus.md
‚îÇ   ‚îú‚îÄ‚îÄ day01_notebook.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ day01_resources/
‚îú‚îÄ‚îÄ day02/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ projects/
‚îÇ   ‚îú‚îÄ‚îÄ project1-mnist-nn/
‚îÇ   ‚îú‚îÄ‚îÄ project2-rnn-textgen/
‚îÇ   ‚îú‚îÄ‚îÄ project3-minGPT/
‚îÇ   ‚îú‚îÄ‚îÄ project4-text-classification/
‚îÇ   ‚îî‚îÄ‚îÄ project5-capstone/
```

---

## üìä Current State of the Course

### Completion Summary
- **Notebooks Created**: 16/50 (Days 1-16) = **32% complete**
- **Syllabi Created**: 50/50 (All days) = **100% complete**
- **Reading Materials**: 50/50 (All days) = **100% complete**
- **Project Directories**: 5/5 = **100% created**

### What's Done
‚úÖ **Phase 1: Foundations (Days 1-10)**
- All 10 notebooks completed with hands-on PyTorch implementations
- Covers neural networks, CNNs, optimization, transfer learning, evaluation metrics
- Full project 1 (MNIST neural network) ready

‚úÖ **Phase 2: Deep Learning & NLP Basics (Days 11-16)**
- 6 notebooks completed covering NLP preprocessing, embeddings, RNNs, LSTMs, Seq2seq, attention
- Foundation for transformer understanding established
- Remaining Phase 2 (Days 17-20): Syllabus/reading ready, notebooks pending

‚úÖ **All Planning Materials**
- Complete syllabus files for all 50 days
- Reading lists for all 50 days
- Clear learning objectives and structure across all phases

### What's Pending
‚è≥ **Days 17-50 Notebooks** (68% remaining)
- **Days 17-20** (4 notebooks): Intro to Transformers, text classification, character-level LMs, project 2
- **Days 21-30** (10 notebooks): Transformer architecture deep dive, GPT/BERT implementation, minGPT project
- **Days 31-40** (10 notebooks): Hugging Face, fine-tuning, MLOps, deployment, project 4
- **Days 41-50** (10 notebooks): RAG, prompt engineering, advanced optimization, capstone project

---

## üéØ Expected Final State

### When Complete, the Course Will Have:

‚úÖ **50 Complete Jupyter Notebooks** (interactive tutorials with:
- Clear learning objectives and code structure
- Step-by-step implementations with explanations
- Running code examples that learners can experiment with
- Visualization of results and concepts
- Debugging and best practices embedded

‚úÖ **5 Portfolio-Ready Capstone Projects**
- Project 1: MNIST neural network in pure PyTorch
- Project 2: Character-level RNN text generation
- Project 3: minGPT implementation (train & generate Shakespeare)
- Project 4: Fine-tuned BERT for text classification with FastAPI deployment
- Project 5: Advanced application (RAG chatbot, document QA, or fine-tuned LLM with LoRA)

‚úÖ **Complete Learning Path**
- 50 structured days progressing from foundations ‚Üí deep learning ‚Üí transformers ‚Üí production ML
- Consistent code style and PyTorch patterns
- Projects integrated with daily lessons
- Real-world ML engineering skills demonstrated

‚úÖ **Reference Documentation**
- Updated COURSE_OVERVIEW.md with completion metrics
- Phase summaries showing learning progression
- Troubleshooting guides and common pitfalls

### Course Readiness Criteria
- All 50 notebooks executable and tested
- All projects deployable with clear instructions
- Clear learning objectives met for each day
- Code quality consistent across all materials
- Time estimates validated (3-4 hours/day realistic)

---

## üéØ Learning Philosophy

This course **assumes you already understand the mathematics** and focuses on:
1. **Implementation skills**: Translating mathematical knowledge into code
2. **Modern tools**: PyTorch, Transformers, production deployment
3. **Practical techniques**: What actually works in industry vs. theory
4. **Systems thinking**: Building complete ML systems, not just models

**What this course does NOT teach:**
- Basic linear algebra, calculus, or probability (you must know these)
- Detailed mathematical proofs (you can derive these yourself)
- Statistical learning theory (covered in graduate courses)
- Basic programming or software engineering

**What this course DOES teach:**
- How to implement deep learning algorithms efficiently
- Modern LLM architectures (Transformers, GPT, BERT)
- Production ML engineering (APIs, deployment, MLOps)
- Practical techniques that aren't in textbooks

---

## üéØ Expected Outcomes

By completing this course, you will:
- ‚úÖ Build **5 portfolio-ready ML/LLM projects** demonstrating end-to-end capabilities
- ‚úÖ Master **PyTorch** and modern deep learning frameworks
- ‚úÖ Understand **Transformer architectures** deeply (GPT, BERT, attention mechanisms)
- ‚úÖ Gain **production ML skills** (Docker, FastAPI, MLflow, deployment)
- ‚úÖ Be competitive for **ML Engineer or LLM Engineer** roles (not just junior positions)
- ‚úÖ Have hands-on experience with **RAG**, **fine-tuning**, and **prompt engineering**

**Career Level:** This prepares you for mid-level ML Engineer roles given your strong mathematical background. You'll have both theoretical understanding AND practical implementation skills.  

# Outline of course

## Day 1: Building Your First Neural Network in PyTorch
Link to syllabus: [day01_syllabus.md](day01/day01_syllabus.md)

Jump straight into building and training a simple neural network using PyTorch to classify handwritten digits from the MNIST dataset.  
The focus is on hands-on learning by coding, running, and experimenting.

## Day 2: Convolutional Neural Networks for Image Classification
Link to syllabus: [day02_syllabus.md](day02/day02_syllabus.md)

Understand and implement a basic Convolutional Neural Network (CNN) in PyTorch for image classification on the MNIST dataset.
Learn how CNNs improve performance over simple MLPs for image data.

## Day 3: Advanced CNNs and Data Augmentation
Link to syllabus: [day03_syllabus.md](day03/day03_syllabus.md)

## Day 4: Understanding Gradient Descent and Backpropagation
Link to syllabus: [day04_syllabus.md](day04/day04_syllabus.md)

## Day 5: Building Neural Networks from Scratch
Link to syllabus: [day05_syllabus.md](day05/day05_syllabus.md)

## Day 6: Transfer Learning and Pretrained Models
Link to syllabus: [day06_syllabus.md](day06/day06_syllabus.md)

## Day 7: Model Evaluation and Metrics
Link to syllabus: [day07_syllabus.md](day07/day07_syllabus.md)

## Day 8: Hyperparameter Tuning and Experimentation
Link to syllabus: [day08_syllabus.md](day08/day08_syllabus.md)

## Day 9: Advanced CNN Architectures
Link to syllabus: [day09_syllabus.md](day09/day09_syllabus.md)

## Day 10: Project 1 - MNIST Classification with Custom NN
Link to syllabus: [day10_syllabus.md](day10/day10_syllabus.md)

---

## Day 11: Introduction to Natural Language Processing
Link to syllabus: [day11_syllabus.md](day11/day11_syllabus.md)

## Day 12: Word Embeddings and Vector Representations
Link to syllabus: [day12_syllabus.md](day12/day12_syllabus.md)

## Day 13: Recurrent Neural Networks (RNNs)
Link to syllabus: [day13_syllabus.md](day13/day13_syllabus.md)

## Day 14: Long Short-Term Memory Networks (LSTMs)
Link to syllabus: [day14_syllabus.md](day14/day14_syllabus.md)

## Day 15: Sequence-to-Sequence Models
Link to syllabus: [day15_syllabus.md](day15/day15_syllabus.md)

## Day 16: Attention Mechanisms
Link to syllabus: [day16_syllabus.md](day16/day16_syllabus.md)

## Day 17: Introduction to Transformers
Link to syllabus: [day17_syllabus.md](day17/day17_syllabus.md)

## Day 18: Text Classification with Deep Learning
Link to syllabus: [day18_syllabus.md](day18/day18_syllabus.md)

## Day 19: Character-Level Models and Text Generation
Link to syllabus: [day19_syllabus.md](day19/day19_syllabus.md)

## Day 20: Project 2 - Character-Level RNN for Text Generation
Link to syllabus: [day20_syllabus.md](day20/day20_syllabus.md)

---

## Day 21: Transformer Encoder Architecture
Link to syllabus: [day21_syllabus.md](day21/day21_syllabus.md)

## Day 22: Transformer Decoder and Full Architecture
Link to syllabus: [day22_syllabus.md](day22/day22_syllabus.md)

## Day 23: Tokenization Strategies for LLMs
Link to syllabus: [day23_syllabus.md](day23/day23_syllabus.md)

## Day 24: Introduction to GPT Architecture
Link to syllabus: [day24_syllabus.md](day24/day24_syllabus.md)

## Day 25: Training GPT from Scratch
Link to syllabus: [day25_syllabus.md](day25/day25_syllabus.md)

## Day 26: BERT and Masked Language Modeling
Link to syllabus: [day26_syllabus.md](day26/day26_syllabus.md)

## Day 27: Karpathy's minGPT - Understanding the Code
Link to syllabus: [day27_syllabus.md](day27/day27_syllabus.md)

## Day 28: Training minGPT on Shakespeare
Link to syllabus: [day28_syllabus.md](day28/day28_syllabus.md)

## Day 29: Advanced Training Techniques for LLMs
Link to syllabus: [day29_syllabus.md](day29/day29_syllabus.md)

## Day 30: Project 3 - minGPT on Shakespeare (Portfolio Project)
Link to syllabus: [day30_syllabus.md](day30/day30_syllabus.md)

---

## Day 31: Introduction to Hugging Face Transformers
Link to syllabus: [day31_syllabus.md](day31/day31_syllabus.md)

## Day 32: Fine-Tuning BERT for Text Classification
Link to syllabus: [day32_syllabus.md](day32/day32_syllabus.md)

## Day 33: Advanced Fine-Tuning Techniques
Link to syllabus: [day33_syllabus.md](day33/day33_syllabus.md)

## Day 34: Working with Large Language Models
Link to syllabus: [day34_syllabus.md](day34/day34_syllabus.md)

## Day 35: Evaluation Metrics for NLP Models
Link to syllabus: [day35_syllabus.md](day35/day35_syllabus.md)

## Day 36: Model Deployment with FastAPI
Link to syllabus: [day36_syllabus.md](day36/day36_syllabus.md)

## Day 37: Docker and Containerization
Link to syllabus: [day37_syllabus.md](day37/day37_syllabus.md)

## Day 38: MLOps Basics with MLflow
Link to syllabus: [day38_syllabus.md](day38/day38_syllabus.md)

## Day 39: Building a Complete ML Pipeline
Link to syllabus: [day39_syllabus.md](day39/day39_syllabus.md)

## Day 40: Project 4 - Fine-Tuned Text Classification System
Link to syllabus: [day40_syllabus.md](day40/day40_syllabus.md)

---

## Day 41: Introduction to Retrieval-Augmented Generation (RAG)
Link to syllabus: [day41_syllabus.md](day41/day41_syllabus.md)

## Day 42: Building a Document Q&A System with RAG
Link to syllabus: [day42_syllabus.md](day42/day42_syllabus.md)

## Day 43: Prompt Engineering and In-Context Learning
Link to syllabus: [day43_syllabus.md](day43/day43_syllabus.md)

## Day 44: LLM Evaluation and Safety
Link to syllabus: [day44_syllabus.md](day44/day44_syllabus.md)

## Day 45: Advanced Training Optimization
Link to syllabus: [day45_syllabus.md](day45/day45_syllabus.md)

## Day 46: LangChain and LLM Applications
Link to syllabus: [day46_syllabus.md](day46/day46_syllabus.md)

## Day 47: Capstone Project Planning
Link to syllabus: [day47_syllabus.md](day47/day47_syllabus.md)

## Day 48: Capstone Implementation - Part 1
Link to syllabus: [day48_syllabus.md](day48/day48_syllabus.md)

## Day 49: Capstone Implementation - Part 2
Link to syllabus: [day49_syllabus.md](day49/day49_syllabus.md)

## Day 50: Final Presentation and Course Completion
Link to syllabus: [day50_syllabus.md](day50/day50_syllabus.md)





