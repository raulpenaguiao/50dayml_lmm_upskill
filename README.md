
# ğŸš€ ML/LLM 50-Day Upskill Course

## ğŸ’¡ Idea of the Course
This course is designed to **upskill software engineers into ML engineers** within 50 days (2 hours per day).  
Through a mix of theory and hands-on projects, participants will learn the core concepts of machine learning, deep learning, and large language models (LLMs).  

By the end, you will have completed **5 portfolio-ready projects** that demonstrate your skills and prepare you for a junior ML Engineer role.

---

## ğŸ“‹ Requirements
Before starting, make sure you have:
- âœ… Solid mathematical basis (linear algebra, calculus, probability)  
- âœ… Python experience (comfortable writing and running code)  
- âœ… Familiarity with virtual environments (conda/venv)  
- âœ… Git for version control and project management  

---

## ğŸ—‚ï¸ Outline of the Course

The course is structured into **5 phases**, each with specific goals and a capstone project.  

### **Phase 1: Foundations of ML & Deep Learning (Days 1â€“10)**
- Python, PyTorch, and ML tooling setup  
- Linear algebra, calculus, probability for ML  
- Classical ML (regression, gradient descent)  
- Neural networks from scratch  

**Project 1:** Implement a simple neural network in PyTorch for MNIST classification  

---

### **Phase 2: Deep Learning & NLP Basics (Days 11â€“20)**
- CNNs for image classification  
- Introduction to NLP (tokenization, embeddings)  
- RNNs, GRUs, LSTMs for sequence modeling  
- Attention mechanism and intro to Transformers  

**Project 2:** Character-level RNN for text generation  

---

### **Phase 3: Transformers & LLM Foundations (Days 21â€“30)**
- Self-attention, multi-head attention  
- Transformer blocks and training  
- Tokenization strategies (BPE, WordPiece)  
- Training a small GPT-like model  

**Project 3:** Karpathyâ€™s *â€œminGPT - Little Shakespeareâ€* (train and generate text with a GPT from scratch)  

---

### **Phase 4: Modern ML/LLM Engineering (Days 31â€“40)**
- Hugging Face Transformers library  
- Fine-tuning pretrained models on custom datasets  
- Evaluation metrics (BLEU, ROUGE, perplexity, accuracy, F1)  
- Model serving with FastAPI & Docker  

**Project 4:** Fine-tuned Transformer (BERT/DistilBERT) for text classification  

---

### **Phase 5: Advanced Topics & Capstone Project (Days 41â€“50)**
- Retrieval-Augmented Generation (RAG)  
- Prompt engineering & evaluation  
- Training optimization techniques  
- MLOps basics (MLflow, model versioning, deployment)  

**Project 5 (Capstone):** End-to-end ML/LLM application (choose one):  
- RAG-powered chatbot with Hugging Face + FAISS  
- Document classification pipeline with deployment  
- Small-scale fine-tuned LLM with LoRA  

---

## ğŸ† Skills Developed
- PyTorch for deep learning  
- Transformer architecture & LLMs  
- Hugging Face ecosystem  
- Fine-tuning & LoRA techniques  
- MLOps fundamentals (tracking, serving, Docker)  
- Building end-to-end ML/LLM applications  

---

## ğŸ“ Repository Structure
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ day01/
â”‚   â”œâ”€â”€ day01_syllabus.md
â”‚   â”œâ”€â”€ day01_notebook.ipynb
â”‚   â””â”€â”€ day01_resources/
â”œâ”€â”€ day02/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ projects/
â”‚   â”œâ”€â”€ project1-mnist-nn/
â”‚   â”œâ”€â”€ project2-rnn-textgen/
â”‚   â”œâ”€â”€ project3-minGPT/
â”‚   â”œâ”€â”€ project4-text-classification/
â”‚   â””â”€â”€ project5-capstone/
```

---

## ğŸ¯ Outcome
By completing this course, you will:  
- Build 5 strong ML/LLM projects for your portfolio  
- Understand and apply modern ML/LLM techniques  
- Be job-ready for a **Junior ML Engineer** role  

# Outline of course

## Day 1: Building Your First Neural Network in PyTorch
Link to syllabus: [day01_syllabus.md](day01/day01_syllabus.md)

## Day 2: Convolutional Neural Networks for Image Classification
Link to syllabus: [day02_syllabus.md](day02/day02_syllabus.md)


