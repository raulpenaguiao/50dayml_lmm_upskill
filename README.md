
# 🚀 ML/LLM 50-Day Upskill Course

## 💡 Idea of the Course
This course is designed to **upskill software engineers into ML engineers** within 50 days (2 hours per day).  
Through a mix of theory and hands-on projects, participants will learn the core concepts of machine learning, deep learning, and large language models (LLMs).  

By the end, you will have completed **5 portfolio-ready projects** that demonstrate your skills and prepare you for a junior ML Engineer role.

---

## 📋 Requirements
Before starting, make sure you have:
- ✅ Solid mathematical basis (linear algebra, calculus, probability)  
- ✅ Python experience (comfortable writing and running code)  
- ✅ Familiarity with virtual environments (conda/venv)  
- ✅ Git for version control and project management  

---

## 🗂️ Outline of the Course

The course is structured into **5 phases**, each with specific goals and a capstone project.  

### **Phase 1: Foundations of ML & Deep Learning (Days 1–10)**
- Python, PyTorch, and ML tooling setup  
- Linear algebra, calculus, probability for ML  
- Classical ML (regression, gradient descent)  
- Neural networks from scratch  

**Project 1:** Implement a simple neural network in PyTorch for MNIST classification  

---

### **Phase 2: Deep Learning & NLP Basics (Days 11–20)**
- CNNs for image classification  
- Introduction to NLP (tokenization, embeddings)  
- RNNs, GRUs, LSTMs for sequence modeling  
- Attention mechanism and intro to Transformers  

**Project 2:** Character-level RNN for text generation  

---

### **Phase 3: Transformers & LLM Foundations (Days 21–30)**
- Self-attention, multi-head attention  
- Transformer blocks and training  
- Tokenization strategies (BPE, WordPiece)  
- Training a small GPT-like model  

**Project 3:** Karpathy’s *“minGPT - Little Shakespeare”* (train and generate text with a GPT from scratch)  

---

### **Phase 4: Modern ML/LLM Engineering (Days 31–40)**
- Hugging Face Transformers library  
- Fine-tuning pretrained models on custom datasets  
- Evaluation metrics (BLEU, ROUGE, perplexity, accuracy, F1)  
- Model serving with FastAPI & Docker  

**Project 4:** Fine-tuned Transformer (BERT/DistilBERT) for text classification  

---

### **Phase 5: Advanced Topics & Capstone Project (Days 41–50)**
- Retrieval-Augmented Generation (RAG)  
- Prompt engineering & evaluation  
- Training optimization techniques  
- MLOps basics (MLflow, model versioning, deployment)  

**Project 5 (Capstone):** End-to-end ML/LLM application (choose one):  
- RAG-powered chatbot with Hugging Face + FAISS  
- Document classification pipeline with deployment  
- Small-scale fine-tuned LLM with LoRA  

---

## 🏆 Skills Developed
- PyTorch for deep learning  
- Transformer architecture & LLMs  
- Hugging Face ecosystem  
- Fine-tuning & LoRA techniques  
- MLOps fundamentals (tracking, serving, Docker)  
- Building end-to-end ML/LLM applications  

---

## 📁 Repository Structure
```
.
├── README.md
├── day01/
│   ├── day01_syllabus.md
│   ├── day01_notebook.ipynb
│   └── day01_resources/
├── day02/
│   └── ...
├── projects/
│   ├── project1-mnist-nn/
│   ├── project2-rnn-textgen/
│   ├── project3-minGPT/
│   ├── project4-text-classification/
│   └── project5-capstone/
```

---

## 🎯 Outcome
By completing this course, you will:  
- Build 5 strong ML/LLM projects for your portfolio  
- Understand and apply modern ML/LLM techniques  
- Be job-ready for a **Junior ML Engineer** role  

# Outline of course

## Day 1: Building Your First Neural Network in PyTorch
Link to syllabus: [day01_syllabus.md](day01/day01_syllabus.md)

## Day 2: Convolutional Neural Networks for Image Classification
Link to syllabus: [day02_syllabus.md](day02/day02_syllabus.md)


