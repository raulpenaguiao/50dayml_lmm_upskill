# Day 34: Working with Large Language Models - Further Reading

This day covers efficient techniques for working with large language models including memory optimization, quantization, and prompt engineering strategies.

## References

1. **Hugging Face Guide to Large Models**
   - [HF Large Models Guide](https://huggingface.co/docs/transformers/perf_train_gpu_one)
   - Official guide to GPU optimization and training large models efficiently.

2. **Quantization: 8-bit and 4-bit**
   - [BitsandBytes Library](https://github.com/TimDettmers/bitsandbytes)
   - Library enabling 8-bit and 4-bit inference and training for memory-efficient LLM usage.

3. **Prompt Engineering for LLMs**
   - [OpenAI: Best Practices for Prompting](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)
   - Best practices for crafting effective prompts for language models.

4. **Flash Attention: Fast and Memory-Efficient Attention**
   - [Dao et al.: Flash Attention](https://arxiv.org/abs/2205.14135)
   - Paper on efficient attention implementation dramatically reducing memory usage.

5. **Running Large Models Locally**
   - [Ollama: Run LLMs Locally](https://ollama.ai/)
   - Tool for running large language models locally with quantization for consumer hardware.

---

**Tip:** Start with reference #3 for prompt engineering, #2 for memory optimization, #1 for comprehensive guide, and #4 for advanced efficiency.
