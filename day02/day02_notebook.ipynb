{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Day 02 of upskilling to ML engineer.\n",
    "This day we will continue to work on the MNIST dataset. Our tasks are:\n",
    "- Understand and implement a basic Convolutional Neural Network (CNN) in PyTorch for image classification.  \n",
    "- Learn how CNNs improve performance over simple MLPs for image data.\n",
    "\n",
    "\n",
    "## What is a CNN\n",
    "A Convolutional Neural Network (CNN) is a type of deep learning architecture specifically designed for processing _grid-like data_ such as images, using convolutional layers that apply filters to detect local features like edges, textures, and patterns. These networks excel at computer vision tasks because they can automatically learn hierarchical representations, starting with simple features in early layers and combining them into more complex patterns in deeper layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision matplotlib scikit-learn > out.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages from pytorch for convolutional neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We'll use the same MNIST dataset from Day 1, which contains 60,000 training images and 10,000 test images of handwritten digits (0-9), each in grayscale at 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first 5 samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(train_dataset[i][0].squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {train_dataset[i][1]}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-architecture",
   "metadata": {},
   "source": [
    "## Build a Simple CNN\n",
    "\n",
    "Our CNN architecture will include:\n",
    "- **Conv Layer 1**: 1 input channel (grayscale) → 32 feature maps, 3x3 kernel, ReLU activation\n",
    "- **MaxPool 1**: 2x2 pooling to reduce spatial dimensions\n",
    "- **Conv Layer 2**: 32 → 64 feature maps, 3x3 kernel, ReLU activation\n",
    "- **MaxPool 2**: 2x2 pooling\n",
    "- **Flatten**: Convert 2D feature maps to 1D vector\n",
    "- **FC Layer 1**: Fully connected layer with 128 neurons\n",
    "- **FC Layer 2**: Output layer with 10 neurons (one per class)\n",
    "\n",
    "**Why CNNs work better for images:**\n",
    "- **Local connectivity**: Each neuron only connects to a small region, preserving spatial structure\n",
    "- **Parameter sharing**: Same filters applied across the entire image, reducing parameters\n",
    "- **Translation invariance**: Features detected anywhere in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-cnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # First convolutional layer: 1 input channel, 32 output channels, 3x3 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        # Second convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Max pooling layer: 2x2 window\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Fully connected layers\n",
    "        # After 2 pooling layers (2x2), 28x28 becomes 7x7\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block: Conv -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -> 14x14\n",
    "        # Second conv block: Conv -> ReLU -> Pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -> 7x7\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        # Fully connected layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "cnn_model = SimpleCNN()\n",
    "\n",
    "# Print model architecture\n",
    "print(\"CNN Model Architecture:\")\n",
    "print(cnn_model)\n",
    "print(\"\\nTrainable parameters:\")\n",
    "total_params = sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-cnn",
   "metadata": {},
   "source": [
    "## Train the CNN\n",
    "We'll train for 3 epochs using the Adam optimizer and CrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "cnn_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = cnn_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] completed. '\n",
    "          f'Average Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "print('\\nTraining completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(range(1, num_epochs+1), train_losses, 'b-', marker='o')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(range(1, num_epochs+1), train_accuracies, 'g-', marker='o')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate",
   "metadata": {},
   "source": [
    "## Evaluate the CNN on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = cnn_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'\\nTest Accuracy of CNN: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-predictions",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions (5 correct, 5 incorrect)\n",
    "num_correct = 5\n",
    "num_incorrect = 5\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    for image, label in test_dataset:\n",
    "        if correct_count >= num_correct and incorrect_count >= num_incorrect:\n",
    "            break\n",
    "            \n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        output = cnn_model(image_tensor)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        \n",
    "        if predicted.item() == label and correct_count < num_correct:\n",
    "            axes[0, correct_count].imshow(image.squeeze().cpu(), cmap='gray')\n",
    "            axes[0, correct_count].set_title(f'✓ Label: {label}')\n",
    "            axes[0, correct_count].axis('off')\n",
    "            correct_count += 1\n",
    "        elif predicted.item() != label and incorrect_count < num_incorrect:\n",
    "            axes[1, incorrect_count].imshow(image.squeeze().cpu(), cmap='gray')\n",
    "            axes[1, incorrect_count].set_title(f'✗ Pred: {predicted.item()}, True: {label}', color='red')\n",
    "            axes[1, incorrect_count].axis('off')\n",
    "            incorrect_count += 1\n",
    "\n",
    "plt.suptitle('CNN Predictions: Correct (top) vs Incorrect (bottom)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confusion-matrix",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix: CNN on MNIST\", fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i in range(10):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = 100 * class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"Digit {i}: {class_acc:.2f}% ({class_correct}/{class_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison",
   "metadata": {},
   "source": [
    "## Comparison: CNN vs MLP\n",
    "\n",
    "Based on Day 1's MLP results and today's CNN:\n",
    "\n",
    "### MLP (from Day 1)\n",
    "- **Architecture**: Simple 2-layer network (784 → 16 → 10)\n",
    "- **Test Accuracy**: ~73-88% (depending on configuration)\n",
    "- **Parameters**: ~12,000 parameters\n",
    "- **Limitation**: Treats image as flat vector, losing spatial structure\n",
    "\n",
    "### CNN (Day 2)\n",
    "- **Architecture**: Conv layers + pooling + FC layers\n",
    "- **Test Accuracy**: Typically 98-99%\n",
    "- **Parameters**: More parameters but better feature learning\n",
    "- **Advantage**: Preserves spatial structure, learns local patterns\n",
    "\n",
    "### Why CNNs Win for Images:\n",
    "1. **Spatial hierarchies**: Early layers detect edges, later layers detect complex patterns\n",
    "2. **Parameter efficiency**: Shared weights across spatial locations\n",
    "3. **Translation invariance**: Features detected anywhere in the image\n",
    "4. **Local connectivity**: Each neuron focuses on a small region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-goals",
   "metadata": {},
   "source": [
    "## Stretch Goals (Optional)\n",
    "\n",
    "Try experimenting with:\n",
    "1. Adding more convolutional layers\n",
    "2. Changing kernel sizes (3x3, 5x5)\n",
    "3. Adjusting the number of filters (16, 32, 64, 128)\n",
    "4. Different optimizers (SGD with momentum, RMSprop)\n",
    "5. Batch normalization layers\n",
    "6. Visualizing learned filters from the first convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-filters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Visualize first layer filters\n",
    "def visualize_filters(model):\n",
    "    # Get the weights from the first convolutional layer\n",
    "    filters = model.conv1.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Normalize filters for visualization\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    \n",
    "    # Plot first 16 filters\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < 32:\n",
    "            ax.imshow(filters[i, 0], cmap='gray')\n",
    "            ax.set_title(f'Filter {i+1}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Learned Filters from First Convolutional Layer', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_filters(cnn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
