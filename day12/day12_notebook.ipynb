{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 12: Word Embeddings and Vector Representations\n",
    "\n",
    "## Phase 2: NLP Basics (Days 11-20)\n",
    "\n",
    "**Estimated Time: 3-4 hours**\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the distributional hypothesis and dense vector representations\n",
    "- Implement Word2Vec (Skip-gram and CBOW) from scratch\n",
    "- Learn GloVe: Global Vectors for Word Representation\n",
    "- Explore embedding arithmetic and semantic relationships\n",
    "- Visualize embeddings in 2D/3D space using dimensionality reduction\n",
    "- Use pretrained embeddings for downstream tasks\n",
    "\n",
    "### Prerequisites\n",
    "- Day 11: Introduction to NLP (tokenization, vocabulary)\n",
    "- Linear algebra (matrix factorization, SVD)\n",
    "- Neural network fundamentals (gradient descent, backpropagation)\n",
    "- Probability theory (softmax, cross-entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Distributional Hypothesis\n",
    "\n",
    "### 1.1 From Sparse to Dense Representations\n",
    "\n",
    "**\"You shall know a word by the company it keeps\"** - J.R. Firth (1957)\n",
    "\n",
    "The distributional hypothesis states that words occurring in similar contexts tend to have similar meanings. This insight forms the foundation of modern word embeddings.\n",
    "\n",
    "#### Limitations of Sparse Representations\n",
    "\n",
    "Recall from Day 11 our sparse representations (BoW, TF-IDF):\n",
    "- High dimensionality (vocabulary size V ~ 10,000 - 1,000,000)\n",
    "- Sparse vectors (mostly zeros)\n",
    "- No semantic similarity captured\n",
    "- \"King\" and \"Queen\" are as different as \"King\" and \"Banana\"\n",
    "\n",
    "#### Dense Embeddings\n",
    "\n",
    "Word embeddings map words to dense, low-dimensional vectors:\n",
    "$$\\text{embed}: \\mathcal{V} \\rightarrow \\mathbb{R}^d$$\n",
    "\n",
    "where $d \\ll |\\mathcal{V}|$ (typically $d \\in [50, 300]$)\n",
    "\n",
    "**Key Properties:**\n",
    "1. **Semantic similarity**: Similar words have similar vectors\n",
    "2. **Compositionality**: Vector arithmetic captures relationships\n",
    "3. **Generalization**: Learn from co-occurrence statistics\n",
    "4. **Transfer**: Pretrained embeddings work across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Sparse vs Dense representations\n",
    "vocab = ['king', 'queen', 'man', 'woman', 'royal', 'crown', 'apple', 'banana']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Sparse one-hot encoding\n",
    "print(\"Sparse One-Hot Encoding:\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "for i, word in enumerate(vocab[:4]):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[i] = 1\n",
    "    print(f\"{word:10s}: {one_hot}\")\n",
    "\n",
    "print(\"\\nCosine similarity (one-hot):\")\n",
    "king_oh = np.array([1, 0, 0, 0, 0, 0, 0, 0])\n",
    "queen_oh = np.array([0, 1, 0, 0, 0, 0, 0, 0])\n",
    "apple_oh = np.array([0, 0, 0, 0, 0, 0, 1, 0])\n",
    "print(f\"sim(king, queen) = {1 - cosine(king_oh, queen_oh):.4f}\")\n",
    "print(f\"sim(king, apple) = {1 - cosine(king_oh, apple_oh):.4f}\")\n",
    "print(\"All pairs are orthogonal - no semantic information!\")\n",
    "\n",
    "# Dense embeddings (hypothetical)\n",
    "print(\"\\nDense Embeddings (hypothetical 4D):\")\n",
    "embeddings = {\n",
    "    'king':   np.array([0.8, 0.9, 0.7, -0.5]),\n",
    "    'queen':  np.array([0.8, 0.9, 0.7, 0.5]),\n",
    "    'man':    np.array([-0.3, 0.2, 0.6, -0.6]),\n",
    "    'woman':  np.array([-0.3, 0.2, 0.6, 0.6]),\n",
    "    'apple':  np.array([-0.8, -0.7, -0.5, 0.1])\n",
    "}\n",
    "\n",
    "for word, vec in list(embeddings.items())[:3]:\n",
    "    print(f\"{word:10s}: {vec}\")\n",
    "\n",
    "print(\"\\nCosine similarity (dense):\")\n",
    "print(f\"sim(king, queen) = {1 - cosine(embeddings['king'], embeddings['queen']):.4f}\")\n",
    "print(f\"sim(king, man)   = {1 - cosine(embeddings['king'], embeddings['man']):.4f}\")\n",
    "print(f\"sim(king, apple) = {1 - cosine(embeddings['king'], embeddings['apple']):.4f}\")\n",
    "print(\"Semantic similarity captured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence Matrices and SVD\n",
    "\n",
    "### 2.1 Count-Based Embeddings\n",
    "\n",
    "Before neural approaches, embeddings were derived from co-occurrence statistics.\n",
    "\n",
    "**Word-Word Co-occurrence Matrix $X$:**\n",
    "$$X_{ij} = \\text{count of word } j \\text{ appearing in context of word } i$$\n",
    "\n",
    "**Singular Value Decomposition:**\n",
    "$$X = U \\Sigma V^T$$\n",
    "\n",
    "The top-$d$ left singular vectors give word embeddings:\n",
    "$$E = U_d \\sqrt{\\Sigma_d}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build co-occurrence matrix from corpus\n",
    "corpus = [\n",
    "    \"the king wore his crown\",\n",
    "    \"the queen wore her crown\",\n",
    "    \"the king and queen ruled the kingdom\",\n",
    "    \"a man and woman walked together\",\n",
    "    \"the man saw the king\",\n",
    "    \"the woman saw the queen\",\n",
    "    \"the crown was made of gold\",\n",
    "    \"the royal family lived in castle\",\n",
    "    \"king and queen are royal\",\n",
    "    \"man and woman are human\"\n",
    "]\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"Build vocabulary from corpus.\"\"\"\n",
    "    words = []\n",
    "    for sent in corpus:\n",
    "        words.extend(sent.lower().split())\n",
    "    word_counts = Counter(words)\n",
    "    # Filter by frequency\n",
    "    vocab = [word for word, count in word_counts.most_common() if count >= 1]\n",
    "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx2word = {i: word for word, i in word2idx.items()}\n",
    "    return vocab, word2idx, idx2word\n",
    "\n",
    "def build_cooccurrence_matrix(corpus, word2idx, window_size=2):\n",
    "    \"\"\"Build word-word co-occurrence matrix.\"\"\"\n",
    "    vocab_size = len(word2idx)\n",
    "    cooccur = np.zeros((vocab_size, vocab_size))\n",
    "    \n",
    "    for sent in corpus:\n",
    "        words = sent.lower().split()\n",
    "        for i, word in enumerate(words):\n",
    "            if word not in word2idx:\n",
    "                continue\n",
    "            word_idx = word2idx[word]\n",
    "            # Context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            for j in range(start, end):\n",
    "                if i != j and words[j] in word2idx:\n",
    "                    context_idx = word2idx[words[j]]\n",
    "                    cooccur[word_idx, context_idx] += 1\n",
    "    \n",
    "    return cooccur\n",
    "\n",
    "vocab, word2idx, idx2word = build_vocab(corpus)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Vocabulary: {vocab[:15]}...\")\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "cooccur_matrix = build_cooccurrence_matrix(corpus, word2idx, window_size=2)\n",
    "\n",
    "# Visualize subset\n",
    "words_of_interest = ['king', 'queen', 'man', 'woman', 'crown', 'royal']\n",
    "indices = [word2idx[w] for w in words_of_interest if w in word2idx]\n",
    "subset_matrix = cooccur_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(subset_matrix, cmap='YlOrRd')\n",
    "ax.set_xticks(range(len(words_of_interest)))\n",
    "ax.set_yticks(range(len(words_of_interest)))\n",
    "ax.set_xticklabels(words_of_interest, rotation=45)\n",
    "ax.set_yticklabels(words_of_interest)\n",
    "plt.colorbar(im)\n",
    "ax.set_title('Word-Word Co-occurrence Matrix')\n",
    "\n",
    "# Annotate\n",
    "for i in range(len(words_of_interest)):\n",
    "    for j in range(len(words_of_interest)):\n",
    "        ax.text(j, i, f'{subset_matrix[i, j]:.0f}', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD-based embeddings\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "def svd_embeddings(cooccur_matrix, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    Compute embeddings using Truncated SVD.\n",
    "    \n",
    "    Apply PPMI (Positive Pointwise Mutual Information) first for better results.\n",
    "    \"\"\"\n",
    "    # PPMI transformation\n",
    "    total = cooccur_matrix.sum()\n",
    "    row_sums = cooccur_matrix.sum(axis=1, keepdims=True)\n",
    "    col_sums = cooccur_matrix.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    # PMI = log(P(w,c) / (P(w) * P(c)))\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        pmi = np.log2((cooccur_matrix * total) / (row_sums * col_sums))\n",
    "        pmi = np.nan_to_num(pmi, neginf=0)\n",
    "    \n",
    "    # Positive PMI\n",
    "    ppmi = np.maximum(pmi, 0)\n",
    "    \n",
    "    # SVD\n",
    "    # For small matrices, use regular SVD\n",
    "    if ppmi.shape[0] <= embedding_dim:\n",
    "        embedding_dim = min(embedding_dim, ppmi.shape[0] - 1)\n",
    "    \n",
    "    U, S, Vt = svds(ppmi.astype(float), k=embedding_dim)\n",
    "    \n",
    "    # Sort by singular values\n",
    "    idx = np.argsort(-S)\n",
    "    U = U[:, idx]\n",
    "    S = S[idx]\n",
    "    \n",
    "    # Embeddings: U * sqrt(S)\n",
    "    embeddings = U * np.sqrt(S)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Compute SVD embeddings\n",
    "embedding_dim = min(10, len(vocab) - 1)\n",
    "svd_emb = svd_embeddings(cooccur_matrix, embedding_dim=embedding_dim)\n",
    "\n",
    "print(f\"SVD Embeddings shape: {svd_emb.shape}\")\n",
    "\n",
    "# Check similarities\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-10)\n",
    "\n",
    "print(\"\\nWord similarities (SVD embeddings):\")\n",
    "if 'king' in word2idx and 'queen' in word2idx:\n",
    "    sim = cosine_similarity(svd_emb[word2idx['king']], svd_emb[word2idx['queen']])\n",
    "    print(f\"sim(king, queen) = {sim:.4f}\")\n",
    "\n",
    "if 'man' in word2idx and 'woman' in word2idx:\n",
    "    sim = cosine_similarity(svd_emb[word2idx['man']], svd_emb[word2idx['woman']])\n",
    "    print(f\"sim(man, woman) = {sim:.4f}\")\n",
    "\n",
    "if 'king' in word2idx and 'crown' in word2idx:\n",
    "    sim = cosine_similarity(svd_emb[word2idx['king']], svd_emb[word2idx['crown']])\n",
    "    print(f\"sim(king, crown) = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec: Neural Word Embeddings\n",
    "\n",
    "### 3.1 Skip-gram Model\n",
    "\n",
    "Word2Vec (Mikolov et al., 2013) learns embeddings by predicting context words from center words.\n",
    "\n",
    "**Skip-gram Objective:**\n",
    "Given center word $w_c$, predict context words $w_o$:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{t+j} | w_t)$$\n",
    "\n",
    "where:\n",
    "$$P(w_o | w_c) = \\frac{\\exp(\\mathbf{u}_{w_o}^T \\mathbf{v}_{w_c})}{\\sum_{w \\in V} \\exp(\\mathbf{u}_w^T \\mathbf{v}_{w_c})}$$\n",
    "\n",
    "- $\\mathbf{v}_w$ : center word embedding (input)\n",
    "- $\\mathbf{u}_w$ : context word embedding (output)\n",
    "- $m$ : context window size\n",
    "\n",
    "### 3.2 Architecture\n",
    "\n",
    "```\n",
    "Input (one-hot)  →  Hidden (embedding)  →  Output (softmax)\n",
    "    [V × 1]              [d × 1]              [V × 1]\n",
    "        \\                  /                    /\n",
    "         W_in [d × V]    W_out [V × d]\n",
    "```\n",
    "\n",
    "The hidden layer weights $W_{in}$ are the word embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecSkipGram:\n",
    "    \"\"\"\n",
    "    Skip-gram Word2Vec implementation from scratch.\n",
    "    \n",
    "    Uses negative sampling for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, learning_rate=0.025):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        # Center word embeddings\n",
    "        self.W_in = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        # Context word embeddings\n",
    "        self.W_out = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Numerically stable softmax.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def forward(self, center_idx):\n",
    "        \"\"\"\n",
    "        Forward pass: compute probability distribution over vocabulary.\n",
    "        \n",
    "        P(w_o | w_c) = softmax(W_out @ v_c)\n",
    "        \"\"\"\n",
    "        # Get center word embedding\n",
    "        v_c = self.W_in[center_idx]  # [embedding_dim]\n",
    "        \n",
    "        # Compute scores for all words\n",
    "        scores = self.W_out @ v_c  # [vocab_size]\n",
    "        \n",
    "        # Softmax probabilities\n",
    "        probs = self.softmax(scores)\n",
    "        \n",
    "        return probs, v_c\n",
    "    \n",
    "    def train_step(self, center_idx, context_idx):\n",
    "        \"\"\"\n",
    "        Single training step with vanilla softmax.\n",
    "        \n",
    "        Note: This is O(V) per step - expensive!\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        probs, v_c = self.forward(center_idx)\n",
    "        \n",
    "        # Compute loss: -log P(w_o | w_c)\n",
    "        loss = -np.log(probs[context_idx] + 1e-10)\n",
    "        \n",
    "        # Backward pass\n",
    "        # Gradient w.r.t. scores: softmax - one_hot\n",
    "        grad_scores = probs.copy()\n",
    "        grad_scores[context_idx] -= 1  # [vocab_size]\n",
    "        \n",
    "        # Gradient w.r.t. W_out: outer product\n",
    "        grad_W_out = np.outer(grad_scores, v_c)  # [vocab_size, embedding_dim]\n",
    "        \n",
    "        # Gradient w.r.t. v_c (center embedding)\n",
    "        grad_v_c = self.W_out.T @ grad_scores  # [embedding_dim]\n",
    "        \n",
    "        # Update parameters\n",
    "        self.W_out -= self.lr * grad_W_out\n",
    "        self.W_in[center_idx] -= self.lr * grad_v_c\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        \"\"\"Get embedding for a word.\"\"\"\n",
    "        return self.W_in[word_idx]\n",
    "    \n",
    "    def most_similar(self, word_idx, word2idx, idx2word, top_k=5):\n",
    "        \"\"\"Find most similar words.\"\"\"\n",
    "        query_vec = self.W_in[word_idx]\n",
    "        \n",
    "        similarities = []\n",
    "        for i in range(self.vocab_size):\n",
    "            if i != word_idx:\n",
    "                sim = cosine_similarity(query_vec, self.W_in[i])\n",
    "                similarities.append((i, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [(idx2word[idx], sim) for idx, sim in similarities[:top_k]]\n",
    "\n",
    "# Demo\n",
    "print(\"Skip-gram Word2Vec Architecture:\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Embedding dimension: 50\")\n",
    "\n",
    "model = Word2VecSkipGram(len(vocab), embedding_dim=50)\n",
    "print(f\"\\nW_in (center embeddings) shape: {model.W_in.shape}\")\n",
    "print(f\"W_out (context embeddings) shape: {model.W_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Negative Sampling\n",
    "\n",
    "The softmax over entire vocabulary is expensive: $O(V)$ per training sample.\n",
    "\n",
    "**Negative Sampling Objective:**\n",
    "Instead of softmax, use binary classification:\n",
    "\n",
    "$$J_{\\text{neg}} = \\log \\sigma(\\mathbf{u}_{w_o}^T \\mathbf{v}_{w_c}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} [\\log \\sigma(-\\mathbf{u}_{w_i}^T \\mathbf{v}_{w_c})]$$\n",
    "\n",
    "where:\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ (sigmoid)\n",
    "- $k$ : number of negative samples (typically 5-20)\n",
    "- $P_n(w) \\propto f(w)^{3/4}$ : noise distribution (smoothed unigram)\n",
    "\n",
    "**Intuition:** Push positive pairs together, push negative pairs apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecNegSampling:\n",
    "    \"\"\"\n",
    "    Word2Vec with Negative Sampling.\n",
    "    \n",
    "    Much more efficient than softmax: O(k) instead of O(V).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, learning_rate=0.025, neg_samples=5):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = learning_rate\n",
    "        self.neg_samples = neg_samples\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.W_in = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.W_out = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # Noise distribution (will be set from corpus)\n",
    "        self.noise_dist = np.ones(vocab_size) / vocab_size\n",
    "    \n",
    "    def set_noise_distribution(self, word_counts):\n",
    "        \"\"\"\n",
    "        Set noise distribution based on word frequencies.\n",
    "        \n",
    "        P_n(w) ∝ count(w)^0.75\n",
    "        \"\"\"\n",
    "        counts = np.array(word_counts) ** 0.75\n",
    "        self.noise_dist = counts / counts.sum()\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "        return np.where(\n",
    "            x >= 0,\n",
    "            1 / (1 + np.exp(-x)),\n",
    "            np.exp(x) / (1 + np.exp(x))\n",
    "        )\n",
    "    \n",
    "    def sample_negatives(self, positive_idx):\n",
    "        \"\"\"Sample negative words.\"\"\"\n",
    "        negatives = []\n",
    "        while len(negatives) < self.neg_samples:\n",
    "            neg_idx = np.random.choice(self.vocab_size, p=self.noise_dist)\n",
    "            if neg_idx != positive_idx:\n",
    "                negatives.append(neg_idx)\n",
    "        return negatives\n",
    "    \n",
    "    def train_step(self, center_idx, context_idx):\n",
    "        \"\"\"\n",
    "        Training step with negative sampling.\n",
    "        \n",
    "        Maximize: log σ(u_o · v_c) + Σ log σ(-u_neg · v_c)\n",
    "        \"\"\"\n",
    "        # Get center word embedding\n",
    "        v_c = self.W_in[center_idx]\n",
    "        \n",
    "        # Positive sample\n",
    "        u_pos = self.W_out[context_idx]\n",
    "        pos_score = np.dot(u_pos, v_c)\n",
    "        pos_prob = self.sigmoid(pos_score)\n",
    "        \n",
    "        # Loss for positive: -log σ(score)\n",
    "        loss = -np.log(pos_prob + 1e-10)\n",
    "        \n",
    "        # Gradient for positive sample\n",
    "        # d/dx [-log σ(x)] = σ(x) - 1\n",
    "        grad_v_c = (pos_prob - 1) * u_pos\n",
    "        grad_u_pos = (pos_prob - 1) * v_c\n",
    "        \n",
    "        # Negative samples\n",
    "        negative_indices = self.sample_negatives(context_idx)\n",
    "        \n",
    "        for neg_idx in negative_indices:\n",
    "            u_neg = self.W_out[neg_idx]\n",
    "            neg_score = np.dot(u_neg, v_c)\n",
    "            neg_prob = self.sigmoid(neg_score)\n",
    "            \n",
    "            # Loss for negative: -log σ(-score) = -log(1 - σ(score))\n",
    "            loss += -np.log(1 - neg_prob + 1e-10)\n",
    "            \n",
    "            # Gradient for negative sample\n",
    "            # d/dx [-log(1-σ(x))] = σ(x)\n",
    "            grad_v_c += neg_prob * u_neg\n",
    "            grad_u_neg = neg_prob * v_c\n",
    "            \n",
    "            # Update negative context embedding\n",
    "            self.W_out[neg_idx] -= self.lr * grad_u_neg\n",
    "        \n",
    "        # Update embeddings\n",
    "        self.W_in[center_idx] -= self.lr * grad_v_c\n",
    "        self.W_out[context_idx] -= self.lr * grad_u_pos\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        return self.W_in[word_idx]\n",
    "    \n",
    "    def most_similar(self, word_idx, idx2word, top_k=5):\n",
    "        \"\"\"Find most similar words using cosine similarity.\"\"\"\n",
    "        query_vec = self.W_in[word_idx]\n",
    "        \n",
    "        # Compute all similarities at once\n",
    "        norms = np.linalg.norm(self.W_in, axis=1)\n",
    "        query_norm = np.linalg.norm(query_vec)\n",
    "        \n",
    "        similarities = (self.W_in @ query_vec) / (norms * query_norm + 1e-10)\n",
    "        \n",
    "        # Get top-k (excluding self)\n",
    "        top_indices = np.argsort(-similarities)\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if idx != word_idx and len(results) < top_k:\n",
    "                results.append((idx2word[idx], similarities[idx]))\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"Word2Vec with Negative Sampling initialized\")\n",
    "print(f\"Number of negative samples: 5\")\n",
    "print(f\"Complexity per step: O(k) = O(5) instead of O(V) = O({len(vocab)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec on our small corpus\n",
    "def generate_training_data(corpus, word2idx, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate (center, context) pairs for skip-gram.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for sent in corpus:\n",
    "        words = sent.lower().split()\n",
    "        word_indices = [word2idx.get(w, -1) for w in words]\n",
    "        \n",
    "        for i, center_idx in enumerate(word_indices):\n",
    "            if center_idx == -1:\n",
    "                continue\n",
    "            \n",
    "            # Context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(word_indices), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j and word_indices[j] != -1:\n",
    "                    pairs.append((center_idx, word_indices[j]))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Generate training pairs\n",
    "training_pairs = generate_training_data(corpus, word2idx, window_size=2)\n",
    "print(f\"Number of training pairs: {len(training_pairs)}\")\n",
    "print(f\"\\nSample pairs:\")\n",
    "for i in range(min(5, len(training_pairs))):\n",
    "    center_idx, context_idx = training_pairs[i]\n",
    "    print(f\"  ({idx2word[center_idx]}, {idx2word[context_idx]})\")\n",
    "\n",
    "# Get word frequencies for noise distribution\n",
    "word_counts = np.zeros(len(vocab))\n",
    "for sent in corpus:\n",
    "    for word in sent.lower().split():\n",
    "        if word in word2idx:\n",
    "            word_counts[word2idx[word]] += 1\n",
    "\n",
    "# Initialize and train model\n",
    "w2v_model = Word2VecNegSampling(len(vocab), embedding_dim=20, learning_rate=0.1, neg_samples=5)\n",
    "w2v_model.set_noise_distribution(word_counts)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "print(f\"\\nTraining Word2Vec for {num_epochs} epochs...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    np.random.shuffle(training_pairs)\n",
    "    \n",
    "    for center_idx, context_idx in training_pairs:\n",
    "        loss = w2v_model.train_step(center_idx, context_idx)\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    avg_loss = epoch_loss / len(training_pairs)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Word2Vec Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate learned embeddings\n",
    "print(\"Learned Word Similarities:\\n\")\n",
    "\n",
    "test_words = ['king', 'queen', 'man', 'woman', 'crown']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in word2idx:\n",
    "        similar = w2v_model.most_similar(word2idx[word], idx2word, top_k=3)\n",
    "        print(f\"Most similar to '{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word}: {score:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Check specific similarities\n",
    "print(\"\\nSemantic Similarity Scores:\")\n",
    "pairs_to_check = [\n",
    "    ('king', 'queen'),\n",
    "    ('man', 'woman'),\n",
    "    ('king', 'crown'),\n",
    "    ('queen', 'crown'),\n",
    "    ('king', 'the')\n",
    "]\n",
    "\n",
    "for word1, word2 in pairs_to_check:\n",
    "    if word1 in word2idx and word2 in word2idx:\n",
    "        vec1 = w2v_model.get_embedding(word2idx[word1])\n",
    "        vec2 = w2v_model.get_embedding(word2idx[word2])\n",
    "        sim = cosine_similarity(vec1, vec2)\n",
    "        print(f\"sim({word1}, {word2}) = {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CBOW (Continuous Bag of Words)\n",
    "\n",
    "### 4.1 Architecture\n",
    "\n",
    "CBOW is the \"inverse\" of Skip-gram: predict center word from context.\n",
    "\n",
    "**CBOW Objective:**\n",
    "$$J(\\theta) = -\\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_{t-m}, \\ldots, w_{t+m})$$\n",
    "\n",
    "**Context Representation:**\n",
    "Average of context word embeddings:\n",
    "$$\\mathbf{h} = \\frac{1}{2m} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\mathbf{v}_{w_{t+j}}$$\n",
    "\n",
    "**Prediction:**\n",
    "$$P(w_t | \\text{context}) = \\frac{\\exp(\\mathbf{u}_{w_t}^T \\mathbf{h})}{\\sum_{w \\in V} \\exp(\\mathbf{u}_w^T \\mathbf{h})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecCBOW:\n",
    "    \"\"\"\n",
    "    CBOW Word2Vec with Negative Sampling.\n",
    "    \n",
    "    Predict center word from context words.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, learning_rate=0.025, neg_samples=5):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = learning_rate\n",
    "        self.neg_samples = neg_samples\n",
    "        \n",
    "        # Embeddings\n",
    "        self.W_in = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.W_out = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # Noise distribution\n",
    "        self.noise_dist = np.ones(vocab_size) / vocab_size\n",
    "    \n",
    "    def set_noise_distribution(self, word_counts):\n",
    "        counts = np.array(word_counts) ** 0.75\n",
    "        self.noise_dist = counts / counts.sum()\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return np.where(\n",
    "            x >= 0,\n",
    "            1 / (1 + np.exp(-x)),\n",
    "            np.exp(x) / (1 + np.exp(x))\n",
    "        )\n",
    "    \n",
    "    def sample_negatives(self, positive_idx):\n",
    "        negatives = []\n",
    "        while len(negatives) < self.neg_samples:\n",
    "            neg_idx = np.random.choice(self.vocab_size, p=self.noise_dist)\n",
    "            if neg_idx != positive_idx:\n",
    "                negatives.append(neg_idx)\n",
    "        return negatives\n",
    "    \n",
    "    def train_step(self, context_indices, center_idx):\n",
    "        \"\"\"\n",
    "        Training step: predict center word from context.\n",
    "        \n",
    "        context_indices: list of word indices in context\n",
    "        center_idx: target word to predict\n",
    "        \"\"\"\n",
    "        # Average context embeddings\n",
    "        h = np.mean(self.W_in[context_indices], axis=0)\n",
    "        \n",
    "        # Positive sample\n",
    "        u_pos = self.W_out[center_idx]\n",
    "        pos_score = np.dot(u_pos, h)\n",
    "        pos_prob = self.sigmoid(pos_score)\n",
    "        \n",
    "        loss = -np.log(pos_prob + 1e-10)\n",
    "        \n",
    "        # Gradients\n",
    "        grad_h = (pos_prob - 1) * u_pos\n",
    "        grad_u_pos = (pos_prob - 1) * h\n",
    "        \n",
    "        # Negative samples\n",
    "        negative_indices = self.sample_negatives(center_idx)\n",
    "        \n",
    "        for neg_idx in negative_indices:\n",
    "            u_neg = self.W_out[neg_idx]\n",
    "            neg_score = np.dot(u_neg, h)\n",
    "            neg_prob = self.sigmoid(neg_score)\n",
    "            \n",
    "            loss += -np.log(1 - neg_prob + 1e-10)\n",
    "            grad_h += neg_prob * u_neg\n",
    "            grad_u_neg = neg_prob * h\n",
    "            \n",
    "            self.W_out[neg_idx] -= self.lr * grad_u_neg\n",
    "        \n",
    "        # Update context word embeddings (distribute gradient equally)\n",
    "        grad_per_context = grad_h / len(context_indices)\n",
    "        for ctx_idx in context_indices:\n",
    "            self.W_in[ctx_idx] -= self.lr * grad_per_context\n",
    "        \n",
    "        self.W_out[center_idx] -= self.lr * grad_u_pos\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        return self.W_in[word_idx]\n",
    "\n",
    "# Generate CBOW training data\n",
    "def generate_cbow_data(corpus, word2idx, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate (context_words, center_word) pairs for CBOW.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for sent in corpus:\n",
    "        words = sent.lower().split()\n",
    "        word_indices = [word2idx.get(w, -1) for w in words]\n",
    "        \n",
    "        for i, center_idx in enumerate(word_indices):\n",
    "            if center_idx == -1:\n",
    "                continue\n",
    "            \n",
    "            # Collect context\n",
    "            context = []\n",
    "            for j in range(max(0, i - window_size), min(len(word_indices), i + window_size + 1)):\n",
    "                if i != j and word_indices[j] != -1:\n",
    "                    context.append(word_indices[j])\n",
    "            \n",
    "            if len(context) > 0:\n",
    "                data.append((context, center_idx))\n",
    "    \n",
    "    return data\n",
    "\n",
    "cbow_data = generate_cbow_data(corpus, word2idx, window_size=2)\n",
    "print(f\"CBOW training samples: {len(cbow_data)}\")\n",
    "print(f\"\\nSample CBOW data:\")\n",
    "for i in range(min(3, len(cbow_data))):\n",
    "    context, center = cbow_data[i]\n",
    "    context_words = [idx2word[idx] for idx in context]\n",
    "    print(f\"  Context: {context_words} -> Center: {idx2word[center]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Skip-gram vs CBOW\n",
    "\n",
    "| Aspect | Skip-gram | CBOW |\n",
    "|--------|-----------|------|\n",
    "| Task | Predict context from center | Predict center from context |\n",
    "| Training | Slower (more samples) | Faster |\n",
    "| Rare words | Better representation | Worse (averaged out) |\n",
    "| Frequent words | Worse | Better |\n",
    "| Semantic relationships | Better | Worse |\n",
    "| Syntactic relationships | Worse | Better |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GloVe: Global Vectors\n",
    "\n",
    "### 5.1 Theory\n",
    "\n",
    "GloVe (Pennington et al., 2014) combines the benefits of:\n",
    "- Global matrix factorization (SVD)\n",
    "- Local context window (Word2Vec)\n",
    "\n",
    "**Key Insight:** Word embeddings should encode ratios of co-occurrence probabilities.\n",
    "\n",
    "$$\\frac{P(k | \\text{ice})}{P(k | \\text{steam})} = \\begin{cases}\n",
    "\\text{large} & k = \\text{solid} \\\\\n",
    "\\text{small} & k = \\text{gas} \\\\\n",
    "\\approx 1 & k = \\text{water}, \\text{fashion}\n",
    "\\end{cases}$$\n",
    "\n",
    "**GloVe Objective:**\n",
    "$$J = \\sum_{i,j=1}^{V} f(X_{ij})(\\mathbf{w}_i^T \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2$$\n",
    "\n",
    "where:\n",
    "- $X_{ij}$ : co-occurrence count\n",
    "- $\\mathbf{w}_i, \\tilde{\\mathbf{w}}_j$ : word and context embeddings\n",
    "- $b_i, \\tilde{b}_j$ : bias terms\n",
    "- $f(x)$ : weighting function to down-weight frequent co-occurrences\n",
    "\n",
    "**Weighting Function:**\n",
    "$$f(x) = \\begin{cases}\n",
    "(x / x_{\\max})^\\alpha & x < x_{\\max} \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $\\alpha = 0.75$ and $x_{\\max} = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe:\n",
    "    \"\"\"\n",
    "    GloVe: Global Vectors for Word Representation.\n",
    "    \n",
    "    Learns embeddings from co-occurrence statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, x_max=100, alpha=0.75, learning_rate=0.05):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.W = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        # Context embeddings\n",
    "        self.W_tilde = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        # Biases\n",
    "        self.b = np.zeros(vocab_size)\n",
    "        self.b_tilde = np.zeros(vocab_size)\n",
    "        \n",
    "        # AdaGrad parameters (sum of squared gradients)\n",
    "        self.gradsq_W = np.ones((vocab_size, embedding_dim))\n",
    "        self.gradsq_W_tilde = np.ones((vocab_size, embedding_dim))\n",
    "        self.gradsq_b = np.ones(vocab_size)\n",
    "        self.gradsq_b_tilde = np.ones(vocab_size)\n",
    "    \n",
    "    def weighting_function(self, x):\n",
    "        \"\"\"Weighting function f(x).\"\"\"\n",
    "        if x < self.x_max:\n",
    "            return (x / self.x_max) ** self.alpha\n",
    "        return 1.0\n",
    "    \n",
    "    def train_step(self, i, j, x_ij):\n",
    "        \"\"\"\n",
    "        Single training step for pair (i, j) with co-occurrence x_ij.\n",
    "        \n",
    "        Uses AdaGrad for adaptive learning rate.\n",
    "        \"\"\"\n",
    "        # Compute inner product and difference from log co-occurrence\n",
    "        diff = np.dot(self.W[i], self.W_tilde[j]) + self.b[i] + self.b_tilde[j] - np.log(x_ij + 1e-10)\n",
    "        \n",
    "        # Weight\n",
    "        f_x = self.weighting_function(x_ij)\n",
    "        \n",
    "        # Weighted squared error loss\n",
    "        loss = f_x * diff ** 2\n",
    "        \n",
    "        # Gradients\n",
    "        grad_common = f_x * diff\n",
    "        \n",
    "        grad_W_i = grad_common * self.W_tilde[j]\n",
    "        grad_W_tilde_j = grad_common * self.W[i]\n",
    "        grad_b_i = grad_common\n",
    "        grad_b_tilde_j = grad_common\n",
    "        \n",
    "        # AdaGrad update\n",
    "        self.gradsq_W[i] += grad_W_i ** 2\n",
    "        self.gradsq_W_tilde[j] += grad_W_tilde_j ** 2\n",
    "        self.gradsq_b[i] += grad_b_i ** 2\n",
    "        self.gradsq_b_tilde[j] += grad_b_tilde_j ** 2\n",
    "        \n",
    "        # Parameter updates\n",
    "        self.W[i] -= self.lr * grad_W_i / np.sqrt(self.gradsq_W[i])\n",
    "        self.W_tilde[j] -= self.lr * grad_W_tilde_j / np.sqrt(self.gradsq_W_tilde[j])\n",
    "        self.b[i] -= self.lr * grad_b_i / np.sqrt(self.gradsq_b[i])\n",
    "        self.b_tilde[j] -= self.lr * grad_b_tilde_j / np.sqrt(self.gradsq_b_tilde[j])\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self, cooccurrence_matrix, num_epochs=50):\n",
    "        \"\"\"\n",
    "        Train GloVe on co-occurrence matrix.\n",
    "        \"\"\"\n",
    "        # Get non-zero entries\n",
    "        nonzero = np.nonzero(cooccurrence_matrix)\n",
    "        indices = list(zip(nonzero[0], nonzero[1]))\n",
    "        \n",
    "        print(f\"Training on {len(indices)} non-zero co-occurrences\")\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i, j in indices:\n",
    "                x_ij = cooccurrence_matrix[i, j]\n",
    "                if x_ij > 0:\n",
    "                    loss = self.train_step(i, j, x_ij)\n",
    "                    epoch_loss += loss\n",
    "            \n",
    "            avg_loss = epoch_loss / len(indices)\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        \"\"\"Get final embedding (sum of W and W_tilde).\"\"\"\n",
    "        return self.W[word_idx] + self.W_tilde[word_idx]\n",
    "    \n",
    "    def get_all_embeddings(self):\n",
    "        \"\"\"Get all embeddings.\"\"\"\n",
    "        return self.W + self.W_tilde\n",
    "\n",
    "# Train GloVe\n",
    "print(\"Training GloVe model...\\n\")\n",
    "glove_model = GloVe(len(vocab), embedding_dim=20, learning_rate=0.1)\n",
    "glove_losses = glove_model.train(cooccur_matrix, num_epochs=100)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(glove_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('GloVe Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GloVe similarities\n",
    "print(\"GloVe Word Similarities:\\n\")\n",
    "\n",
    "for word in ['king', 'queen', 'man', 'woman']:\n",
    "    if word in word2idx:\n",
    "        query_vec = glove_model.get_embedding(word2idx[word])\n",
    "        \n",
    "        # Find similar words\n",
    "        similarities = []\n",
    "        for other_word, other_idx in word2idx.items():\n",
    "            if other_word != word:\n",
    "                other_vec = glove_model.get_embedding(other_idx)\n",
    "                sim = cosine_similarity(query_vec, other_vec)\n",
    "                similarities.append((other_word, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"Most similar to '{word}':\")\n",
    "        for sim_word, score in similarities[:3]:\n",
    "            print(f\"  {sim_word}: {score:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Arithmetic and Analogies\n",
    "\n",
    "### 6.1 The Famous Equation\n",
    "\n",
    "$$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$$\n",
    "\n",
    "**Mathematical Interpretation:**\n",
    "If embeddings capture semantic relationships, then:\n",
    "- $\\mathbf{v}_{\\text{king}} - \\mathbf{v}_{\\text{man}}$ = \"royalty\" concept\n",
    "- Adding $\\mathbf{v}_{\\text{woman}}$ = female + royalty = queen\n",
    "\n",
    "**3CosAdd Method:**\n",
    "$$\\text{argmax}_{b^*} \\cos(b^*, b - a + a^*)$$\n",
    "\n",
    "Find word $b^*$ such that $a : a^* :: b : b^*$\n",
    "\n",
    "**3CosMul Method (often better):**\n",
    "$$\\text{argmax}_{b^*} \\frac{\\cos(b^*, b) \\cdot \\cos(b^*, a^*)}{\\cos(b^*, a) + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_3cosadd(a, a_star, b, embeddings, word2idx, idx2word):\n",
    "    \"\"\"\n",
    "    Solve analogy: a is to a* as b is to ?\n",
    "    \n",
    "    Using 3CosAdd: argmax cos(v, b - a + a*)\n",
    "    \"\"\"\n",
    "    if a not in word2idx or a_star not in word2idx or b not in word2idx:\n",
    "        return None\n",
    "    \n",
    "    # Get embeddings\n",
    "    v_a = embeddings[word2idx[a]]\n",
    "    v_a_star = embeddings[word2idx[a_star]]\n",
    "    v_b = embeddings[word2idx[b]]\n",
    "    \n",
    "    # Compute target vector\n",
    "    target = v_b - v_a + v_a_star\n",
    "    \n",
    "    # Find most similar (excluding input words)\n",
    "    exclude = {word2idx[a], word2idx[a_star], word2idx[b]}\n",
    "    \n",
    "    best_word = None\n",
    "    best_sim = -float('inf')\n",
    "    \n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx not in exclude:\n",
    "            sim = cosine_similarity(target, embeddings[idx])\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_word = idx2word[idx]\n",
    "    \n",
    "    return best_word, best_sim\n",
    "\n",
    "def analogy_3cosmul(a, a_star, b, embeddings, word2idx, idx2word, eps=0.001):\n",
    "    \"\"\"\n",
    "    Solve analogy using 3CosMul.\n",
    "    \n",
    "    argmax (cos(v, b) * cos(v, a*)) / (cos(v, a) + eps)\n",
    "    \"\"\"\n",
    "    if a not in word2idx or a_star not in word2idx or b not in word2idx:\n",
    "        return None\n",
    "    \n",
    "    v_a = embeddings[word2idx[a]]\n",
    "    v_a_star = embeddings[word2idx[a_star]]\n",
    "    v_b = embeddings[word2idx[b]]\n",
    "    \n",
    "    exclude = {word2idx[a], word2idx[a_star], word2idx[b]}\n",
    "    \n",
    "    best_word = None\n",
    "    best_score = -float('inf')\n",
    "    \n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx not in exclude:\n",
    "            v = embeddings[idx]\n",
    "            \n",
    "            # Compute similarities (shifted to [0, 1])\n",
    "            cos_b = (cosine_similarity(v, v_b) + 1) / 2\n",
    "            cos_a_star = (cosine_similarity(v, v_a_star) + 1) / 2\n",
    "            cos_a = (cosine_similarity(v, v_a) + 1) / 2\n",
    "            \n",
    "            score = (cos_b * cos_a_star) / (cos_a + eps)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_word = idx2word[idx]\n",
    "    \n",
    "    return best_word, best_score\n",
    "\n",
    "# Test analogies on our small corpus\n",
    "print(\"Word Analogies:\\n\")\n",
    "\n",
    "# Get embeddings from trained models\n",
    "w2v_embeddings = w2v_model.W_in\n",
    "glove_embeddings = glove_model.get_all_embeddings()\n",
    "\n",
    "# Test: man -> woman as king -> ?\n",
    "analogies = [\n",
    "    ('man', 'woman', 'king'),  # king -> ?\n",
    "    ('woman', 'man', 'queen'), # queen -> ?\n",
    "]\n",
    "\n",
    "print(\"Word2Vec Analogies (3CosAdd):\")\n",
    "for a, a_star, b in analogies:\n",
    "    result = analogy_3cosadd(a, a_star, b, w2v_embeddings, word2idx, idx2word)\n",
    "    if result:\n",
    "        word, score = result\n",
    "        print(f\"  {a} : {a_star} :: {b} : {word} (sim={score:.4f})\")\n",
    "\n",
    "print(\"\\nGloVe Analogies (3CosAdd):\")\n",
    "for a, a_star, b in analogies:\n",
    "    result = analogy_3cosadd(a, a_star, b, glove_embeddings, word2idx, idx2word)\n",
    "    if result:\n",
    "        word, score = result\n",
    "        print(f\"  {a} : {a_star} :: {b} : {word} (sim={score:.4f})\")\n",
    "\n",
    "print(\"\\nNote: Results limited by small corpus size. With larger datasets,\")\n",
    "print(\"these methods produce remarkable semantic relationships!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Embeddings\n",
    "\n",
    "### 7.1 Dimensionality Reduction\n",
    "\n",
    "To visualize high-dimensional embeddings in 2D/3D:\n",
    "\n",
    "**PCA (Principal Component Analysis):**\n",
    "- Linear projection to top principal components\n",
    "- Preserves global structure\n",
    "- Fast, deterministic\n",
    "\n",
    "**t-SNE (t-Distributed Stochastic Neighbor Embedding):**\n",
    "- Non-linear dimensionality reduction\n",
    "- Preserves local structure (clusters)\n",
    "- Better for visualization but slower\n",
    "\n",
    "**UMAP (Uniform Manifold Approximation and Projection):**\n",
    "- Preserves both local and global structure\n",
    "- Faster than t-SNE\n",
    "- Often preferred for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings using PCA and t-SNE\n",
    "\n",
    "# Use Word2Vec embeddings\n",
    "all_embeddings = w2v_model.W_in\n",
    "words = [idx2word[i] for i in range(len(vocab))]\n",
    "\n",
    "# PCA reduction\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(all_embeddings)\n",
    "\n",
    "# t-SNE reduction\n",
    "# Note: t-SNE requires perplexity < n_samples\n",
    "perplexity = min(5, len(vocab) - 1)\n",
    "tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_iter=1000)\n",
    "embeddings_tsne = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PCA plot\n",
    "axes[0].scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], alpha=0.7)\n",
    "for i, word in enumerate(words):\n",
    "    axes[0].annotate(word, (embeddings_pca[i, 0], embeddings_pca[i, 1]),\n",
    "                      fontsize=9, alpha=0.8)\n",
    "axes[0].set_title('Word Embeddings - PCA Projection')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE plot\n",
    "axes[1].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], alpha=0.7)\n",
    "for i, word in enumerate(words):\n",
    "    axes[1].annotate(word, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                      fontsize=9, alpha=0.8)\n",
    "axes[1].set_title('Word Embeddings - t-SNE Projection')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObserve how semantically related words cluster together!\")\n",
    "print(\"With larger corpora, these clusters become much more pronounced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using Pretrained Embeddings\n",
    "\n",
    "### 8.1 Why Pretrained?\n",
    "\n",
    "Training embeddings requires massive corpora:\n",
    "- Original Word2Vec: Google News (~100B words)\n",
    "- GloVe: Common Crawl (840B tokens)\n",
    "- fastText: Wikipedia + Common Crawl\n",
    "\n",
    "**Advantages of Pretrained Embeddings:**\n",
    "1. Rich semantic representations from large corpora\n",
    "2. No need for expensive training\n",
    "3. Transfer learning to downstream tasks\n",
    "4. Better generalization\n",
    "\n",
    "### 8.2 Loading Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating pretrained embeddings loading\n",
    "# In practice, you'd download from:\n",
    "# - GloVe: https://nlp.stanford.edu/projects/glove/\n",
    "# - Word2Vec: https://code.google.com/archive/p/word2vec/\n",
    "# - fastText: https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "def load_glove_embeddings(file_path, vocab_limit=None):\n",
    "    \"\"\"\n",
    "    Load pretrained GloVe embeddings from file.\n",
    "    \n",
    "    File format: word dim1 dim2 ... dimN\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if vocab_limit and i >= vocab_limit:\n",
    "                break\n",
    "            \n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Example of creating an embedding matrix for a specific vocabulary\n",
    "def create_embedding_matrix(word2idx, pretrained_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a given vocabulary.\n",
    "    \n",
    "    Initialize unknown words randomly.\n",
    "    \"\"\"\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "    \n",
    "    found_count = 0\n",
    "    for word, idx in word2idx.items():\n",
    "        if word in pretrained_embeddings:\n",
    "            embedding_matrix[idx] = pretrained_embeddings[word]\n",
    "            found_count += 1\n",
    "    \n",
    "    coverage = found_count / vocab_size * 100\n",
    "    print(f\"Vocabulary coverage: {found_count}/{vocab_size} ({coverage:.1f}%)\")\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "print(\"Pretrained Embedding Loading Functions\")\n",
    "print(\"\\nIn practice, you would:\")\n",
    "print(\"1. Download pretrained embeddings (e.g., glove.6B.300d.txt)\")\n",
    "print(\"2. Load them using load_glove_embeddings()\")\n",
    "print(\"3. Create embedding matrix for your task vocabulary\")\n",
    "print(\"4. Use in neural network as embedding layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch integration example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple text classifier with pretrained embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Load pretrained weights\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(\n",
    "                torch.from_numpy(pretrained_embeddings)\n",
    "            )\n",
    "            if freeze_embeddings:\n",
    "                self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        # Classifier\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len] token indices\n",
    "        \"\"\"\n",
    "        # Embed tokens: [batch_size, seq_len, embedding_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Average pooling over sequence\n",
    "        pooled = embedded.mean(dim=1)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Classify\n",
    "        hidden = self.relu(self.fc1(pooled))\n",
    "        output = self.fc2(hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Demo\n",
    "vocab_size = 1000\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "num_classes = 2\n",
    "\n",
    "# Simulate pretrained embeddings\n",
    "pretrained = np.random.randn(vocab_size, embedding_dim).astype(np.float32)\n",
    "\n",
    "# Create model\n",
    "model = TextClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    pretrained_embeddings=pretrained,\n",
    "    freeze_embeddings=False  # Set True to keep embeddings fixed\n",
    ")\n",
    "\n",
    "print(\"Text Classifier Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 4\n",
    "seq_len = 20\n",
    "sample_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "output = model(sample_input)\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Limitations of Static Embeddings\n",
    "\n",
    "### 9.1 Polysemy Problem\n",
    "\n",
    "Static embeddings assign ONE vector per word type, regardless of context:\n",
    "\n",
    "**Example: \"bank\"**\n",
    "- \"I deposited money at the **bank**\" (financial institution)\n",
    "- \"The river **bank** was muddy\" (riverside)\n",
    "\n",
    "Both senses get merged into a single vector!\n",
    "\n",
    "### 9.2 Other Limitations\n",
    "\n",
    "1. **No context sensitivity**: Same embedding regardless of surrounding words\n",
    "2. **Out-of-vocabulary (OOV)**: No representation for unseen words\n",
    "3. **Morphology ignored**: \"run\", \"runs\", \"running\" are separate\n",
    "4. **Bias encoded**: Societal biases in training data are learned\n",
    "\n",
    "### 9.3 Solutions (Preview of Days 14-15)\n",
    "\n",
    "- **fastText**: Subword embeddings (handles OOV)\n",
    "- **ELMo**: Contextualized embeddings\n",
    "- **BERT**: Bidirectional contextualized representations\n",
    "- **GPT**: Autoregressive language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the polysemy problem\n",
    "\n",
    "# Hypothetical single embedding for \"bank\"\n",
    "print(\"The Polysemy Problem:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sentences with different meanings of \"bank\"\n",
    "sentences = [\n",
    "    \"I need to go to the bank to deposit my check\",\n",
    "    \"The river bank was covered with wildflowers\",\n",
    "    \"The pilot decided to bank the aircraft left\",\n",
    "    \"Don't bank on him coming to the party\"\n",
    "]\n",
    "\n",
    "print(\"Different meanings of 'bank':\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n",
    "\n",
    "print(\"\\nStatic embedding issue:\")\n",
    "print(\"All these different meanings map to a SINGLE vector!\")\n",
    "print(\"\\nContextualized embeddings (ELMo, BERT) solve this by\")\n",
    "print(\"computing different vectors based on context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bias in Word Embeddings\n",
    "\n",
    "### 10.1 Gender Bias Example\n",
    "\n",
    "Word embeddings trained on large corpora encode societal biases:\n",
    "\n",
    "$$\\text{man} - \\text{woman} \\approx \\text{doctor} - \\text{nurse}$$\n",
    "$$\\text{man} - \\text{woman} \\approx \\text{computer programmer} - \\text{homemaker}$$\n",
    "\n",
    "### 10.2 Debiasing Techniques\n",
    "\n",
    "1. **Hard Debiasing** (Bolukbasi et al., 2016):\n",
    "   - Identify gender direction\n",
    "   - Project out gender component from neutral words\n",
    "   - Equalize gender pairs\n",
    "\n",
    "2. **Counterfactual Data Augmentation**:\n",
    "   - Swap gendered words in training data\n",
    "   - Balance gender representation\n",
    "\n",
    "3. **Adversarial Learning**:\n",
    "   - Train embeddings to be unpredictive of protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple debiasing illustration\n",
    "\n",
    "def identify_bias_direction(embeddings, word2idx, gendered_pairs):\n",
    "    \"\"\"\n",
    "    Identify the gender direction from gendered word pairs.\n",
    "    \n",
    "    gendered_pairs: list of (male_word, female_word) tuples\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "    \n",
    "    for male_word, female_word in gendered_pairs:\n",
    "        if male_word in word2idx and female_word in word2idx:\n",
    "            diff = embeddings[word2idx[male_word]] - embeddings[word2idx[female_word]]\n",
    "            differences.append(diff)\n",
    "    \n",
    "    if not differences:\n",
    "        return None\n",
    "    \n",
    "    # PCA to get principal direction\n",
    "    diff_matrix = np.array(differences)\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(diff_matrix)\n",
    "    gender_direction = pca.components_[0]\n",
    "    \n",
    "    return gender_direction / np.linalg.norm(gender_direction)\n",
    "\n",
    "def debias_word(embedding, bias_direction):\n",
    "    \"\"\"\n",
    "    Remove bias component from word embedding.\n",
    "    \n",
    "    Project out the bias direction.\n",
    "    \"\"\"\n",
    "    # Component along bias direction\n",
    "    bias_component = np.dot(embedding, bias_direction) * bias_direction\n",
    "    \n",
    "    # Remove it\n",
    "    debiased = embedding - bias_component\n",
    "    \n",
    "    return debiased\n",
    "\n",
    "# Example\n",
    "print(\"Word Embedding Debiasing:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'man' in word2idx and 'woman' in word2idx:\n",
    "    # Identify gender direction\n",
    "    gendered_pairs = [('man', 'woman'), ('king', 'queen')]\n",
    "    gender_dir = identify_bias_direction(w2v_embeddings, word2idx, gendered_pairs)\n",
    "    \n",
    "    if gender_dir is not None:\n",
    "        print(\"Gender direction identified from word pairs\")\n",
    "        print(f\"Direction vector norm: {np.linalg.norm(gender_dir):.4f}\")\n",
    "        \n",
    "        # Check word projections onto gender direction\n",
    "        print(\"\\nProjections onto gender direction:\")\n",
    "        for word in ['king', 'queen', 'man', 'woman', 'the', 'and']:\n",
    "            if word in word2idx:\n",
    "                proj = np.dot(w2v_embeddings[word2idx[word]], gender_dir)\n",
    "                print(f\"  {word:10s}: {proj:+.4f}\")\n",
    "\n",
    "print(\"\\nNote: Real debiasing requires careful consideration of:\")\n",
    "print(\"- Which words should be gendered (he/she) vs neutral (doctor)\")\n",
    "print(\"- Multiple bias types (race, age, religion)\")\n",
    "print(\"- Downstream task impact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation of Word Embeddings\n",
    "\n",
    "### 11.1 Intrinsic Evaluation\n",
    "\n",
    "**Word Similarity Tasks:**\n",
    "- SimLex-999\n",
    "- WordSim-353\n",
    "- MEN dataset\n",
    "\n",
    "**Analogy Tasks:**\n",
    "- Google analogy dataset (19,544 analogies)\n",
    "- Semantic: capital-country, currency\n",
    "- Syntactic: adjective-adverb, verb tenses\n",
    "\n",
    "### 11.2 Extrinsic Evaluation\n",
    "\n",
    "Evaluate embeddings on downstream tasks:\n",
    "- Sentiment analysis\n",
    "- Named Entity Recognition\n",
    "- Part-of-Speech tagging\n",
    "- Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_word_similarity(embeddings, word2idx, test_pairs):\n",
    "    \"\"\"\n",
    "    Evaluate embeddings on word similarity task.\n",
    "    \n",
    "    test_pairs: list of (word1, word2, human_score) tuples\n",
    "    \n",
    "    Returns Spearman correlation between model and human scores.\n",
    "    \"\"\"\n",
    "    model_scores = []\n",
    "    human_scores = []\n",
    "    \n",
    "    for word1, word2, human_score in test_pairs:\n",
    "        if word1 in word2idx and word2 in word2idx:\n",
    "            vec1 = embeddings[word2idx[word1]]\n",
    "            vec2 = embeddings[word2idx[word2]]\n",
    "            model_sim = cosine_similarity(vec1, vec2)\n",
    "            \n",
    "            model_scores.append(model_sim)\n",
    "            human_scores.append(human_score)\n",
    "    \n",
    "    if len(model_scores) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Spearman correlation\n",
    "    from scipy.stats import spearmanr\n",
    "    correlation, p_value = spearmanr(model_scores, human_scores)\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "def evaluate_analogies(embeddings, word2idx, idx2word, analogies):\n",
    "    \"\"\"\n",
    "    Evaluate on analogy task.\n",
    "    \n",
    "    analogies: list of (a, a*, b, b*) tuples\n",
    "    \n",
    "    Returns accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for a, a_star, b, b_star in analogies:\n",
    "        if all(w in word2idx for w in [a, a_star, b, b_star]):\n",
    "            result = analogy_3cosadd(a, a_star, b, embeddings, word2idx, idx2word)\n",
    "            if result:\n",
    "                predicted, _ = result\n",
    "                if predicted == b_star:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n",
    "\n",
    "# Example evaluation\n",
    "print(\"Word Embedding Evaluation:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Synthetic test data (would use real datasets in practice)\n",
    "test_similarity_pairs = [\n",
    "    ('king', 'queen', 0.8),\n",
    "    ('king', 'man', 0.5),\n",
    "    ('man', 'woman', 0.7),\n",
    "    ('king', 'the', 0.1)\n",
    "]\n",
    "\n",
    "test_analogies = [\n",
    "    ('man', 'woman', 'king', 'queen'),\n",
    "]\n",
    "\n",
    "# Evaluate Word2Vec\n",
    "sim_corr = evaluate_word_similarity(w2v_embeddings, word2idx, test_similarity_pairs)\n",
    "analogy_acc = evaluate_analogies(w2v_embeddings, word2idx, idx2word, test_analogies)\n",
    "\n",
    "print(f\"Word2Vec Results:\")\n",
    "print(f\"  Word Similarity Correlation: {sim_corr:.4f}\")\n",
    "print(f\"  Analogy Accuracy: {analogy_acc:.1%}\")\n",
    "\n",
    "# Evaluate GloVe\n",
    "sim_corr_glove = evaluate_word_similarity(glove_embeddings, word2idx, test_similarity_pairs)\n",
    "analogy_acc_glove = evaluate_analogies(glove_embeddings, word2idx, idx2word, test_analogies)\n",
    "\n",
    "print(f\"\\nGloVe Results:\")\n",
    "print(f\"  Word Similarity Correlation: {sim_corr_glove:.4f}\")\n",
    "print(f\"  Analogy Accuracy: {analogy_acc_glove:.1%}\")\n",
    "\n",
    "print(\"\\nNote: Real evaluation uses standardized datasets like:\")\n",
    "print(\"- SimLex-999 for similarity\")\n",
    "print(\"- Google Analogies dataset (semantic + syntactic)\")\n",
    "print(\"- BATS (Bigger Analogy Test Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Best Practices and Summary\n",
    "\n",
    "### 12.1 Choosing Embeddings\n",
    "\n",
    "**For Most Tasks:**\n",
    "1. Start with pretrained embeddings (GloVe, fastText)\n",
    "2. Fine-tune on your domain data\n",
    "3. Consider contextualized embeddings for complex tasks\n",
    "\n",
    "**Embedding Dimensions:**\n",
    "- Small vocab/task: 50-100 dimensions\n",
    "- General use: 300 dimensions (standard)\n",
    "- Complex tasks: 300-1024 dimensions\n",
    "\n",
    "**Window Size:**\n",
    "- Small (2-5): More syntactic relationships\n",
    "- Large (5-15): More semantic relationships\n",
    "\n",
    "### 12.2 Key Takeaways\n",
    "\n",
    "1. **Dense embeddings** capture semantic similarity in continuous space\n",
    "2. **Word2Vec** learns from local context windows (prediction-based)\n",
    "3. **GloVe** combines global co-occurrence with local windows\n",
    "4. **Embedding arithmetic** reveals learned semantic relationships\n",
    "5. **Pretrained embeddings** provide transfer learning benefits\n",
    "6. **Static embeddings** have limitations (polysemy, OOV, bias)\n",
    "7. **Contextualized embeddings** (ELMo, BERT) address these limitations\n",
    "\n",
    "### 12.3 Next Steps\n",
    "\n",
    "- **Day 13**: Recurrent Neural Networks for text sequences\n",
    "- **Day 14**: Advanced RNNs (LSTM, GRU) and language modeling\n",
    "- **Day 15**: Attention mechanisms and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Word2Vec architecture diagram\n",
    "axes[0, 0].text(0.5, 0.8, 'Skip-gram Architecture', ha='center', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].text(0.5, 0.6, 'Input (one-hot)', ha='center', fontsize=11)\n",
    "axes[0, 0].arrow(0.5, 0.55, 0, -0.1, head_width=0.05, head_length=0.02, fc='blue')\n",
    "axes[0, 0].text(0.5, 0.4, 'W_in (embeddings)', ha='center', fontsize=11, color='blue')\n",
    "axes[0, 0].arrow(0.5, 0.35, 0, -0.1, head_width=0.05, head_length=0.02, fc='blue')\n",
    "axes[0, 0].text(0.5, 0.2, 'W_out @ v_c', ha='center', fontsize=11)\n",
    "axes[0, 0].arrow(0.5, 0.15, 0, -0.1, head_width=0.05, head_length=0.02, fc='blue')\n",
    "axes[0, 0].text(0.5, 0.0, 'Softmax (context prediction)', ha='center', fontsize=11)\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "axes[0, 0].set_ylim(-0.1, 1)\n",
    "axes[0, 0].axis('off')\n",
    "axes[0, 0].set_title('Word2Vec', fontsize=12)\n",
    "\n",
    "# 2. Training losses comparison\n",
    "epochs = np.arange(1, len(losses) + 1)\n",
    "axes[0, 1].plot(epochs, losses, 'b-', label='Word2Vec', linewidth=2)\n",
    "axes[0, 1].plot(epochs, glove_losses[:len(epochs)], 'r-', label='GloVe', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Training Convergence')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Embedding properties\n",
    "properties = ['Semantic\\nSimilarity', 'Analogy\\nArithmetic', 'Training\\nSpeed', \n",
    "              'Memory\\nEfficiency', 'OOV\\nHandling']\n",
    "word2vec_scores = [4, 4.5, 3, 4, 1]\n",
    "glove_scores = [4.5, 4, 4.5, 3, 1]\n",
    "fasttext_scores = [4, 4, 3.5, 3.5, 5]\n",
    "\n",
    "x = np.arange(len(properties))\n",
    "width = 0.25\n",
    "\n",
    "axes[1, 0].bar(x - width, word2vec_scores, width, label='Word2Vec', color='blue', alpha=0.7)\n",
    "axes[1, 0].bar(x, glove_scores, width, label='GloVe', color='red', alpha=0.7)\n",
    "axes[1, 0].bar(x + width, fasttext_scores, width, label='fastText', color='green', alpha=0.7)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(properties, rotation=45, ha='right')\n",
    "axes[1, 0].set_ylabel('Score (1-5)')\n",
    "axes[1, 0].set_title('Embedding Method Comparison')\n",
    "axes[1, 0].legend(loc='lower right')\n",
    "axes[1, 0].set_ylim(0, 6)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Timeline of embedding methods\n",
    "methods = ['LSA', 'Word2Vec', 'GloVe', 'fastText', 'ELMo', 'BERT']\n",
    "years = [1988, 2013, 2014, 2016, 2018, 2018]\n",
    "types = ['Count', 'Predict', 'Hybrid', 'Subword', 'Context', 'Context']\n",
    "\n",
    "axes[1, 1].scatter(years, range(len(methods)), s=200, c='blue', alpha=0.6, zorder=5)\n",
    "for i, (method, year, typ) in enumerate(zip(methods, years, types)):\n",
    "    axes[1, 1].text(year + 0.5, i, f'{method}\\n({typ})', ha='left', va='center', fontsize=10)\n",
    "axes[1, 1].set_yticks([])\n",
    "axes[1, 1].set_xlabel('Year')\n",
    "axes[1, 1].set_title('Evolution of Word Embeddings')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1, 1].set_xlim(1985, 2022)\n",
    "\n",
    "plt.suptitle('Word Embeddings: Summary', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Implement Hierarchical Softmax\n",
    "Implement hierarchical softmax for Word2Vec to reduce complexity from O(V) to O(log V).\n",
    "\n",
    "### Exercise 2: Train on Larger Corpus\n",
    "Download a larger text corpus (e.g., Wikipedia snippets) and train Word2Vec. Evaluate on standard analogy tasks.\n",
    "\n",
    "### Exercise 3: Subword Embeddings\n",
    "Implement character n-gram embeddings (fastText-style) to handle out-of-vocabulary words.\n",
    "\n",
    "### Exercise 4: Bias Analysis\n",
    "Using pretrained embeddings, identify and quantify biases. Implement debiasing and measure the impact.\n",
    "\n",
    "### Exercise 5: Domain-Specific Embeddings\n",
    "Train embeddings on domain-specific text (medical, legal, technical) and compare to general embeddings.\n",
    "\n",
    "### Exercise 6: Multilingual Embeddings\n",
    "Research and implement cross-lingual embedding alignment (e.g., MUSE, VecMap).\n",
    "\n",
    "### Exercise 7: Embedding Compression\n",
    "Implement embedding compression techniques (quantization, dimensionality reduction) and evaluate trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for Exercise 3: Subword Embeddings\n",
    "\n",
    "def get_ngrams(word, n=3):\n",
    "    \"\"\"\n",
    "    Get character n-grams for a word.\n",
    "    \n",
    "    Add special boundary markers < and >.\n",
    "    \"\"\"\n",
    "    word = f\"<{word}>\"\n",
    "    ngrams = []\n",
    "    \n",
    "    for i in range(len(word) - n + 1):\n",
    "        ngrams.append(word[i:i+n])\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Example\n",
    "word = \"where\"\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"3-grams: {get_ngrams(word, n=3)}\")\n",
    "print(f\"4-grams: {get_ngrams(word, n=4)}\")\n",
    "\n",
    "# fastText represents a word as sum of:\n",
    "# 1. Word embedding itself\n",
    "# 2. All character n-gram embeddings\n",
    "# This allows handling OOV words!\n",
    "\n",
    "oov_word = \"wherefore\"\n",
    "print(f\"\\nOOV word: {oov_word}\")\n",
    "print(f\"3-grams: {get_ngrams(oov_word, n=3)}\")\n",
    "print(\"\\nMany n-grams overlap with 'where' -> similar embedding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Mikolov, T., et al. (2013). \"Efficient Estimation of Word Representations in Vector Space.\" (Word2Vec)\n",
    "2. Mikolov, T., et al. (2013). \"Distributed Representations of Words and Phrases and their Compositionality.\" (Negative Sampling)\n",
    "3. Pennington, J., et al. (2014). \"GloVe: Global Vectors for Word Representation.\"\n",
    "4. Bojanowski, P., et al. (2017). \"Enriching Word Vectors with Subword Information.\" (fastText)\n",
    "5. Bolukbasi, T., et al. (2016). \"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings.\"\n",
    "6. Levy, O., & Goldberg, Y. (2014). \"Neural Word Embedding as Implicit Matrix Factorization.\"\n",
    "7. Schnabel, T., et al. (2015). \"Evaluation Methods for Unsupervised Word Embeddings.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
