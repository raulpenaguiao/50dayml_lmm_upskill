# Day 17: Introduction to Transformers - Further Reading

This day introduces the Transformer architecture, which replaced RNNs for most NLP tasks by enabling parallel processing and better capturing long-range dependencies through self-attention.

## References

1. **Attention Is All You Need**
   - [Vaswani et al.: Transformer Paper](https://arxiv.org/abs/1706.03762)
   - Groundbreaking paper introducing the Transformer architecture with self-attention and multi-head attention, revolutionizing NLP.

2. **The Illustrated Transformer**
   - [Jay Alammar's Visual Explanation](http://jalammar.github.io/illustrated-transformer/)
   - Excellent visual guide to Transformer architecture with clear diagrams explaining self-attention, multi-head attention, and positional encoding.

3. **Self-Attention with Relative Position Representations**
   - [Shaw et al.: Relative Position in Self-Attention](https://arxiv.org/abs/1803.02155)
   - Paper improving positional encoding in transformers using relative position representations for better modeling.

4. **PyTorch Transformer Tutorial**
   - [PyTorch Transformer from Scratch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
   - Official step-by-step tutorial implementing a complete Transformer model from scratch with practical examples.

5. **Understanding Positional Encodings**
   - [Positional Encoding Deep Dive](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)
   - Detailed explanation of sinusoidal positional encodings and why they work better than learned embeddings.

---

**Tip:** Start with reference #2 for visual intuition, then #1 for theory, #5 for positional encodings, and #4 for implementation.
