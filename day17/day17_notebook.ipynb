{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 17: Introduction to Transformers\n",
    "\n",
    "**Goal:** Understand the Transformer architecture and why it replaced RNNs for most NLP tasks.\n",
    "\n",
    "**Time estimate:** 3-4 hours\n",
    "\n",
    "**What you'll learn:**\n",
    "- Self-attention mechanism and scaled dot-product attention\n",
    "- Multi-head attention\n",
    "- Positional encodings\n",
    "- Complete Transformer block\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Transformers? (RNN Limitations)\n",
    "\n",
    "### The Problem with RNNs/LSTMs\n",
    "\n",
    "From Days 13-16, we saw RNNs process sequences **sequentially**:\n",
    "\n",
    "```\n",
    "Input:  [token_0, token_1, token_2, ..., token_n]\n",
    "RNN:     h_0  -->  h_1  -->  h_2  -->  ...  -->  h_n\n",
    "```\n",
    "\n",
    "**Issues:**\n",
    "1. **Sequential processing is slow**: Can't parallelize across time steps\n",
    "2. **Long-range dependencies fade**: Information decays over long sequences\n",
    "3. **Vanishing/exploding gradients**: Even LSTMs have difficulty with very long sequences\n",
    "4. **Limited by hidden state size**: All information compressed into fixed-size vector\n",
    "\n",
    "### The Transformer Solution\n",
    "\n",
    "**Key Innovation:** Replace sequential recurrence with **self-attention** that can attend to **any** position in one step.\n",
    "\n",
    "```\n",
    "Input:  [token_0, token_1, token_2, ..., token_n]\n",
    "         |         |         |              |\n",
    "         +-------- Self-Attention --------+\n",
    "         |         |         |              |\n",
    "Output: [out_0, out_1, out_2, ..., out_n]\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- ✅ **Fully parallelizable**: All positions processed simultaneously\n",
    "- ✅ **Direct long-range connections**: Each position can attend to any other position\n",
    "- ✅ **Constant path length**: Information flows directly, no RNN compression\n",
    "- ✅ **Interpretable**: Attention weights show what model attends to\n",
    "\n",
    "**Result:** Transformers became the foundation for:\n",
    "- BERT (2018)\n",
    "- GPT family (2018-2023)\n",
    "- T5, RoBERTa, XLNet, ELECTRA, and most modern NLP models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Self-Attention Mechanism\n",
    "\n",
    "### Intuition: \"What should I pay attention to?\"\n",
    "\n",
    "When processing token `i` in a sequence, we want to:\n",
    "1. Compare token `i` with **all** other tokens\n",
    "2. Compute **relevance scores** (how important is each token?)\n",
    "3. **Aggregate information** from all tokens, weighted by relevance\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = **Query** matrix (shape: `[seq_len, d_k]`)\n",
    "- $K$ = **Key** matrix (shape: `[seq_len, d_k]`)\n",
    "- $V$ = **Value** matrix (shape: `[seq_len, d_v]`)\n",
    "- $d_k$ = dimension of keys (used for scaling)\n",
    "\n",
    "**Step by step:**\n",
    "\n",
    "1. **Compute attention scores**: $QK^T$ (shape: `[seq_len, seq_len]`)\n",
    "   - Entry $(i, j)$ = how much token $i$ should attend to token $j$\n",
    "   \n",
    "2. **Scale by $\\sqrt{d_k}$**: Stabilizes gradients\n",
    "   - Without scaling, dot products can be very large\n",
    "   - Leads to softmax with tiny gradients\n",
    "   \n",
    "3. **Apply softmax**: Normalize to get probabilities\n",
    "   - Each row sums to 1\n",
    "   - Represents \"attention distribution\"\n",
    "   \n",
    "4. **Weight values**: Multiply by $V$\n",
    "   - Aggregate value vectors weighted by attention\n",
    "\n",
    "### Example: Attention in Action\n",
    "\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "When processing \"cat\", attention might compute:\n",
    "- \"The\" (high relevance - article modifying cat)\n",
    "- \"sat\" (high relevance - main verb, subject)\n",
    "- \"on\" (medium relevance - location)\n",
    "- \"the\" (medium relevance - another article)\n",
    "- \"mat\" (medium relevance - location object)\n",
    "\n",
    "The output for \"cat\" is a weighted combination of all word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix, shape (batch_size, seq_len, d_k)\n",
    "        K: Key matrix, shape (batch_size, seq_len, d_k)\n",
    "        V: Value matrix, shape (batch_size, seq_len, d_v)\n",
    "        mask: Optional mask for preventing attention to future tokens\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output, shape (batch_size, seq_len, d_v)\n",
    "        attention_weights: Attention weights, shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores (Q @ K^T) / sqrt(d_k)\n",
    "    # Shape: (batch_size, seq_len, seq_len)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply mask (optional, for causal attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 3: Apply softmax\n",
    "    # Shape: (batch_size, seq_len, seq_len)\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Handle NaN from masked positions\n",
    "    attention_weights = torch.nan_to_num(attention_weights, 0.0)\n",
    "    \n",
    "    # Step 4: Multiply by values\n",
    "    # Shape: (batch_size, seq_len, d_v)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "print(\"✓ Scaled dot-product attention implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Attention: Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example\n",
    "# Sequence of 4 tokens, embedding dimension = 64\n",
    "seq_len = 4\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "batch_size = 1\n",
    "\n",
    "# Random Q, K, V (in practice, come from embeddings)\n",
    "Q = torch.randn(batch_size, seq_len, d_k).to(device)\n",
    "K = torch.randn(batch_size, seq_len, d_k).to(device)\n",
    "V = torch.randn(batch_size, seq_len, d_v).to(device)\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights (for first sequence):\")\n",
    "print(attention_weights[0])\n",
    "print(f\"\\nRow sums (should be 1.0): {attention_weights[0].sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more interpretable example with sentence tokens\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "seq_len = len(words)\n",
    "\n",
    "# Create attention weights with some structure\n",
    "# (In real case, these come from trained model)\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(1, seq_len, 64)\n",
    "K = torch.randn(1, seq_len, 64)\n",
    "V = torch.randn(1, seq_len, 64)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Visualize attention patterns\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(attention_weights[0].detach().cpu().numpy(), \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='viridis',\n",
    "            xticklabels=words,\n",
    "            yticklabels=words,\n",
    "            cbar_kws={'label': 'Attention Weight'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Key (attend to)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Query (attend from)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Self-Attention Weights\\n\"The cat sat on the mat\"', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Each row shows: FROM which token (left) TO which tokens (top)\")\n",
    "print(f\"\\nExample: Token '{words[1]}' (cat) attends to:\")\n",
    "for j, word in enumerate(words):\n",
    "    weight = attention_weights[0, 1, j].item()\n",
    "    print(f\"  {word:6s}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Multi-Head Attention\n",
    "\n",
    "### Motivation: Why Multiple Heads?\n",
    "\n",
    "Single-head attention focuses on **one type of relationship**.\n",
    "\n",
    "**Example:**\n",
    "- \"The cat sat on the mat\"\n",
    "- One head might learn: \"subject-verb dependencies\"\n",
    "- Another head might learn: \"noun-adjective modifiers\"\n",
    "- Another might learn: \"prepositional phrases\"\n",
    "\n",
    "With **multiple heads**, the model can simultaneously attend to:\n",
    "- Different types of relationships\n",
    "- Different parts of the sequence\n",
    "- Different levels of abstraction\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "$$\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(head_1, ..., head_h) W^O$$\n",
    "\n",
    "Where each head is:\n",
    "$$head_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$\n",
    "\n",
    "**Parameters:**\n",
    "- $h$ = number of heads\n",
    "- $W_i^Q, W_i^K, W_i^V$ = linear projections for head $i$\n",
    "- $W^O$ = output projection\n",
    "- Each head operates on dimension $d_k = d_{model}/h$\n",
    "\n",
    "**Benefit:** Computational cost similar to single-head with dimension $d_{model}$, but much more expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head self-attention layer.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Dimension of model (embedding dimension)\n",
    "        num_heads: Number of attention heads\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.register_buffer('scale', torch.tensor(self.d_k ** 0.5))\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor, shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Output tensor, shape (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention weights from all heads\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Step 1: Linear projections\n",
    "        # Shape: (batch_size, seq_len, d_model)\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Step 2: Split into multiple heads\n",
    "        # Reshape from (batch_size, seq_len, d_model)\n",
    "        # to (batch_size, seq_len, num_heads, d_k)\n",
    "        # then transpose to (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 3: Apply attention for each head\n",
    "        # Compute scores: (batch_size, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Attention weights: (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = torch.nan_to_num(attention_weights, 0.0)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # (batch_size, num_heads, seq_len, d_k)\n",
    "        attended_values = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Step 4: Concatenate heads\n",
    "        # Transpose from (batch_size, num_heads, seq_len, d_k)\n",
    "        # to (batch_size, seq_len, num_heads, d_k)\n",
    "        # then reshape to (batch_size, seq_len, d_model)\n",
    "        concat = attended_values.transpose(1, 2).contiguous()\n",
    "        concat = concat.view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Step 5: Final linear projection\n",
    "        output = self.W_o(concat)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"✓ Multi-head attention layer implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-head attention layer\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "seq_len = 6\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads).to(device)\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  (batch_size={batch_size}, num_heads={num_heads}, seq_len={seq_len}, seq_len={seq_len})\")\n",
    "\n",
    "# Count parameters\n",
    "params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"\\nTotal parameters: {params:,}\")\n",
    "print(f\"  (4 linear layers: 4 × {d_model} × {d_model} = {4 * d_model * d_model:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Multiple Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights for visualization\n",
    "words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "x_demo = torch.randn(1, len(words), d_model).to(device)\n",
    "output_demo, attn_weights_demo = mha(x_demo)\n",
    "\n",
    "# Plot attention patterns for each head\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    \n",
    "    # Get attention weights for this head\n",
    "    head_attn = attn_weights_demo[0, head_idx].detach().cpu().numpy()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(head_attn, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='Blues',\n",
    "                xticklabels=words,\n",
    "                yticklabels=words,\n",
    "                ax=ax,\n",
    "                cbar=False)\n",
    "    \n",
    "    ax.set_title(f'Head {head_idx + 1}', fontweight='bold')\n",
    "    ax.set_xlabel('Attend to')\n",
    "    ax.set_ylabel('Attend from')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Patterns', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Each head learns different patterns!\")\n",
    "print(\"Head 1 might focus on adjacent words, Head 2 on content words, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Positional Encodings\n",
    "\n",
    "### The Problem: Where is Position Information?\n",
    "\n",
    "Attention is **permutation-invariant**:\n",
    "```\n",
    "\"The cat sat\" → same output as \"cat The sat\"\n",
    "```\n",
    "\n",
    "This is because:\n",
    "- Attention only depends on **similarity** between tokens\n",
    "- No inherent notion of position or order\n",
    "- Need to **inject position information** into embeddings\n",
    "\n",
    "### Solution: Positional Encodings\n",
    "\n",
    "Add position-dependent values to token embeddings.\n",
    "\n",
    "**Sinusoidal Positional Encodings (from \"Attention is All You Need\"):**\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "Where:\n",
    "- $pos$ = position in sequence (0, 1, 2, ...)\n",
    "- $i$ = dimension index (0, 1, 2, ...)\n",
    "- $d_{model}$ = embedding dimension\n",
    "\n",
    "**Why sinusoidal?**\n",
    "1. Creates unique pattern for each position\n",
    "2. Model can learn relative positions (linear transformations of sine/cosine)\n",
    "3. Generalizes to sequences longer than training\n",
    "4. Fixed (non-learned) - reduces parameters\n",
    "\n",
    "### Alternative: Learned Positional Embeddings\n",
    "\n",
    "Simply learn embedding table:\n",
    "$$PE = \\text{Embedding}(pos)$$\n",
    "\n",
    "- ✅ Simpler, model can learn whatever helps\n",
    "- ❌ Limited to training sequence length\n",
    "- Less common now, sinusoidal is standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding using sinusoidal functions.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Dimension of model\n",
    "        max_seq_len: Maximum sequence length\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create position indices: [0, 1, 2, ..., max_seq_len-1]\n",
    "        positions = torch.arange(max_seq_len).unsqueeze(1)  # Shape: (max_seq_len, 1)\n",
    "        \n",
    "        # Create dimension indices: [0, 1, 2, ..., d_model-1]\n",
    "        dimensions = torch.arange(d_model).unsqueeze(0)  # Shape: (1, d_model)\n",
    "        \n",
    "        # Compute angle rates: 10000^(2i/d_model)\n",
    "        angle_rates = 1 / (10000 ** (2 * (dimensions // 2) / d_model))\n",
    "        \n",
    "        # Compute angles: pos / 10000^(2i/d_model)\n",
    "        angle_rads = positions * angle_rates  # Shape: (max_seq_len, d_model)\n",
    "        \n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        angle_rads[:, 0::2] = torch.sin(angle_rads[:, 0::2])  # sin for 0, 2, 4, ...\n",
    "        angle_rads[:, 1::2] = torch.cos(angle_rads[:, 1::2])  # cos for 1, 3, 5, ...\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of module state)\n",
    "        self.register_buffer('pe', angle_rads.unsqueeze(0))  # Shape: (1, max_seq_len, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings, shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x + positional_encoding\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "print(\"✓ Positional encoding layer implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create positional encoding\n",
    "d_model = 512\n",
    "pe = PositionalEncoding(d_model, max_seq_len=100)\n",
    "\n",
    "# Extract positional encodings (without the input, just the PE)\n",
    "pos_enc = pe.pe[0, :50, :].detach().cpu().numpy()  # First 50 positions\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot as heatmap\n",
    "im = ax.imshow(pos_enc.T, cmap='coolwarm', aspect='auto')\n",
    "\n",
    "ax.set_xlabel('Position in Sequence', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Dimension Index', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Sinusoidal Positional Encodings (d_model={d_model})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Encoding Value', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Each position has a unique pattern\")\n",
    "print(\"- Different dimensions oscillate at different frequencies\")\n",
    "print(\"- Lower dimensions change slowly (capture long-range positions)\")\n",
    "print(\"- Higher dimensions change quickly (capture fine-grained positions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property: Learning Relative Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that sinusoidal PE can represent relative positions\n",
    "# PE(pos + k) can be computed from PE(pos) via linear transformation\n",
    "\n",
    "pos_enc_small = PositionalEncoding(d_model=64, max_seq_len=100)\n",
    "pe_table = pos_enc_small.pe[0].detach().cpu()  # Shape: (100, 64)\n",
    "\n",
    "# Pick two positions\n",
    "pos1, pos2 = 10, 15\n",
    "pe_pos1 = pe_table[pos1]\n",
    "pe_pos2 = pe_table[pos2]\n",
    "\n",
    "# Compute distance\n",
    "offset = pos2 - pos1\n",
    "pe_offset = pe_table[offset]\n",
    "\n",
    "# In theory: PE(pos2) ≈ some linear transformation of PE(pos1)\n",
    "# This allows model to learn relative position relationships\n",
    "\n",
    "print(f\"Position 1: {pos1}\")\n",
    "print(f\"Position 2: {pos2}\")\n",
    "print(f\"Offset: {offset}\")\n",
    "print(f\"\\nPE dimensions match: {pe_pos1.shape} = {pe_pos2.shape} = {pe_offset.shape}\")\n",
    "print(f\"\\nKey property: Sinusoidal encoding allows model to learn\")\n",
    "print(f\"relative positions through linear transformations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Transformer Block\n",
    "\n",
    "### Complete Transformer Encoder Block\n",
    "\n",
    "A complete transformer block combines:\n",
    "1. **Multi-head self-attention**\n",
    "2. **Add & Norm (residual connection + layer norm)**\n",
    "3. **Feed-forward network**\n",
    "4. **Add & Norm**\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Input (batch_size, seq_len, d_model)\n",
    "    |\n",
    "    ├─→ Multi-Head Attention\n",
    "    |       |\n",
    "    |       ├─→ LayerNorm\n",
    "    |       ├─→ Attention\n",
    "    |       └─→ Add (residual)\n",
    "    |\n",
    "    ├─→ Feed-Forward Network\n",
    "    |       |\n",
    "    |       ├─→ LayerNorm\n",
    "    |       ├─→ Linear → ReLU → Linear\n",
    "    |       └─→ Add (residual)\n",
    "    |\n",
    "Output (batch_size, seq_len, d_model)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**Layer Normalization:**\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "- Normalizes each feature dimension independently\n",
    "- Stabilizes training in residual networks\n",
    "\n",
    "**Residual Connection:**\n",
    "$$y = \\text{Sublayer}(x) + x$$\n",
    "\n",
    "- Helps gradient flow in deep networks\n",
    "- Preserves identity for efficient updates\n",
    "\n",
    "**Feed-Forward Network:**\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "- Typical expansion: $d_{model} \\to 4 \\times d_{model} \\to d_{model}$\n",
    "- Applied independently to each position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Input/output dimension\n",
    "        d_ff: Hidden layer dimension (typically 4 × d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n\n\nclass TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer encoder block.\n",
    "    \n",
    "    Args:\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        d_ff: Feed-forward hidden dimension\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Output, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, _ = self.attention(x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        x = self.norm1(x + attn_output)  # Add & Norm\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        x = self.norm2(x + ffn_output)  # Add & Norm\n",
    "        \n",
    "        return x\n\n\nprint(\"✓ Transformer block implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer block\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "seq_len = 10\n",
    "batch_size = 4\n",
    "\n",
    "transformer_block = TransformerBlock(d_model, num_heads, d_ff, dropout=0.1).to(device)\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = transformer_block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "params = sum(p.numel() for p in transformer_block.parameters())\n",
    "print(f\"\\nTransformer block parameters: {params:,}\")\n",
    "\n",
    "# Breakdown\n",
    "attn_params = sum(p.numel() for p in transformer_block.attention.parameters())\n",
    "ffn_params = sum(p.numel() for p in transformer_block.ffn.parameters())\n",
    "norm_params = sum(p.numel() for p in transformer_block.norm1.parameters()) + \\\n",
    "              sum(p.numel() for p in transformer_block.norm2.parameters())\n",
    "\n",
    "print(f\"  Multi-head attention: {attn_params:,}\")\n",
    "print(f\"  Feed-forward: {ffn_params:,}\")\n",
    "print(f\"  Layer norms: {norm_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Complete Transformer Encoder\n",
    "\n",
    "Stack multiple transformer blocks with embeddings and positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer encoder (stack of transformer blocks).\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: Size of vocabulary\n",
    "        d_model: Model dimension\n",
    "        num_heads: Number of attention heads\n",
    "        num_layers: Number of transformer blocks\n",
    "        d_ff: Feed-forward dimension\n",
    "        max_seq_len: Maximum sequence length\n",
    "        dropout: Dropout rate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, \n",
    "                 d_ff=2048, max_seq_len=1000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer norm\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token IDs, shape (batch_size, seq_len)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Encoded representations, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)  # Scale embeddings\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        return x\n\n\nprint(\"✓ Transformer encoder implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Complete Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer encoder\n",
    "vocab_size = 10000\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "d_ff = 1024\n",
    "seq_len = 20\n",
    "batch_size = 8\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Create input (random token IDs)\n",
    "x = torch.randint(1, vocab_size, (batch_size, seq_len)).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input (token IDs): {x[0]}\")\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Output (embeddings): {output[0, 0, :5]} ... (showing first 5 dims)\")\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Breakdown by component\n",
    "embedding_params = sum(p.numel() for p in encoder.embedding.parameters())\n",
    "transformer_params = sum(p.numel() for p in \n",
    "                         [p for block in encoder.transformer_blocks \n",
    "                          for p in block.parameters()])\n",
    "norm_params = sum(p.numel() for p in encoder.final_norm.parameters())\n",
    "\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Embeddings: {embedding_params:,}\")\n",
    "print(f\"  Transformer blocks: {transformer_params:,}\")\n",
    "print(f\"  Final norm: {norm_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparison with RNNs\n",
    "\n",
    "### Why Transformers Won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Transformer vs RNN properties\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Property': [\n",
    "        'Parallelization',\n",
    "        'Long-range dependencies',\n",
    "        'Gradient flow path length',\n",
    "        'Interpretability',\n",
    "        'Parameter efficiency',\n",
    "        'Memory usage',\n",
    "        'Training speed',\n",
    "        'Inference on long seqs',\n",
    "    ],\n",
    "    'RNN/LSTM': [\n",
    "        '❌ Sequential',\n",
    "        '⚠️ Difficult (vanishing gradients)',\n",
    "        '❌ O(seq_len)',\n",
    "        '⚠️ Black box hidden states',\n",
    "        '✅ Smaller models',\n",
    "        '✅ O(hidden_dim)',\n",
    "        '❌ Slow',\n",
    "        '❌ O(seq_len) per token',\n",
    "    ],\n",
    "    'Transformer': [\n",
    "        '✅ Fully parallel',\n",
    "        '✅ Direct attention',\n",
    "        '✅ O(1)',\n",
    "        '✅ Attention weights',\n",
    "        '⚠️ Larger (for same capacity)',\n",
    "        '⚠️ O(seq_len²) (quadratic)',\n",
    "        '✅ Fast',\n",
    "        '⚠️ O(seq_len²) per token',\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Insight: Transformers trade memory for parallelization and interpretability\")\n",
    "print(\"O(seq_len²) is acceptable for modern hardware, especially with batching\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze computational complexity\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sequence lengths\n",
    "seq_lengths = np.array([10, 50, 100, 200, 500, 1000])\n",
    "\n",
    "# Complexity (arbitrary units)\n",
    "rnn_complexity = seq_lengths  # O(n)\n",
    "transformer_complexity = seq_lengths ** 2  # O(n²)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "ax = axes[0]\n",
    "ax.plot(seq_lengths, rnn_complexity, 'o-', label='RNN O(n)', linewidth=2, markersize=8)\n",
    "ax.plot(seq_lengths, transformer_complexity, 's-', label='Transformer O(n²)', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Computational Cost (units)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Computational Complexity (Linear Scale)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "ax = axes[1]\n",
    "ax.loglog(seq_lengths, rnn_complexity, 'o-', label='RNN O(n)', linewidth=2, markersize=8)\n",
    "ax.loglog(seq_lengths, transformer_complexity, 's-', label='Transformer O(n²)', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Computational Cost (units)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Computational Complexity (Log Scale)', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(f\"- At seq_len=100: Transformer is {(100**2) / 100:.0f}x more expensive\")\n",
    "print(f\"- At seq_len=1000: Transformer is {(1000**2) / 1000:.0f}x more expensive\")\n",
    "print(f\"\\nBut: Transformers are 10-100x faster in practice due to GPU parallelization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Key Takeaways\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "✅ **Self-Attention:** How to compute attention weights and aggregate information  \n",
    "✅ **Multi-Head Attention:** Learning multiple types of relationships simultaneously  \n",
    "✅ **Positional Encodings:** Injecting position information into embeddings  \n",
    "✅ **Transformer Block:** Combining attention, FFN, and normalization  \n",
    "✅ **Transformer Encoder:** Full encoder architecture  \n",
    "\n",
    "### Why Transformers Matter\n",
    "\n",
    "**Parallelization:** No sequential dependency, enables GPUs  \n",
    "**Long-range:** Direct attention to any position  \n",
    "**Interpretability:** Attention weights reveal model reasoning  \n",
    "**Scalability:** Works with very large datasets and models  \n",
    "\n",
    "### Foundation for Days 18-30\n",
    "\n",
    "**Day 18:** Text Classification with Transformers  \n",
    "**Days 21-22:** Transformer decoder (for generation)  \n",
    "**Days 24-25:** Building GPT from scratch  \n",
    "**Day 26:** BERT (encoder-only)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Exercises\n",
    "\n",
    "### Basic\n",
    "1. **Modify attention mask:** Create a causal mask for autoregressive attention\n",
    "2. **Add dropout:** Add dropout to scaled dot-product attention\n",
    "3. **Experiment with heads:** Compare 1-head vs 8-head vs 16-head attention\n",
    "\n",
    "### Intermediate\n",
    "1. **Implement relative position encoding:** Use Shaw et al. 2018 approach\n",
    "2. **Add learnable positional embeddings:** Replace sinusoidal with learned PE\n",
    "3. **Analyze attention:** What patterns do different heads learn?\n",
    "\n",
    "### Advanced\n",
    "1. **Implement efficient attention:** Linear attention (Linformer) or sparse patterns\n",
    "2. **Add layer weight sharing:** Reuse weights across blocks (like Albert)\n",
    "3. **Rotary positional embeddings (RoPE):** Modern alternative to sinusoidal\n",
    "4. **Analyze gradient flow:** How deep can Transformers get without exploding gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Summary\n",
    "\n",
    "You've successfully implemented a complete Transformer encoder from scratch!\n",
    "\n",
    "**Key Components:**\n",
    "- Scaled dot-product attention\n",
    "- Multi-head attention with separate heads\n",
    "- Sinusoidal positional encodings\n",
    "- Transformer block with residual connections\n",
    "- Complete encoder stack\n",
    "\n",
    "**Next:** Tomorrow (Day 18) we'll use Transformers for text classification and explore fine-tuning with pretrained models.\n",
    "\n",
    "**Further Reading:**\n",
    "- \"Attention is All You Need\" (Vaswani et al., 2017) - Original paper\n",
    "- \"The Illustrated Transformer\" (Jay Alammar) - Excellent visual guide\n",
    "- PyTorch Transformer Tutorial - Official implementation guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
