# Day 35: Evaluation Metrics for NLP Models

## üéØ Goal
Master evaluation metrics specific to NLP tasks including BLEU, ROUGE, and perplexity.

---

## üìö Topics Covered
- Classification metrics review
- BLEU score for translation
- ROUGE for summarization
- Perplexity for language models
- Human evaluation considerations

---

## üìù Syllabus

### 1. Classification Metrics
- Accuracy, precision, recall, F1
- Macro vs micro vs weighted
- Multi-class vs multi-label
- ROC-AUC for NLP

### 2. Generation Metrics
- BLEU (Bilingual Evaluation Understudy)
- ROUGE (Recall-Oriented Understudy for Gisting Evaluation)
- METEOR
- CIDEr

### 3. Language Model Metrics
- Perplexity calculation
- Cross-entropy loss
- Bits per character
- Interpretation

### 4. Advanced Metrics
- BERTScore
- BLEURT
- Human evaluation
- A/B testing

---

## ‚úÖ Tasks

1. **Implement Metrics**
   - Calculate BLEU scores
   - Compute ROUGE scores
   - Measure perplexity

2. **Evaluation Suite**
   - Build evaluation pipeline
   - Test on multiple models
   - Compare results

3. **Metric Analysis**
   - Understand metric limitations
   - Correlation with human judgment
   - When to use each metric

4. **Comprehensive Evaluation**
   - Evaluate your models from previous days
   - Create evaluation report
   - Visualize results

---

## üí° Stretch Goals (Optional)
- Implement BERTScore
- Create custom metrics
- Build evaluation dashboard
- Study metric failure cases
