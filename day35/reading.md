# Day 35: Evaluation Metrics for NLP Models - Further Reading

This day covers comprehensive evaluation metrics for NLP tasks including classification metrics, translation metrics (BLEU), summarization metrics (ROUGE), and language model metrics.

## References

1. **BLEU: a Method for Automatic Evaluation of Machine Translation**
   - [Papineni et al.: BLEU Paper](https://aclanthology.org/P02-1040.pdf)
   - Seminal paper introducing BLEU score for evaluating machine translation quality.

2. **ROUGE: A Package for Automatic Evaluation of Summaries**
   - [Lin: ROUGE Paper](https://aclanthology.org/W04-1013.pdf)
   - Paper introducing ROUGE metrics for evaluating summarization and paraphrase tasks.

3. **BERTScore: Evaluating Text Generation with BERT**
   - [Zhang et al.: BERTScore](https://arxiv.org/abs/1904.09675)
   - Modern metric using BERT embeddings for more semantic-aware evaluation of generated text.

4. **SacreBLEU: Efficient BLEU Computation**
   - [Post: SacreBLEU](https://aclanthology.org/W18-6319.pdf)
   - Standardized BLEU implementation addressing inconsistencies across implementations.

5. **Scikit-learn Metrics Documentation**
   - [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
   - Comprehensive reference for classification metrics including precision, recall, F1, and ROC-AUC.

---

**Tip:** Start with reference #5 for classification basics, #1 for BLEU, #2 for ROUGE, and #3 for modern semantic metrics.
