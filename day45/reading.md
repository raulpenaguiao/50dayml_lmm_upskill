# Day 45: Advanced Training Optimization - Further Reading

This day covers cutting-edge techniques for efficient LLM training and inference including quantization, pruning, and knowledge distillation.

## References

1. **Scaling Laws for Neural Language Models**
   - [Kaplan et al.: Scaling Laws](https://arxiv.org/abs/2001.08361)
   - Important research on understanding scaling relationships guiding model and data sizing.

2. **Model Quantization: 4-bit and 8-bit Training**
   - [BitsandBytes: Efficient LLM Training](https://github.com/TimDettmers/bitsandbytes)
   - Library enabling 4-bit and 8-bit training and inference for memory-efficient LLMs.

3. **Knowledge Distillation from LLMs**
   - [Hinton et al.: Distilling Knowledge](https://arxiv.org/abs/1503.02531)
   - Classic paper on knowledge distillation for training smaller models from larger ones.

4. **Pruning and Sparsity in Transformers**
   - [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1906.02107)
   - Paper on neural network pruning showing structured networks can match dense models.

5. **Flash Attention and Efficient Attention**
   - [Dao et al.: Flash Attention](https://arxiv.org/abs/2205.14135)
   - State-of-the-art efficient attention implementation significantly reducing memory and compute.

---

**Tip:** Start with reference #1 for scaling laws, #2 for quantization, #3 for distillation, #4 for pruning, and #5 for efficient attention.
