# Day 31: Introduction to Hugging Face Transformers

## üéØ Goal
Learn to use the Hugging Face Transformers library for loading and using pretrained models.

---

## üìö Topics Covered
- Hugging Face ecosystem overview
- Loading pretrained models
- Tokenizers and pipelines
- Model hub exploration
- Basic inference

---

## üìù Syllabus

### 1. Hugging Face Ecosystem
- Transformers library
- Datasets library
- Tokenizers library
- Model Hub
- Spaces and demos

### 2. Loading Models
- AutoModel and AutoTokenizer
- Model architectures (BERT, GPT-2, T5, etc.)
- Configuration classes
- Device management

### 3. Pipelines
- High-level pipeline API
- Text classification
- Text generation
- Question answering
- Other tasks

### 4. Tokenizers
- Fast tokenizers
- Encoding and decoding
- Special tokens
- Padding and truncation

---

## ‚úÖ Tasks

1. **Setup Environment**
   - Install transformers
   - Install datasets
   - Configure GPU/CPU

2. **Use Pipelines**
   - Text classification pipeline
   - Generation pipeline
   - QA pipeline
   - Compare different models

3. **Manual Model Loading**
   - Load BERT model
   - Load GPT-2 model
   - Run inference
   - Understand outputs

4. **Explore Model Hub**
   - Browse available models
   - Test different architectures
   - Compare model sizes

---

## üí° Stretch Goals (Optional)
- Try multilingual models
- Experiment with different task types
- Explore community models
- Create model card
