{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9: Advanced CNN Architectures\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Mathematical Prerequisites:**\n",
    "- Linear algebra (matrix operations, dimensions)\n",
    "- Calculus (gradient flow through networks)\n",
    "- Understanding of convolutions from signal processing\n",
    "- Optimization theory (gradient vanishing/explosion)\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Today we implement famous CNN architectures from their original papers:\n",
    "1. **VGG Network** (Simonyan & Zisserman, 2014)\n",
    "2. **ResNet** (He et al., 2015)\n",
    "3. **Inception Module** (Szegedy et al., 2014)\n",
    "\n",
    "We'll understand:\n",
    "- Why deeper networks were difficult to train\n",
    "- How skip connections solve gradient vanishing\n",
    "- Mathematical analysis of gradient flow\n",
    "- Trade-offs between depth, width, and parameters\n",
    "\n",
    "**Goal:** Build ResNet from scratch and achieve competitive performance on CIFAR-10\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Theory - The Depth Problem\n",
    "\n",
    "### 1.1 The Vanishing/Exploding Gradient Problem\n",
    "\n",
    "For a deep network with $L$ layers:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta_1} = \\frac{\\partial L}{\\partial h_L} \\cdot \\frac{\\partial h_L}{\\partial h_{L-1}} \\cdot \\ldots \\cdot \\frac{\\partial h_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial \\theta_1}\n",
    "$$\n",
    "\n",
    "**Problem:** This is a product of $L$ terms. If each term has magnitude:\n",
    "- $< 1$: Gradients vanish exponentially $\\rightarrow$ Early layers don't learn\n",
    "- $> 1$: Gradients explode exponentially $\\rightarrow$ Training diverges\n",
    "\n",
    "**Empirical observation:** Very deep plain networks (no skip connections) have **higher training error** than shallower networks. This is NOT overfitting—it's optimization failure!\n",
    "\n",
    "### 1.2 The Degradation Problem\n",
    "\n",
    "**Hypothesis (He et al., 2015):** If deeper networks can represent identity mappings, they should at worst perform as well as shallower networks.\n",
    "\n",
    "**Reality:** Optimization algorithms struggle to learn identity mappings in deep plain networks.\n",
    "\n",
    "**Solution:** Make identity mapping **explicit** via skip connections.\n",
    "\n",
    "### 1.3 Residual Learning\n",
    "\n",
    "Instead of learning $H(x)$, learn $F(x) = H(x) - x$ (the residual).\n",
    "\n",
    "**Output:** $H(x) = F(x) + x$\n",
    "\n",
    "**Key insight:** If identity is optimal, $F(x) = 0$ is easier to learn than $H(x) = x$.\n",
    "\n",
    "**Gradient flow:**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\frac{\\partial H}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\left(1 + \\frac{\\partial F}{\\partial x}\\right)\n",
    "$$\n",
    "\n",
    "The \"1\" term ensures gradient always flows directly through skip connection, even if $\\frac{\\partial F}{\\partial x}$ is small!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 specific normalization\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: VGG Network\n",
    "\n",
    "### 3.1 VGG Architecture Philosophy\n",
    "\n",
    "**Key ideas from Simonyan & Zisserman (2014):**\n",
    "1. Use very small (3×3) convolutional filters\n",
    "2. Stack multiple conv layers before pooling\n",
    "3. Increase depth systematically\n",
    "\n",
    "**Why 3×3?**\n",
    "- Two 3×3 layers = one 5×5 receptive field\n",
    "- Three 3×3 layers = one 7×7 receptive field\n",
    "- **Fewer parameters:** $3 \\times (3^2 \\cdot C^2) = 27C^2$ vs $7^2 \\cdot C^2 = 49C^2$\n",
    "- **More non-linearities:** Each layer adds ReLU\n",
    "\n",
    "### 3.2 VGG16 Architecture\n",
    "\n",
    "```\n",
    "Input: 224×224×3\n",
    "|\n",
    "Conv3-64 × 2  →  MaxPool  →  112×112×64\n",
    "|\n",
    "Conv3-128 × 2  →  MaxPool  →  56×56×128\n",
    "|\n",
    "Conv3-256 × 3  →  MaxPool  →  28×28×256\n",
    "|\n",
    "Conv3-512 × 3  →  MaxPool  →  14×14×512\n",
    "|\n",
    "Conv3-512 × 3  →  MaxPool  →  7×7×512\n",
    "|\n",
    "FC-4096  →  FC-4096  →  FC-1000  →  Softmax\n",
    "```\n",
    "\n",
    "**Total parameters:** ~138 million (mostly in FC layers!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    \"\"\"VGG building block: stack of conv layers followed by maxpool.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, num_convs):\n",
    "        super(VGGBlock, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_convs):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Conv2d(in_channels, out_channels, 3, padding=1))\n",
    "            else:\n",
    "                layers.append(nn.Conv2d(out_channels, out_channels, 3, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        \n",
    "        layers.append(nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    \"\"\"VGG-style network adapted for CIFAR-10 (32×32 images).\n",
    "    \n",
    "    Architecture:\n",
    "    - 2 conv blocks with 64 and 128 channels\n",
    "    - 3 conv blocks with 256, 512, 512 channels\n",
    "    - FC layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGGNet, self).__init__()\n",
    "        \n",
    "        # Feature extractor (conv blocks)\n",
    "        self.features = nn.Sequential(\n",
    "            VGGBlock(3, 64, 2),      # 32 → 16\n",
    "            VGGBlock(64, 128, 2),    # 16 → 8\n",
    "            VGGBlock(128, 256, 3),   # 8 → 4\n",
    "            VGGBlock(256, 512, 3),   # 4 → 2\n",
    "            VGGBlock(512, 512, 3),   # 2 → 1\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create VGG model\n",
    "vgg_model = VGGNet(num_classes=10).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in vgg_model.parameters())\n",
    "print(f\"VGGNet Parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "output = vgg_model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ResNet (Residual Networks)\n",
    "\n",
    "### 4.1 Residual Block\n",
    "\n",
    "**Basic Block (ResNet-18, ResNet-34):**\n",
    "```\n",
    "x ───────────────────┐\n",
    "|                    |\n",
    "Conv3x3 → BN → ReLU  |\n",
    "|                    |\n",
    "Conv3x3 → BN         |\n",
    "|                    |\n",
    "+  ←─────────────────┘\n",
    "|\n",
    "ReLU\n",
    "```\n",
    "\n",
    "**Bottleneck Block (ResNet-50, 101, 152):**\n",
    "```\n",
    "x ───────────────────────┐\n",
    "|                        |\n",
    "Conv1x1 → BN → ReLU      | (reduce channels)\n",
    "|                        |\n",
    "Conv3x3 → BN → ReLU      | (process)\n",
    "|                        |\n",
    "Conv1x1 → BN             | (expand channels)\n",
    "|                        |\n",
    "+  ←─────────────────────┘\n",
    "|\n",
    "ReLU\n",
    "```\n",
    "\n",
    "### 4.2 Handling Dimension Mismatch\n",
    "\n",
    "When spatial dimensions or channels change, we need **projection shortcut:**\n",
    "$$\n",
    "y = F(x, W_i) + W_s x\n",
    "$$\n",
    "where $W_s$ is a 1×1 conv that matches dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic residual block for ResNet-18/34.\n",
    "    \n",
    "    Two 3x3 conv layers with skip connection.\n",
    "    \"\"\"\n",
    "    \n",
    "    expansion = 1  # Output channels = planes * expansion\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # First conv (may downsample spatial dimensions)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # Second conv (maintains dimensions)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # Skip connection (identity or projection)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            # Projection shortcut to match dimensions\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Main branch (residual)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Skip connection\n",
    "        out += self.shortcut(x)\n",
    "        \n",
    "        # Final ReLU\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    \"\"\"Bottleneck residual block for ResNet-50/101/152.\n",
    "    \n",
    "    1x1 reduce → 3x3 process → 1x1 expand.\n",
    "    More efficient for deeper networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    expansion = 4  # Output channels = planes * 4\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        \n",
    "        # 1x1 conv to reduce channels\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # 3x3 conv (may downsample)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        # 1x1 conv to expand channels\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1,\n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        \n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Complete ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"ResNet for CIFAR-10 (32×32 images).\n",
    "    \n",
    "    Modified from original (224×224) by:\n",
    "    - Smaller initial conv (3×3 instead of 7×7)\n",
    "    - No initial maxpool\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.in_planes = 64\n",
    "        \n",
    "        # Initial conv layer (no maxpool for CIFAR)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        \"\"\"Create a layer with multiple residual blocks.\"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        # Residual layers\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        # Global average pooling\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    \"\"\"ResNet-18: 18 layers with basic blocks.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def ResNet34():\n",
    "    \"\"\"ResNet-34: 34 layers with basic blocks.\"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet50():\n",
    "    \"\"\"ResNet-50: 50 layers with bottleneck blocks.\"\"\"\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "# Create ResNet-18\n",
    "resnet18 = ResNet18().to(device)\n",
    "resnet18_params = sum(p.numel() for p in resnet18.parameters())\n",
    "print(f\"ResNet-18 Parameters: {resnet18_params:,}\")\n",
    "\n",
    "# Create ResNet-34\n",
    "resnet34 = ResNet34().to(device)\n",
    "resnet34_params = sum(p.numel() for p in resnet34.parameters())\n",
    "print(f\"ResNet-34 Parameters: {resnet34_params:,}\")\n",
    "\n",
    "# Create ResNet-50\n",
    "resnet50 = ResNet50().to(device)\n",
    "resnet50_params = sum(p.numel() for p in resnet50.parameters())\n",
    "print(f\"ResNet-50 Parameters: {resnet50_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(2, 3, 32, 32).to(device)\n",
    "output = resnet18(dummy_input)\n",
    "print(f\"\\nResNet-18 output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Plain Network (No Skip Connections)\n",
    "\n",
    "For comparison, let's build a plain network with the same depth but NO skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainBlock(nn.Module):\n",
    "    \"\"\"Plain block WITHOUT skip connection.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PlainBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        return out  # NO skip connection!\n",
    "\n",
    "\n",
    "class PlainNet(nn.Module):\n",
    "    \"\"\"Plain network (no skip connections) for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PlainNet, self).__init__()\n",
    "        \n",
    "        self.in_planes = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def PlainNet18():\n",
    "    return PlainNet(PlainBlock, [2, 2, 2, 2])\n",
    "\n",
    "# Create plain network\n",
    "plain_net = PlainNet18().to(device)\n",
    "plain_params = sum(p.numel() for p in plain_net.parameters())\n",
    "print(f\"PlainNet-18 Parameters: {plain_params:,}\")\n",
    "print(f\"\\nNote: PlainNet has similar parameters to ResNet but NO skip connections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Inception Module (Optional)\n",
    "\n",
    "### 6.1 Multi-Scale Feature Extraction\n",
    "\n",
    "**Key insight:** Different objects/features may have different scales. Instead of choosing one filter size, use multiple in parallel!\n",
    "\n",
    "**Original Inception Module:**\n",
    "```\n",
    "Input\n",
    "  ├─ 1×1 conv ─────────────┐\n",
    "  ├─ 1×1 conv → 3×3 conv ──┤\n",
    "  ├─ 1×1 conv → 5×5 conv ──┤  Concatenate\n",
    "  └─ 3×3 maxpool → 1×1 conv┘\n",
    "```\n",
    "\n",
    "**1×1 convolutions:**\n",
    "- Reduce channels (dimensionality reduction)\n",
    "- Cross-channel information\n",
    "- Add non-linearity\n",
    "- Reduce computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    \"\"\"Simplified Inception module.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, ch1x1, ch3x3_reduce, ch3x3, \n",
    "                 ch5x5_reduce, ch5x5, pool_proj):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        # 1×1 branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch1x1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 3×3 branch (with 1×1 reduction)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch3x3_reduce, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch3x3_reduce),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch3x3_reduce, ch3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(ch3x3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 5×5 branch (with 1×1 reduction)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ch5x5_reduce, kernel_size=1),\n",
    "            nn.BatchNorm2d(ch5x5_reduce),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch5x5_reduce, ch5x5, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(ch5x5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Pool branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_proj),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        return torch.cat([out1, out2, out3, out4], dim=1)\n",
    "\n",
    "\n",
    "# Test Inception module\n",
    "inception = InceptionModule(64, ch1x1=16, ch3x3_reduce=24, ch3x3=32,\n",
    "                           ch5x5_reduce=8, ch5x5=16, pool_proj=16).to(device)\n",
    "dummy = torch.randn(2, 64, 32, 32).to(device)\n",
    "out = inception(dummy)\n",
    "print(f\"Inception module output shape: {out.shape}\")\n",
    "print(f\"Output channels: 16 + 32 + 16 + 16 = {16+32+16+16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Training and Comparison\n",
    "\n",
    "### 7.1 Training Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=20, lr=0.1):\n",
    "    \"\"\"Train model and track metrics.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / (pbar.n + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Testing\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * correct / total\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, '\n",
    "              f'Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%')\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'test_accs': test_accs,\n",
    "        'total_time': total_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Train ResNet-18 (Main Focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet-18\n",
    "print(\"=\"*60)\n",
    "print(\"Training ResNet-18\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resnet18 = ResNet18().to(device)\n",
    "resnet18_results = train_model(resnet18, train_loader, test_loader, num_epochs=20)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {resnet18_results['test_accs'][-1]:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {max(resnet18_results['test_accs']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Train Plain Network (for Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PlainNet-18 (no skip connections)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training PlainNet-18 (No Skip Connections)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plain_net = PlainNet18().to(device)\n",
    "plain_results = train_model(plain_net, train_loader, test_loader, num_epochs=20)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {plain_results['test_accs'][-1]:.2f}%\")\n",
    "print(f\"Best Test Accuracy: {max(plain_results['test_accs']):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparison and Analysis\n",
    "\n",
    "### 8.1 ResNet vs PlainNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(resnet18_results['train_losses'], label='ResNet-18', linewidth=2)\n",
    "axes[0].plot(plain_results['train_losses'], label='PlainNet-18', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Training Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy\n",
    "axes[1].plot(resnet18_results['train_accs'], label='ResNet-18', linewidth=2)\n",
    "axes[1].plot(plain_results['train_accs'], label='PlainNet-18', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Training Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "axes[2].plot(resnet18_results['test_accs'], label='ResNet-18', linewidth=2)\n",
    "axes[2].plot(plain_results['test_accs'], label='PlainNet-18', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[2].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=11)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Skip Connections: ResNet vs Plain Network', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'ResNet-18':>15} {'PlainNet-18':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Parameters':.<25} {resnet18_params:>15,} {plain_params:>15,}\")\n",
    "print(f\"{'Best Test Acc (%)':.<25} {max(resnet18_results['test_accs']):>15.2f} {max(plain_results['test_accs']):>15.2f}\")\n",
    "print(f\"{'Final Test Acc (%)':.<25} {resnet18_results['test_accs'][-1]:>15.2f} {plain_results['test_accs'][-1]:>15.2f}\")\n",
    "print(f\"{'Training Time (s)':.<25} {resnet18_results['total_time']:>15.2f} {plain_results['total_time']:>15.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improvement = max(resnet18_results['test_accs']) - max(plain_results['test_accs'])\n",
    "print(f\"\\nSkip connections improvement: +{improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Gradient Flow Analysis\n",
    "\n",
    "Let's analyze how gradients flow through the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norms(model, train_loader):\n",
    "    \"\"\"Get gradient norms for each layer.\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Get one batch\n",
    "    inputs, labels = next(iter(train_loader))\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward and backward\n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Collect gradient norms\n",
    "    grad_norms = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms[name] = param.grad.norm().item()\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# Get gradients for both models\n",
    "resnet_grads = get_gradient_norms(resnet18, train_loader)\n",
    "plain_grads = get_gradient_norms(plain_net, train_loader)\n",
    "\n",
    "# Filter conv layer gradients\n",
    "resnet_conv_grads = {k: v for k, v in resnet_grads.items() if 'conv' in k and 'weight' in k}\n",
    "plain_conv_grads = {k: v for k, v in plain_grads.items() if 'conv' in k and 'weight' in k}\n",
    "\n",
    "# Plot gradient norms by layer depth\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ResNet gradients\n",
    "resnet_values = list(resnet_conv_grads.values())\n",
    "ax1.bar(range(len(resnet_values)), resnet_values, alpha=0.7)\n",
    "ax1.set_xlabel('Layer (depth)', fontsize=12)\n",
    "ax1.set_ylabel('Gradient Norm', fontsize=12)\n",
    "ax1.set_title('ResNet-18: Gradient Norms by Layer', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# PlainNet gradients\n",
    "plain_values = list(plain_conv_grads.values())\n",
    "ax2.bar(range(len(plain_values)), plain_values, alpha=0.7, color='orange')\n",
    "ax2.set_xlabel('Layer (depth)', fontsize=12)\n",
    "ax2.set_ylabel('Gradient Norm', fontsize=12)\n",
    "ax2.set_title('PlainNet-18: Gradient Norms by Layer', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ResNet gradient norm range: [{min(resnet_values):.6f}, {max(resnet_values):.6f}]\")\n",
    "print(f\"PlainNet gradient norm range: [{min(plain_values):.6f}, {max(plain_values):.6f}]\")\n",
    "print(f\"\\nGradient range ratio: {max(resnet_values)/min(resnet_values):.2f} (ResNet) vs \"\n",
    "      f\"{max(plain_values)/min(plain_values):.2f} (PlainNet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Architecture Comparison Summary\n",
    "\n",
    "### 9.1 Parameters vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "architectures = {\n",
    "    'VGGNet': {\n",
    "        'params': total_params,  # VGG params from earlier\n",
    "        'depth': '16 conv + 3 FC',\n",
    "        'key_feature': 'Stacked 3x3 convs',\n",
    "        'year': 2014\n",
    "    },\n",
    "    'ResNet-18': {\n",
    "        'params': resnet18_params,\n",
    "        'depth': '18 layers',\n",
    "        'key_feature': 'Skip connections',\n",
    "        'year': 2015\n",
    "    },\n",
    "    'ResNet-34': {\n",
    "        'params': resnet34_params,\n",
    "        'depth': '34 layers',\n",
    "        'key_feature': 'Skip connections',\n",
    "        'year': 2015\n",
    "    },\n",
    "    'ResNet-50': {\n",
    "        'params': resnet50_params,\n",
    "        'depth': '50 layers',\n",
    "        'key_feature': 'Bottleneck blocks',\n",
    "        'year': 2015\n",
    "    },\n",
    "    'PlainNet-18': {\n",
    "        'params': plain_params,\n",
    "        'depth': '18 layers',\n",
    "        'key_feature': 'No skip connections',\n",
    "        'year': '-'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nArchitecture Comparison:\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Architecture':<15} {'Parameters':>15} {'Depth':<20} {'Key Feature':<25} {'Year':>6}\")\n",
    "print(\"-\"*90)\n",
    "for name, info in architectures.items():\n",
    "    print(f\"{name:<15} {info['params']:>15,} {info['depth']:<20} {info['key_feature']:<25} {info['year']:>6}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Visualize parameters\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "names = list(architectures.keys())\n",
    "params = [architectures[n]['params'] for n in names]\n",
    "\n",
    "bars = ax.bar(names, [p/1e6 for p in params], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Parameters (Millions)', fontsize=12)\n",
    "ax.set_title('Model Complexity: Parameter Count', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, p in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "            f'{p/1e6:.2f}M', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Key Takeaways and Mathematical Insights\n",
    "\n",
    "### 10.1 Why Skip Connections Work\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "For a residual block $H(x) = F(x) + x$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_l} = \\frac{\\partial L}{\\partial x_L} \\cdot \\frac{\\partial x_L}{\\partial x_l}\n",
    "$$\n",
    "\n",
    "For ResNets:\n",
    "$$\n",
    "x_L = x_l + \\sum_{i=l}^{L-1} F(x_i)\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "$$\n",
    "\\frac{\\partial x_L}{\\partial x_l} = 1 + \\frac{\\partial}{\\partial x_l}\\sum_{i=l}^{L-1} F(x_i)\n",
    "$$\n",
    "\n",
    "**The \"+1\" term ensures gradient always flows**, regardless of how small the residual gradients are!\n",
    "\n",
    "### 10.2 Architecture Design Principles\n",
    "\n",
    "**VGG:**\n",
    "- Simplicity: All 3×3 convolutions\n",
    "- Pros: Easy to understand and implement\n",
    "- Cons: Very large (138M parameters), slow to train\n",
    "\n",
    "**ResNet:**\n",
    "- Skip connections enable very deep networks (100+ layers)\n",
    "- Pros: Efficient, scalable, state-of-the-art performance\n",
    "- Cons: More complex design\n",
    "\n",
    "**Inception:**\n",
    "- Multi-scale feature extraction\n",
    "- Pros: Captures features at different scales efficiently\n",
    "- Cons: Complex architecture, harder to tune\n",
    "\n",
    "### 10.3 Modern Best Practices\n",
    "\n",
    "1. **Use skip connections** for networks deeper than ~20 layers\n",
    "2. **Batch Normalization** after every conv layer\n",
    "3. **Global Average Pooling** instead of FC layers (reduces parameters)\n",
    "4. **Bottleneck blocks** for very deep networks (computational efficiency)\n",
    "5. **Data augmentation** is crucial for generalization\n",
    "6. **Learning rate scheduling** (cosine annealing, step decay)\n",
    "7. **Regularization** (weight decay, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've implemented famous CNN architectures from their original papers. You now understand:\n",
    "\n",
    "✅ **VGG Network:** Simplicity of stacked 3×3 convolutions  \n",
    "✅ **ResNet:** Skip connections solve gradient vanishing  \n",
    "✅ **Inception Module:** Multi-scale feature extraction  \n",
    "✅ **The Degradation Problem:** Why plain deep networks fail  \n",
    "✅ **Residual Learning:** Learn $F(x) = H(x) - x$ instead of $H(x)$  \n",
    "✅ **Gradient Flow Analysis:** Mathematical proof of why skip connections help  \n",
    "✅ **Architecture Trade-offs:** Parameters vs depth vs performance  \n",
    "\n",
    "**Key Achievement:**\n",
    "- Implemented ResNet-18, ResNet-34, ResNet-50 from scratch\n",
    "- Demonstrated skip connection benefit (ResNet vs PlainNet)\n",
    "- Achieved competitive accuracy on CIFAR-10\n",
    "\n",
    "**Mathematical Insight:**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial H} \\cdot \\left(1 + \\frac{\\partial F}{\\partial x}\\right)\n",
    "$$\n",
    "The \"+1\" ensures gradient flow even when residual gradients are small!\n",
    "\n",
    "**Time spent:** ~3-4 hours\n",
    "\n",
    "**Next:** Day 10 - Project 1: MNIST Classification (portfolio-quality project combining all learnings from Days 1-9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
