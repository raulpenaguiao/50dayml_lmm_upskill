{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 15: Attention Mechanisms and Transformers\n",
    "\n",
    "## Phase 2: NLP Basics (Days 11-20)\n",
    "\n",
    "**Estimated Time: 4-5 hours**\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand attention as a soft addressing mechanism\n",
    "- Implement Bahdanau (additive) and Luong (multiplicative) attention\n",
    "- Master the Transformer architecture from \"Attention Is All You Need\"\n",
    "- Implement self-attention and multi-head attention from scratch\n",
    "- Understand positional encoding for sequence order\n",
    "- Build a complete Transformer encoder\n",
    "- Appreciate why Transformers revolutionized NLP\n",
    "\n",
    "### Prerequisites\n",
    "- Day 14: LSTM and GRU Networks\n",
    "- Understanding of sequence-to-sequence models\n",
    "- Linear algebra (matrix operations, softmax)\n",
    "- Neural network fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Bottleneck Problem\n",
    "\n",
    "### 1.1 Sequence-to-Sequence Limitation\n",
    "\n",
    "In encoder-decoder architectures (Day 14):\n",
    "- Encoder compresses entire input into fixed-size vector\n",
    "- This single vector must capture all information\n",
    "- **Bottleneck**: Long sequences lose information\n",
    "\n",
    "**Example**: Translating a long sentence\n",
    "- First words may be forgotten by the time encoding is complete\n",
    "- Decoder has no way to \"look back\" at specific input positions\n",
    "\n",
    "### 1.2 Attention: The Solution\n",
    "\n",
    "**Key Idea**: Instead of single context vector, allow decoder to \"attend\" to all encoder states.\n",
    "\n",
    "$$\\text{context}_t = \\sum_{s=1}^{S} \\alpha_{t,s} \\cdot h_s$$\n",
    "\n",
    "Where:\n",
    "- $h_s$: Encoder hidden state at position $s$\n",
    "- $\\alpha_{t,s}$: Attention weight (how much to focus on position $s$ when generating token $t$)\n",
    "- $\\sum_s \\alpha_{t,s} = 1$ (normalized via softmax)\n",
    "\n",
    "**Intuition**: Like a spotlight that moves across the input, focusing on relevant parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bottleneck problem and attention solution\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 1. Bottleneck Problem\n",
    "ax = axes[0]\n",
    "ax.set_title('Seq2Seq Bottleneck Problem', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Encoder states\n",
    "for i in range(5):\n",
    "    rect = plt.Rectangle((i*1.5, 3), 1, 1, color='lightblue', alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i*1.5 + 0.5, 3.5, f'$h_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "    if i < 4:\n",
    "        ax.arrow(i*1.5 + 1, 3.5, 0.4, 0, head_width=0.1, fc='black')\n",
    "\n",
    "# Final context vector (bottleneck)\n",
    "circle = plt.Circle((7.5, 3.5), 0.6, color='red', alpha=0.8)\n",
    "ax.add_patch(circle)\n",
    "ax.text(7.5, 3.5, 'c', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "ax.text(7.5, 2.5, 'BOTTLENECK\\n(single vector)', ha='center', fontsize=10, color='red')\n",
    "\n",
    "# Arrow to decoder\n",
    "ax.arrow(7.5, 2.9, 0, -1, head_width=0.2, fc='red')\n",
    "\n",
    "# Decoder states\n",
    "for i in range(3):\n",
    "    rect = plt.Rectangle((i*2 + 2, 0.5), 1, 1, color='lightgreen', alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i*2 + 2.5, 1, f'$s_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "\n",
    "ax.text(4.5, 4.8, 'Encoder', ha='center', fontsize=12)\n",
    "ax.text(4.5, -0.2, 'Decoder', ha='center', fontsize=12)\n",
    "ax.set_xlim(-0.5, 9)\n",
    "ax.set_ylim(-0.5, 5.5)\n",
    "ax.axis('off')\n",
    "\n",
    "# 2. Attention Solution\n",
    "ax = axes[1]\n",
    "ax.set_title('Attention Mechanism Solution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Encoder states\n",
    "for i in range(5):\n",
    "    rect = plt.Rectangle((i*1.5, 4), 1, 1, color='lightblue', alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i*1.5 + 0.5, 4.5, f'$h_{i+1}$', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Decoder state\n",
    "rect = plt.Rectangle((3.5, 1), 1, 1, color='lightgreen', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(4, 1.5, '$s_t$', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Attention weights (arrows with different widths)\n",
    "attention_weights = [0.05, 0.1, 0.5, 0.3, 0.05]  # Focus on position 3\n",
    "colors = plt.cm.Reds(np.array(attention_weights) / max(attention_weights))\n",
    "\n",
    "for i, (w, c) in enumerate(zip(attention_weights, colors)):\n",
    "    ax.plot([i*1.5 + 0.5, 4], [4, 2], color=c, linewidth=w*10 + 0.5, alpha=0.8)\n",
    "    ax.text(i*1.5 + 0.5, 3.3, f'{w:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# Context vector\n",
    "circle = plt.Circle((4, 2.7), 0.3, color='purple', alpha=0.8)\n",
    "ax.add_patch(circle)\n",
    "ax.text(4, 2.7, 'c', ha='center', va='center', fontsize=10, color='white')\n",
    "ax.text(5, 2.7, '$c_t = \\\\sum_i \\\\alpha_i h_i$', fontsize=11)\n",
    "\n",
    "ax.text(3.5, 5.3, 'Encoder', ha='center', fontsize=12)\n",
    "ax.text(4, 0.3, 'Decoder', ha='center', fontsize=12)\n",
    "ax.text(7, 3.5, 'Attention\\nweights', ha='center', fontsize=10, color='red')\n",
    "\n",
    "ax.set_xlim(-0.5, 9)\n",
    "ax.set_ylim(-0.5, 6)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: Information compressed into single vector (bottleneck)\")\n",
    "print(\"Right: Decoder can attend to all encoder states with learned weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Mechanisms\n",
    "\n",
    "### 2.1 General Attention Framework\n",
    "\n",
    "Attention computes:\n",
    "1. **Score**: How relevant is each source position?\n",
    "2. **Weights**: Normalize scores (softmax)\n",
    "3. **Context**: Weighted sum of values\n",
    "\n",
    "$$\\text{score}(h_t, h_s) = \\text{alignment function}$$\n",
    "$$\\alpha_{t,s} = \\frac{\\exp(\\text{score}(h_t, h_s))}{\\sum_{s'} \\exp(\\text{score}(h_t, h_{s'}))}$$\n",
    "$$c_t = \\sum_s \\alpha_{t,s} h_s$$\n",
    "\n",
    "### 2.2 Bahdanau Attention (Additive)\n",
    "\n",
    "Also called \"concat\" attention:\n",
    "$$\\text{score}(h_t, h_s) = v^T \\tanh(W_1 h_t + W_2 h_s)$$\n",
    "\n",
    "### 2.3 Luong Attention (Multiplicative)\n",
    "\n",
    "Also called \"dot-product\" attention:\n",
    "$$\\text{score}(h_t, h_s) = h_t^T W h_s \\quad \\text{(general)}$$\n",
    "$$\\text{score}(h_t, h_s) = h_t^T h_s \\quad \\text{(dot)}$$\n",
    "\n",
    "Multiplicative is faster, additive sometimes more expressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bahdanau (Additive) Attention.\n",
    "    \n",
    "    score(h_t, h_s) = v^T tanh(W_1 h_t + W_2 h_s)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch, hidden_dim] - decoder state\n",
    "        keys: [batch, seq_len, hidden_dim] - encoder states\n",
    "        values: [batch, seq_len, hidden_dim] - encoder states (usually same as keys)\n",
    "        mask: [batch, seq_len] - optional mask for padding\n",
    "        \"\"\"\n",
    "        # Expand query for broadcasting\n",
    "        query = query.unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Compute scores\n",
    "        # W1(query): [batch, 1, hidden_dim]\n",
    "        # W2(keys): [batch, seq_len, hidden_dim]\n",
    "        scores = self.v(torch.tanh(self.W1(query) + self.W2(keys)))  # [batch, seq_len, 1]\n",
    "        scores = scores.squeeze(-1)  # [batch, seq_len]\n",
    "        \n",
    "        # Apply mask (set padded positions to -inf)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax to get weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # [batch, seq_len]\n",
    "        \n",
    "        # Compute context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), values)  # [batch, 1, hidden_dim]\n",
    "        context = context.squeeze(1)  # [batch, hidden_dim]\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Luong (Multiplicative) Attention.\n",
    "    \n",
    "    Three variants:\n",
    "    - dot: score = h_t^T h_s\n",
    "    - general: score = h_t^T W h_s\n",
    "    - concat: score = v^T tanh(W[h_t; h_s]) (similar to Bahdanau)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, method='general'):\n",
    "        super().__init__()\n",
    "        self.method = method\n",
    "        \n",
    "        if method == 'general':\n",
    "            self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        elif method == 'concat':\n",
    "            self.W = nn.Linear(hidden_dim * 2, hidden_dim, bias=False)\n",
    "            self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch, hidden_dim]\n",
    "        keys: [batch, seq_len, hidden_dim]\n",
    "        values: [batch, seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        if self.method == 'dot':\n",
    "            # [batch, seq_len]\n",
    "            scores = torch.bmm(keys, query.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            # [batch, seq_len]\n",
    "            scores = torch.bmm(self.W(keys), query.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            query_expanded = query.unsqueeze(1).expand(-1, keys.size(1), -1)\n",
    "            concat = torch.cat([query_expanded, keys], dim=-1)\n",
    "            scores = self.v(torch.tanh(self.W(concat))).squeeze(-1)\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Context\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), values).squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "# Test attention mechanisms\n",
    "print(\"Attention Mechanism Implementations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "hidden_dim = 64\n",
    "\n",
    "# Dummy data\n",
    "query = torch.randn(batch_size, hidden_dim)\n",
    "keys = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "values = keys.clone()  # Usually same as keys\n",
    "\n",
    "# Test Bahdanau\n",
    "bahdanau = BahdanauAttention(hidden_dim)\n",
    "context_b, weights_b = bahdanau(query, keys, values)\n",
    "print(f\"Bahdanau Attention:\")\n",
    "print(f\"  Context shape: {context_b.shape}\")\n",
    "print(f\"  Weights shape: {weights_b.shape}\")\n",
    "print(f\"  Weights sum: {weights_b.sum(dim=-1)}\")\n",
    "\n",
    "# Test Luong variants\n",
    "for method in ['dot', 'general']:\n",
    "    luong = LuongAttention(hidden_dim, method=method)\n",
    "    context_l, weights_l = luong(query, keys, values)\n",
    "    print(f\"\\nLuong ({method}) Attention:\")\n",
    "    print(f\"  Context shape: {context_l.shape}\")\n",
    "    print(f\"  Weights sum: {weights_l.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights\n",
    "\n",
    "# Simulate translation attention\n",
    "source_sentence = \"The cat sat on the mat\".split()\n",
    "target_sentence = \"Le chat assis sur le tapis\".split()\n",
    "\n",
    "# Simulated attention weights (what we'd expect)\n",
    "attention_matrix = np.array([\n",
    "    [0.8, 0.1, 0.0, 0.0, 0.1, 0.0],  # Le -> The\n",
    "    [0.1, 0.8, 0.0, 0.0, 0.1, 0.0],  # chat -> cat\n",
    "    [0.0, 0.1, 0.8, 0.1, 0.0, 0.0],  # assis -> sat\n",
    "    [0.0, 0.0, 0.1, 0.7, 0.1, 0.1],  # sur -> on\n",
    "    [0.0, 0.0, 0.0, 0.1, 0.8, 0.1],  # le -> the\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.1, 0.9],  # tapis -> mat\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "\n",
    "plt.xticks(range(len(source_sentence)), source_sentence, rotation=45, ha='right')\n",
    "plt.yticks(range(len(target_sentence)), target_sentence)\n",
    "\n",
    "plt.xlabel('Source (English)', fontsize=12)\n",
    "plt.ylabel('Target (French)', fontsize=12)\n",
    "plt.title('Attention Weights in Translation\\n(Simulated)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Annotate with values\n",
    "for i in range(len(target_sentence)):\n",
    "    for j in range(len(source_sentence)):\n",
    "        plt.text(j, i, f'{attention_matrix[i, j]:.1f}', ha='center', va='center',\n",
    "                color='white' if attention_matrix[i, j] > 0.5 else 'black', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Attention learns word alignments automatically!\")\n",
    "print(\"Each target word focuses on relevant source word(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Attention: Attending to Yourself\n",
    "\n",
    "### 3.1 From Cross-Attention to Self-Attention\n",
    "\n",
    "**Cross-attention**: Query from decoder, keys/values from encoder\n",
    "- \"What parts of the input should I focus on?\"\n",
    "\n",
    "**Self-attention**: Query, keys, and values all from same sequence\n",
    "- \"How do different parts of this sequence relate to each other?\"\n",
    "\n",
    "### 3.2 Query-Key-Value Framework\n",
    "\n",
    "For each position $i$ in sequence:\n",
    "- **Query** $q_i = W_Q x_i$: \"What am I looking for?\"\n",
    "- **Key** $k_i = W_K x_i$: \"What do I contain?\"\n",
    "- **Value** $v_i = W_V x_i$: \"What information do I provide?\"\n",
    "\n",
    "**Attention**:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "The $\\sqrt{d_k}$ scaling prevents softmax saturation for high dimensions.\n",
    "\n",
    "### 3.3 Why Self-Attention?\n",
    "\n",
    "1. **Captures long-range dependencies**: Direct connection between any two positions\n",
    "2. **Parallelizable**: No sequential computation like RNNs\n",
    "3. **Interpretable**: Attention weights show what model focuses on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=None):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch, ..., seq_len_q, d_k]\n",
    "        key: [batch, ..., seq_len_k, d_k]\n",
    "        value: [batch, ..., seq_len_v, d_v] (seq_len_v == seq_len_k)\n",
    "        mask: [batch, ..., seq_len_q, seq_len_k]\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        \n",
    "        if self.temperature is None:\n",
    "            temperature = math.sqrt(d_k)\n",
    "        else:\n",
    "            temperature = self.temperature\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # [batch, ..., seq_len_q, seq_len_k]\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / temperature\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # [batch, ..., seq_len_q, d_v]\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention layer.\n",
    "    \n",
    "    Projects input to Q, K, V and computes scaled dot-product attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Projection matrices\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "        \n",
    "        # Self-attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test self-attention\n",
    "print(\"Self-Attention Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 64\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "self_attn = SelfAttention(d_model)\n",
    "\n",
    "output, weights = self_attn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights for first sample:\")\n",
    "print(weights[0].detach().numpy().round(3))\n",
    "print(f\"\\nEach row sums to 1: {weights[0].sum(dim=-1).detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize self-attention pattern\n",
    "\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "seq_len = len(sentence)\n",
    "d_model = 32\n",
    "\n",
    "# Create embeddings (random for demo)\n",
    "embeddings = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Compute self-attention\n",
    "self_attn = SelfAttention(d_model)\n",
    "_, attention_weights = self_attn(embeddings)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 1. Attention weights matrix\n",
    "ax = axes[0]\n",
    "weights_np = attention_weights[0].detach().numpy()\n",
    "im = ax.imshow(weights_np, cmap='Blues', aspect='auto')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(sentence)\n",
    "ax.set_yticklabels(sentence)\n",
    "ax.set_xlabel('Keys (attending to)')\n",
    "ax.set_ylabel('Queries (from)')\n",
    "ax.set_title('Self-Attention Weights')\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax.text(j, i, f'{weights_np[i, j]:.2f}', ha='center', va='center', fontsize=9)\n",
    "\n",
    "# 2. Illustration of self-attention connections\n",
    "ax = axes[1]\n",
    "ax.set_title('Self-Attention: Each position attends to all positions')\n",
    "\n",
    "# Draw words\n",
    "for i, word in enumerate(sentence):\n",
    "    rect = plt.Rectangle((i*2, 0), 1.5, 0.8, color='lightblue', alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(i*2 + 0.75, 0.4, word, ha='center', va='center', fontsize=11)\n",
    "\n",
    "# Draw attention from \"sat\" (index 2)\n",
    "query_idx = 2\n",
    "query_weights = weights_np[query_idx]\n",
    "\n",
    "for i, w in enumerate(query_weights):\n",
    "    # Arrow from query to key\n",
    "    if w > 0.05:  # Only show significant connections\n",
    "        ax.annotate('', \n",
    "                   xy=(i*2 + 0.75, 0.8), \n",
    "                   xytext=(query_idx*2 + 0.75, 2),\n",
    "                   arrowprops=dict(arrowstyle='->', \n",
    "                                  color=plt.cm.Reds(w), \n",
    "                                  lw=w*10,\n",
    "                                  alpha=0.8))\n",
    "        ax.text(i*2 + 0.75, 1.3, f'{w:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# Query position\n",
    "rect = plt.Rectangle((query_idx*2, 2), 1.5, 0.8, color='salmon', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(query_idx*2 + 0.75, 2.4, f'Query: \"{sentence[query_idx]}\"', ha='center', va='center', fontsize=11)\n",
    "\n",
    "ax.set_xlim(-0.5, 10)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.suptitle('Self-Attention Visualization', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Attention\n",
    "\n",
    "### 4.1 Motivation\n",
    "\n",
    "Single attention head focuses on one type of relationship.\n",
    "\n",
    "**Multi-head attention**: Multiple attention heads in parallel\n",
    "- Each head can learn different relationships\n",
    "- One head: syntactic relationships\n",
    "- Another head: semantic relationships\n",
    "- Another head: coreference resolution\n",
    "\n",
    "### 4.2 Mathematics\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where:\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "- Split $d_{model}$ into $h$ heads, each of dimension $d_k = d_{model} / h$\n",
    "- Learn different attention patterns\n",
    "- Concatenate and project back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention from \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: [batch, seq_len_q, d_model]\n",
    "        key: [batch, seq_len_k, d_model]\n",
    "        value: [batch, seq_len_v, d_model]\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        Q = self.W_Q(query)  # [batch, seq_len, d_model]\n",
    "        K = self.W_K(key)\n",
    "        V = self.W_V(value)\n",
    "        \n",
    "        # 2. Reshape to [batch, num_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. Apply attention\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # Add head dimension\n",
    "        \n",
    "        attn_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "        # attn_output: [batch, num_heads, seq_len, d_k]\n",
    "        # attention_weights: [batch, num_heads, seq_len_q, seq_len_k]\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
    "        # [batch, seq_len, d_model]\n",
    "        \n",
    "        # 5. Final projection\n",
    "        output = self.W_O(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "print(\"Multi-Head Attention Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Self-attention: query = key = value = x\n",
    "output, weights = mha(x, x, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"  - {num_heads} attention heads\")\n",
    "print(f\"  - Each head has d_k = {d_model // num_heads}\")\n",
    "\n",
    "# Parameter count\n",
    "params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"\\nTotal parameters: {params:,}\")\n",
    "print(f\"  W_Q, W_K, W_V: {d_model * d_model} each\")\n",
    "print(f\"  W_O: {d_model * d_model}\")\n",
    "print(f\"  Total: 4 * {d_model}^2 = {4 * d_model**2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple attention heads\n",
    "\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]\n",
    "seq_len = len(sentence)\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "x = torch.randn(1, seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "_, attention_weights = mha(x, x, x)\n",
    "\n",
    "# attention_weights: [1, num_heads, seq_len, seq_len]\n",
    "weights_np = attention_weights[0].detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    im = ax.imshow(weights_np[head_idx], cmap='viridis', aspect='auto')\n",
    "    \n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(sentence, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(sentence)\n",
    "    ax.set_title(f'Head {head_idx + 1}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    if head_idx >= 2:\n",
    "        ax.set_xlabel('Keys')\n",
    "    if head_idx % 2 == 0:\n",
    "        ax.set_ylabel('Queries')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: Different Heads Learn Different Patterns',\n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each head can specialize in different types of relationships:\")\n",
    "print(\"- Syntactic structure (subject-verb)\")\n",
    "print(\"- Semantic similarity\")\n",
    "print(\"- Positional patterns\")\n",
    "print(\"- Coreference (pronouns to nouns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding\n",
    "\n",
    "### 5.1 The Position Problem\n",
    "\n",
    "Self-attention is **permutation invariant**:\n",
    "- \"The cat sat\" and \"sat cat The\" give same attention weights\n",
    "- Order information is lost!\n",
    "\n",
    "**Solution**: Add position information to embeddings\n",
    "\n",
    "### 5.2 Sinusoidal Positional Encoding\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "**Properties:**\n",
    "1. Unique encoding for each position\n",
    "2. Fixed for all sequences (no learning needed)\n",
    "3. Can generalize to longer sequences\n",
    "4. Relative positions are easy to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding from the Transformer paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Compute the positional encodings\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_seq_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "pe = PositionalEncoding(d_model, max_len, dropout=0.0)\n",
    "pe_values = pe.pe[0].numpy()  # [max_len, d_model]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Full encoding matrix\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(pe_values[:50, :64], cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_xlabel('Embedding Dimension')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Positional Encoding Matrix (first 50 pos, 64 dims)')\n",
    "\n",
    "# 2. Individual dimensions\n",
    "ax = axes[0, 1]\n",
    "positions = range(50)\n",
    "for dim in [0, 1, 10, 11, 50, 51]:\n",
    "    ax.plot(positions, pe_values[:50, dim], label=f'dim {dim}')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Encoding Value')\n",
    "ax.set_title('Positional Encoding for Different Dimensions')\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Frequency vs dimension\n",
    "ax = axes[1, 0]\n",
    "dims = range(0, d_model, 2)\n",
    "wavelengths = 2 * np.pi * (10000 ** (np.array(dims) / d_model))\n",
    "ax.plot(dims, wavelengths)\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Wavelength')\n",
    "ax.set_title('Wavelength of Sinusoid by Dimension')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Similarity between positions\n",
    "ax = axes[1, 1]\n",
    "# Compute cosine similarity between positions\n",
    "num_positions = 30\n",
    "pos_encodings = pe_values[:num_positions]\n",
    "norms = np.linalg.norm(pos_encodings, axis=1, keepdims=True)\n",
    "normalized = pos_encodings / norms\n",
    "similarity = normalized @ normalized.T\n",
    "\n",
    "im = ax.imshow(similarity, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('Position')\n",
    "ax.set_title('Cosine Similarity Between Positional Encodings')\n",
    "\n",
    "plt.suptitle('Sinusoidal Positional Encoding Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insights:\")\n",
    "print(\"1. Low dimensions: High frequency (rapid changes with position)\")\n",
    "print(\"2. High dimensions: Low frequency (slow changes)\")\n",
    "print(\"3. Each position has unique encoding\")\n",
    "print(\"4. Nearby positions have higher similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Transformer Architecture\n",
    "\n",
    "### 6.1 Overview\n",
    "\n",
    "The Transformer (Vaswani et al., 2017) uses:\n",
    "- **Only attention**, no recurrence\n",
    "- **Encoder-Decoder** structure\n",
    "- **Multi-head self-attention**\n",
    "- **Position-wise feedforward networks**\n",
    "- **Residual connections and layer normalization**\n",
    "\n",
    "### 6.2 Encoder Block\n",
    "\n",
    "Each encoder layer:\n",
    "1. Multi-Head Self-Attention\n",
    "2. Add & Norm (residual connection + layer norm)\n",
    "3. Position-wise Feed-Forward Network\n",
    "4. Add & Norm\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "### 6.3 Decoder Block\n",
    "\n",
    "Each decoder layer:\n",
    "1. Masked Multi-Head Self-Attention\n",
    "2. Add & Norm\n",
    "3. Multi-Head Cross-Attention (to encoder)\n",
    "4. Add & Norm\n",
    "5. Position-wise Feed-Forward Network\n",
    "6. Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, attention_weights = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, \n",
    "                 max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len] token indices\n",
    "        \"\"\"\n",
    "        # Embed tokens and scale by sqrt(d_model)\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        attention_weights_all = []\n",
    "        for layer in self.layers:\n",
    "            x, attn_weights = layer(x, mask)\n",
    "            attention_weights_all.append(attn_weights)\n",
    "        \n",
    "        return x, attention_weights_all\n",
    "\n",
    "# Build Transformer Encoder\n",
    "print(\"Transformer Encoder Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vocab_size = 10000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Model dimension: {d_model}\")\n",
    "print(f\"  Number of heads: {num_heads}\")\n",
    "print(f\"  Feed-forward dimension: {d_ff}\")\n",
    "print(f\"  Number of layers: {num_layers}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Test\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "output, attention_weights = encoder(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention weight tensors: {len(attention_weights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Transformer architecture\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 14))\n",
    "\n",
    "# Colors\n",
    "emb_color = '#90EE90'\n",
    "attn_color = '#FFB6C1'\n",
    "ffn_color = '#87CEEB'\n",
    "norm_color = '#DDA0DD'\n",
    "\n",
    "y_start = 12\n",
    "\n",
    "# Input\n",
    "rect = plt.Rectangle((3, y_start), 4, 0.6, color='lightgray', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(5, y_start + 0.3, 'Input Tokens', ha='center', va='center', fontsize=11)\n",
    "\n",
    "# Embedding\n",
    "rect = plt.Rectangle((3, y_start - 1.2), 4, 0.6, color=emb_color, alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(5, y_start - 0.9, 'Token Embedding + Positional Encoding', ha='center', va='center', fontsize=10)\n",
    "ax.arrow(5, y_start, 0, -0.3, head_width=0.1, fc='black')\n",
    "\n",
    "# Encoder layers (draw 2 as example)\n",
    "for layer_idx in range(2):\n",
    "    y_offset = layer_idx * 4\n",
    "    y_base = y_start - 2.5 - y_offset\n",
    "    \n",
    "    # Layer box\n",
    "    rect = plt.Rectangle((2.5, y_base - 3.5), 5, 3.8, fill=False, \n",
    "                         edgecolor='black', linewidth=2, linestyle='--')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.5, y_base - 1.7, f'Encoder\\nLayer {layer_idx + 1}', ha='center', \n",
    "           va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Multi-Head Attention\n",
    "    rect = plt.Rectangle((3, y_base - 0.8), 4, 0.6, color=attn_color, alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(5, y_base - 0.5, 'Multi-Head\\nSelf-Attention', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Add & Norm\n",
    "    rect = plt.Rectangle((3, y_base - 1.8), 4, 0.6, color=norm_color, alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(5, y_base - 1.5, 'Add & Norm', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Feed-Forward\n",
    "    rect = plt.Rectangle((3, y_base - 2.8), 4, 0.6, color=ffn_color, alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(5, y_base - 2.5, 'Feed Forward', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Add & Norm\n",
    "    rect = plt.Rectangle((3, y_base - 3.8), 4, 0.6, color=norm_color, alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(5, y_base - 3.5, 'Add & Norm', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Arrows\n",
    "    ax.arrow(5, y_base, 0, -0.1, head_width=0.1, fc='black')\n",
    "    ax.arrow(5, y_base - 0.8, 0, -0.3, head_width=0.1, fc='black')\n",
    "    ax.arrow(5, y_base - 1.8, 0, -0.3, head_width=0.1, fc='black')\n",
    "    ax.arrow(5, y_base - 2.8, 0, -0.3, head_width=0.1, fc='black')\n",
    "    \n",
    "    # Residual connections\n",
    "    ax.plot([7.5, 8, 8, 7.5], [y_base, y_base, y_base - 1.5, y_base - 1.5], 'g-', linewidth=2)\n",
    "    ax.plot([7.5, 8, 8, 7.5], [y_base - 1.8, y_base - 1.8, y_base - 3.5, y_base - 3.5], 'g-', linewidth=2)\n",
    "\n",
    "# ... (more layers)\n",
    "ax.text(5, y_start - 11, '⋮', ha='center', fontsize=20)\n",
    "ax.text(5, y_start - 11.5, f'(×{num_layers} layers)', ha='center', fontsize=10)\n",
    "\n",
    "# Output\n",
    "rect = plt.Rectangle((3, y_start - 13), 4, 0.6, color='lightyellow', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(5, y_start - 12.7, 'Encoder Output', ha='center', va='center', fontsize=11)\n",
    "\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(-2, 13)\n",
    "ax.axis('off')\n",
    "ax.set_title('Transformer Encoder Architecture', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Rectangle((0, 0), 1, 1, color=emb_color, alpha=0.8, label='Embedding'),\n",
    "    plt.Rectangle((0, 0), 1, 1, color=attn_color, alpha=0.8, label='Attention'),\n",
    "    plt.Rectangle((0, 0), 1, 1, color=ffn_color, alpha=0.8, label='Feed-Forward'),\n",
    "    plt.Rectangle((0, 0), 1, 1, color=norm_color, alpha=0.8, label='Layer Norm'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Transformer for Classification\n",
    "\n",
    "### 7.1 Using Encoder for Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder for sequence classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers,\n",
    "                 num_classes, max_seq_len=512, dropout=0.1, pooling='cls'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pooling = pooling\n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch, seq_len]\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        encoded, _ = self.encoder(x, mask)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Pool\n",
    "        if self.pooling == 'cls':\n",
    "            # Use first token (like BERT's [CLS])\n",
    "            pooled = encoded[:, 0, :]\n",
    "        elif self.pooling == 'mean':\n",
    "            # Mean pooling\n",
    "            pooled = encoded.mean(dim=1)\n",
    "        elif self.pooling == 'max':\n",
    "            # Max pooling\n",
    "            pooled = encoded.max(dim=1)[0]\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test classifier\n",
    "print(\"Transformer Classifier\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=10000,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=512,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    pooling='mean'\n",
    ")\n",
    "\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 4\n",
    "seq_len = 50\n",
    "x = torch.randint(0, 10000, (batch_size, seq_len))\n",
    "output = model(x)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (logits): {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transformers vs RNNs\n",
    "\n",
    "### 8.1 Computational Comparison\n",
    "\n",
    "| Aspect | RNN/LSTM | Transformer |\n",
    "|--------|----------|-------------|\n",
    "| Parallelization | Sequential | Fully parallel |\n",
    "| Long-range dependencies | Difficult (vanishing gradient) | Direct connections |\n",
    "| Computational complexity | O(n) per layer | O(n²) per layer |\n",
    "| Memory | O(1) for long sequences | O(n²) attention matrix |\n",
    "| Training speed | Slow (sequential) | Fast (parallel) |\n",
    "| Interpretability | Hidden state is opaque | Attention weights interpretable |\n",
    "\n",
    "### 8.2 Path Length Comparison\n",
    "\n",
    "Maximum path length between any two positions:\n",
    "- **RNN**: O(n) - must traverse sequence\n",
    "- **Transformer**: O(1) - direct attention connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Transformer vs LSTM on sequence length\n",
    "\n",
    "import time\n",
    "\n",
    "def measure_forward_time(model, seq_lengths, vocab_size, batch_size=32, num_runs=3):\n",
    "    \"\"\"Measure forward pass time for different sequence lengths.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        \n",
    "        # Warm up\n",
    "        _ = model(x)\n",
    "        \n",
    "        # Measure\n",
    "        run_times = []\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = model(x)\n",
    "            end = time.time()\n",
    "            run_times.append(end - start)\n",
    "        \n",
    "        times.append(np.mean(run_times))\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Create models\n",
    "vocab_size = 5000\n",
    "d_model = 64\n",
    "\n",
    "# LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.lstm = nn.LSTM(d_model, d_model, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "lstm_model = LSTMClassifier(vocab_size, d_model)\n",
    "transformer_model = TransformerClassifier(\n",
    "    vocab_size, d_model, num_heads=4, d_ff=256, num_layers=2, num_classes=2\n",
    ")\n",
    "\n",
    "# Measure times\n",
    "seq_lengths = [10, 25, 50, 100, 200]\n",
    "\n",
    "print(\"Measuring forward pass times...\")\n",
    "lstm_times = measure_forward_time(lstm_model, seq_lengths, vocab_size)\n",
    "transformer_times = measure_forward_time(transformer_model, seq_lengths, vocab_size)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "ax = axes[0]\n",
    "ax.plot(seq_lengths, lstm_times, 'b-o', linewidth=2, markersize=8, label='LSTM')\n",
    "ax.plot(seq_lengths, transformer_times, 'r-s', linewidth=2, markersize=8, label='Transformer')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Forward Pass Time (seconds)')\n",
    "ax.set_title('Inference Time vs Sequence Length')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Path length illustration\n",
    "ax = axes[1]\n",
    "rnn_path = seq_lengths  # O(n)\n",
    "transformer_path = [1] * len(seq_lengths)  # O(1)\n",
    "\n",
    "ax.plot(seq_lengths, rnn_path, 'b-o', linewidth=2, markersize=8, label='RNN (O(n))')\n",
    "ax.plot(seq_lengths, transformer_path, 'r-s', linewidth=2, markersize=8, label='Transformer (O(1))')\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Maximum Path Length')\n",
    "ax.set_title('Path Length Between Distant Positions')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Transformer vs LSTM Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"1. Transformer: Constant path length for any two positions\")\n",
    "print(\"2. LSTM: Path length grows with distance\")\n",
    "print(\"3. Transformer: Better at long-range dependencies\")\n",
    "print(\"4. Transformer: More parallelizable (but O(n²) memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Why Transformers Changed Everything\n",
    "\n",
    "### 9.1 The Revolution\n",
    "\n",
    "Since \"Attention Is All You Need\" (2017):\n",
    "\n",
    "**NLP:**\n",
    "- BERT (2018): Bidirectional encoder, pretraining\n",
    "- GPT (2018-2023): Autoregressive generation\n",
    "- T5 (2020): Text-to-text framework\n",
    "- ChatGPT, Claude, etc.\n",
    "\n",
    "**Computer Vision:**\n",
    "- Vision Transformer (ViT, 2020)\n",
    "- DETR for object detection\n",
    "- SWIN Transformer\n",
    "\n",
    "**Other Domains:**\n",
    "- AlphaFold 2 (protein structure)\n",
    "- Music generation\n",
    "- Code generation\n",
    "\n",
    "### 9.2 Key Innovations\n",
    "\n",
    "1. **Scalability**: Train on massive datasets efficiently\n",
    "2. **Transfer learning**: Pre-train once, fine-tune for many tasks\n",
    "3. **Long-range dependencies**: Direct attention connections\n",
    "4. **Interpretability**: Attention weights show model focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of Transformer-based models\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "models = [\n",
    "    (2017, 'Transformer', 'Original', 65e6),\n",
    "    (2018.4, 'GPT-1', 'Decoder', 117e6),\n",
    "    (2018.8, 'BERT', 'Encoder', 340e6),\n",
    "    (2019.2, 'GPT-2', 'Decoder', 1.5e9),\n",
    "    (2019.8, 'XLNet', 'Encoder', 340e6),\n",
    "    (2020.0, 'T5', 'Enc-Dec', 11e9),\n",
    "    (2020.5, 'GPT-3', 'Decoder', 175e9),\n",
    "    (2022.0, 'ChatGPT', 'Decoder', 175e9),\n",
    "    (2023.0, 'GPT-4', 'Decoder', 1000e9),\n",
    "]\n",
    "\n",
    "years = [m[0] for m in models]\n",
    "names = [m[1] for m in models]\n",
    "types = [m[2] for m in models]\n",
    "params = [m[3] for m in models]\n",
    "\n",
    "# Color by type\n",
    "type_colors = {'Original': 'gray', 'Encoder': 'blue', 'Decoder': 'green', 'Enc-Dec': 'orange'}\n",
    "colors = [type_colors[t] for t in types]\n",
    "\n",
    "# Plot\n",
    "ax.scatter(years, np.log10(params), c=colors, s=200, alpha=0.7, zorder=5)\n",
    "\n",
    "for year, name, param in zip(years, names, params):\n",
    "    ax.annotate(f'{name}\\n({param/1e9:.1f}B)' if param >= 1e9 else f'{name}\\n({param/1e6:.0f}M)',\n",
    "               (year, np.log10(param)),\n",
    "               textcoords=\"offset points\",\n",
    "               xytext=(0, 15),\n",
    "               ha='center',\n",
    "               fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Parameters (log scale)', fontsize=12)\n",
    "ax.set_yticks([7, 8, 9, 10, 11, 12])\n",
    "ax.set_yticklabels(['10M', '100M', '1B', '10B', '100B', '1T'])\n",
    "ax.set_title('Growth of Transformer-Based Models', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "for type_name, color in type_colors.items():\n",
    "    ax.scatter([], [], c=color, s=100, label=type_name, alpha=0.7)\n",
    "ax.legend(loc='upper left', title='Architecture')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Exponential growth in model size!\")\n",
    "print(f\"From {models[0][3]/1e6:.0f}M parameters (2017) to ~1T parameters (2023)\")\n",
    "print(\"That's ~15,000x increase in 6 years!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Attention solves the bottleneck problem**\n",
    "   - Decoder can access all encoder states\n",
    "   - Learned alignments between sequences\n",
    "\n",
    "2. **Self-attention captures relationships**\n",
    "   - Query-Key-Value framework\n",
    "   - Scaled dot-product: $\\text{softmax}(QK^T/\\sqrt{d_k})V$\n",
    "   - Direct connection between any positions\n",
    "\n",
    "3. **Multi-head attention enables diverse patterns**\n",
    "   - Multiple attention heads in parallel\n",
    "   - Each head specializes in different relationships\n",
    "\n",
    "4. **Positional encoding provides order information**\n",
    "   - Sinusoidal encodings\n",
    "   - Learnable alternatives\n",
    "\n",
    "5. **Transformer architecture:**\n",
    "   - Multi-Head Attention → Add & Norm → FFN → Add & Norm\n",
    "   - Residual connections preserve gradients\n",
    "   - Fully parallelizable\n",
    "\n",
    "6. **Advantages over RNNs:**\n",
    "   - Constant path length for long-range dependencies\n",
    "   - Parallelizable training\n",
    "   - Better scalability\n",
    "\n",
    "### Next Steps (Days 16-20)\n",
    "\n",
    "- **Day 16**: Sequence-to-Sequence models with attention\n",
    "- **Day 17**: BERT and pretraining\n",
    "- **Day 18**: GPT and autoregressive models\n",
    "- **Day 19**: Fine-tuning pretrained models\n",
    "- **Day 20**: Project 2 - NLP Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Masked Self-Attention\n",
    "Implement causal (masked) self-attention for autoregressive generation.\n",
    "\n",
    "### Exercise 2: Transformer Decoder\n",
    "Build the complete Transformer decoder with masked self-attention and cross-attention.\n",
    "\n",
    "### Exercise 3: Learnable Positional Encodings\n",
    "Implement learnable positional embeddings and compare with sinusoidal.\n",
    "\n",
    "### Exercise 4: Relative Position Attention\n",
    "Implement relative positional encodings (Shaw et al., 2018).\n",
    "\n",
    "### Exercise 5: Attention Visualization Tool\n",
    "Build an interactive tool to visualize attention patterns for different inputs.\n",
    "\n",
    "### Exercise 6: Sparse Attention\n",
    "Implement local/sparse attention patterns (like Longformer) for long sequences.\n",
    "\n",
    "### Exercise 7: Vision Transformer\n",
    "Apply the Transformer architecture to image classification (ViT-style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for Exercise 1: Masked Self-Attention\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create causal mask for autoregressive attention.\n",
    "    \n",
    "    Position i can only attend to positions j <= i\n",
    "    \"\"\"\n",
    "    # Upper triangular matrix with -inf (will become 0 after softmax)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    return ~mask  # True = attend, False = mask\n",
    "\n",
    "# Visualize causal mask\n",
    "seq_len = 6\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(mask.float().numpy(), cmap='Blues')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Causal Mask\\n(White = Masked, Blue = Attend)')\n",
    "plt.xticks(range(seq_len))\n",
    "plt.yticks(range(seq_len))\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        text = '✓' if mask[i, j] else '✗'\n",
    "        plt.text(j, i, text, ha='center', va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Causal mask ensures position i only attends to positions <= i\")\n",
    "print(\"This enables autoregressive (left-to-right) generation.\")\n",
    "print(\"\\nExercise: Integrate this mask into MultiHeadAttention for decoder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Vaswani, A., et al. (2017). \"Attention Is All You Need.\" NeurIPS.\n",
    "2. Bahdanau, D., et al. (2015). \"Neural Machine Translation by Jointly Learning to Align and Translate.\" ICLR.\n",
    "3. Luong, M. T., et al. (2015). \"Effective Approaches to Attention-based Neural Machine Translation.\" EMNLP.\n",
    "4. Devlin, J., et al. (2019). \"BERT: Pre-training of Deep Bidirectional Transformers.\" NAACL.\n",
    "5. Radford, A., et al. (2018-2023). \"GPT Series.\" OpenAI.\n",
    "6. Shaw, P., et al. (2018). \"Self-Attention with Relative Position Representations.\" NAACL.\n",
    "7. Dosovitskiy, A., et al. (2020). \"An Image is Worth 16x16 Words: Transformers for Image Recognition.\" ICLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
