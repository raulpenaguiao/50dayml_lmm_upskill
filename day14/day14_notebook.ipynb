{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 14: LSTM and GRU Networks\n",
    "\n",
    "## Phase 2: NLP Basics (Days 11-20)\n",
    "\n",
    "**Estimated Time: 3-4 hours**\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand gating mechanisms and their role in solving vanishing gradients\n",
    "- Implement LSTM (Long Short-Term Memory) from scratch\n",
    "- Master the LSTM cell equations and information flow\n",
    "- Implement GRU (Gated Recurrent Unit) architecture\n",
    "- Compare LSTM, GRU, and vanilla RNN performance\n",
    "- Build sequence models for real-world tasks\n",
    "- Apply advanced RNN techniques: stacking, dropout, attention\n",
    "\n",
    "### Prerequisites\n",
    "- Day 13: Recurrent Neural Networks\n",
    "- Understanding of vanishing gradient problem\n",
    "- Neural network fundamentals\n",
    "- Linear algebra and calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Gating Mechanism\n",
    "\n",
    "### 1.1 Motivation: Why Gates?\n",
    "\n",
    "Recall from Day 13:\n",
    "- Vanilla RNNs suffer from vanishing/exploding gradients\n",
    "- Cannot learn long-range dependencies\n",
    "- Information decays exponentially through time\n",
    "\n",
    "**Key Insight**: What if we could *control* information flow?\n",
    "- Selectively **remember** important information\n",
    "- Selectively **forget** irrelevant information\n",
    "- Allow gradients to flow unimpeded through time\n",
    "\n",
    "### 1.2 Gates as Learned Switches\n",
    "\n",
    "A **gate** is a sigmoid-activated vector that controls information flow:\n",
    "\n",
    "$$g = \\sigma(W_g \\cdot [h_{t-1}, x_t] + b_g) \\in (0, 1)^d$$\n",
    "\n",
    "- Values near 0: Block information\n",
    "- Values near 1: Pass information through\n",
    "- Element-wise multiplication controls each dimension independently\n",
    "\n",
    "**Analogy**: Like a water pipe with adjustable valves at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gating mechanism\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Sigmoid function (gate activation)\n",
    "ax = axes[0, 0]\n",
    "x = np.linspace(-6, 6, 100)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "ax.plot(x, sigmoid, 'b-', linewidth=2)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.fill_between(x, 0, sigmoid, alpha=0.3)\n",
    "ax.set_xlabel('Input')\n",
    "ax.set_ylabel('Gate Value')\n",
    "ax.set_title('Sigmoid Gate Function\\n$g = \\\\sigma(z)$', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(-4, 0.8, 'BLOCK', fontsize=12, ha='center')\n",
    "ax.text(4, 0.8, 'PASS', fontsize=12, ha='center')\n",
    "\n",
    "# 2. Element-wise gating\n",
    "ax = axes[0, 1]\n",
    "information = np.array([0.8, -0.5, 0.3, 0.9, -0.2])\n",
    "gate_values = np.array([0.9, 0.1, 0.8, 0.2, 0.7])\n",
    "gated_info = information * gate_values\n",
    "\n",
    "x_pos = np.arange(len(information))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x_pos - width, information, width, label='Information', color='blue', alpha=0.7)\n",
    "ax.bar(x_pos, gate_values, width, label='Gate', color='orange', alpha=0.7)\n",
    "ax.bar(x_pos + width, gated_info, width, label='Gated Output', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'Dim {i+1}' for i in range(len(information))])\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Element-wise Gating\\n$output = information \\\\odot gate$', fontsize=12)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Gate controlling information flow\n",
    "ax = axes[1, 0]\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "# Draw information flow diagram\n",
    "# Input\n",
    "rect = plt.Rectangle((1, 4), 2, 2, color='lightblue', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(2, 5, 'Info\\n$x$', ha='center', va='center', fontsize=11)\n",
    "\n",
    "# Gate\n",
    "circle = plt.Circle((5, 5), 0.8, color='orange', alpha=0.8)\n",
    "ax.add_patch(circle)\n",
    "ax.text(5, 5, '$g$', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax.text(5, 3.5, 'Gate\\n$\\\\sigma(Wx+b)$', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Output\n",
    "rect = plt.Rectangle((7, 4), 2, 2, color='lightgreen', alpha=0.8)\n",
    "ax.add_patch(rect)\n",
    "ax.text(8, 5, 'Output\\n$g \\\\odot x$', ha='center', va='center', fontsize=11)\n",
    "\n",
    "# Arrows\n",
    "ax.annotate('', xy=(4.2, 5), xytext=(3, 5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "ax.annotate('', xy=(7, 5), xytext=(5.8, 5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=2))\n",
    "\n",
    "ax.set_title('Gate as Information Controller', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "# 4. Why gates help with gradients\n",
    "ax = axes[1, 1]\n",
    "time_steps = 50\n",
    "\n",
    "# Vanilla RNN gradient decay (worst case)\n",
    "vanilla_grad = 0.9 ** np.arange(time_steps)\n",
    "\n",
    "# Gated RNN (additive interaction allows gradient to flow)\n",
    "gated_grad = np.ones(time_steps)\n",
    "for t in range(1, time_steps):\n",
    "    # Gate value (learned to be close to 1 for important info)\n",
    "    forget_gate = 0.95 + 0.05 * np.random.rand()\n",
    "    gated_grad[t] = gated_grad[t-1] * forget_gate\n",
    "\n",
    "ax.plot(vanilla_grad, 'r-', linewidth=2, label='Vanilla RNN')\n",
    "ax.plot(gated_grad, 'g-', linewidth=2, label='Gated RNN (LSTM)')\n",
    "ax.set_xlabel('Time Steps Back')\n",
    "ax.set_ylabel('Gradient Magnitude (relative)')\n",
    "ax.set_title('Gradient Flow: Vanilla vs Gated', fontsize=12)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.suptitle('The Gating Mechanism', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM: Long Short-Term Memory\n",
    "\n",
    "### 2.1 Architecture Overview\n",
    "\n",
    "LSTM (Hochreiter & Schmidhuber, 1997) introduces:\n",
    "- **Cell state** $C_t$: Long-term memory (the \"conveyor belt\")\n",
    "- **Hidden state** $h_t$: Short-term memory (output)\n",
    "- **Three gates**: Forget, Input, Output\n",
    "\n",
    "### 2.2 LSTM Equations\n",
    "\n",
    "**Forget Gate**: What to forget from cell state\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**Input Gate**: What new information to store\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "**Candidate Cell State**: New information to potentially add\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "**Cell State Update**: Combine old and new\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "**Output Gate**: What to output from cell state\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "**Hidden State**: Filtered cell state\n",
    "$$h_t = o_t \\odot \\tanh(C_t)$$\n",
    "\n",
    "### 2.3 Key Insight: Additive Updates\n",
    "\n",
    "The cell state update:\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "This is **additive** (like ResNet skip connections), not multiplicative!\n",
    "- Gradients flow through the addition\n",
    "- When $f_t \\approx 1$, gradient flows unimpeded\n",
    "- Solves vanishing gradient problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    \"\"\"\n",
    "    LSTM Cell implementation from scratch.\n",
    "    \n",
    "    Implements the full LSTM equations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Concatenated input size\n",
    "        concat_dim = input_dim + hidden_dim\n",
    "        \n",
    "        # Initialize weights (Xavier)\n",
    "        scale = np.sqrt(2.0 / (concat_dim + hidden_dim))\n",
    "        \n",
    "        # Forget gate\n",
    "        self.W_f = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_f = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Input gate\n",
    "        self.W_i = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_i = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Candidate cell state\n",
    "        self.W_c = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_c = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Output gate\n",
    "        self.W_o = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_o = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Initialize forget gate bias to 1 (important!)\n",
    "        # This encourages remembering at the start\n",
    "        self.b_f = np.ones((hidden_dim, 1))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, x_t, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM cell.\n",
    "        \n",
    "        x_t: [input_dim, 1]\n",
    "        h_prev: [hidden_dim, 1]\n",
    "        c_prev: [hidden_dim, 1]\n",
    "        \n",
    "        Returns: h_t, c_t, cache\n",
    "        \"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat = np.vstack([h_prev, x_t])  # [hidden_dim + input_dim, 1]\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = self.sigmoid(self.W_f @ concat + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = self.sigmoid(self.W_i @ concat + self.b_i)\n",
    "        \n",
    "        # Candidate cell state\n",
    "        c_tilde = np.tanh(self.W_c @ concat + self.b_c)\n",
    "        \n",
    "        # New cell state\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = self.sigmoid(self.W_o @ concat + self.b_o)\n",
    "        \n",
    "        # New hidden state\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "        \n",
    "        # Cache for backprop\n",
    "        cache = {\n",
    "            'x_t': x_t,\n",
    "            'h_prev': h_prev,\n",
    "            'c_prev': c_prev,\n",
    "            'concat': concat,\n",
    "            'f_t': f_t,\n",
    "            'i_t': i_t,\n",
    "            'c_tilde': c_tilde,\n",
    "            'c_t': c_t,\n",
    "            'o_t': o_t,\n",
    "            'h_t': h_t\n",
    "        }\n",
    "        \n",
    "        return h_t, c_t, cache\n",
    "\n",
    "# Test LSTM cell\n",
    "print(\"LSTM Cell Implementation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "input_dim = 10\n",
    "hidden_dim = 20\n",
    "\n",
    "lstm_cell = LSTMCell(input_dim, hidden_dim)\n",
    "\n",
    "# Initialize states\n",
    "h_prev = np.zeros((hidden_dim, 1))\n",
    "c_prev = np.zeros((hidden_dim, 1))\n",
    "x_t = np.random.randn(input_dim, 1)\n",
    "\n",
    "# Forward pass\n",
    "h_t, c_t, cache = lstm_cell.forward(x_t, h_prev, c_prev)\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"\\nInput x_t shape: {x_t.shape}\")\n",
    "print(f\"Previous h shape: {h_prev.shape}\")\n",
    "print(f\"Previous c shape: {c_prev.shape}\")\n",
    "print(f\"\\nNew h_t shape: {h_t.shape}\")\n",
    "print(f\"New c_t shape: {c_t.shape}\")\n",
    "\n",
    "print(f\"\\nGate values (mean):\")\n",
    "print(f\"  Forget gate: {cache['f_t'].mean():.4f}\")\n",
    "print(f\"  Input gate: {cache['i_t'].mean():.4f}\")\n",
    "print(f\"  Output gate: {cache['o_t'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM cell architecture\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Main cell outline\n",
    "cell_rect = plt.Rectangle((2, 2), 10, 8, fill=False, edgecolor='black', linewidth=2)\n",
    "ax.add_patch(cell_rect)\n",
    "ax.text(7, 10.5, 'LSTM Cell', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Cell state line (conveyor belt)\n",
    "ax.plot([0, 14], [8.5, 8.5], 'b-', linewidth=3)\n",
    "ax.text(0.5, 9, '$C_{t-1}$', fontsize=12, fontweight='bold')\n",
    "ax.text(13.5, 9, '$C_t$', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Hidden state input/output\n",
    "ax.plot([0, 2], [4, 4], 'g-', linewidth=2)\n",
    "ax.plot([12, 14], [4, 4], 'g-', linewidth=2)\n",
    "ax.text(0.5, 4.5, '$h_{t-1}$', fontsize=12, fontweight='bold')\n",
    "ax.text(13, 4.5, '$h_t$', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Input\n",
    "ax.plot([7, 7], [0, 2], 'orange', linewidth=2)\n",
    "ax.text(7, 0.5, '$x_t$', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Forget gate (×)\n",
    "forget_gate = plt.Circle((4, 8.5), 0.5, color='red', alpha=0.8)\n",
    "ax.add_patch(forget_gate)\n",
    "ax.text(4, 8.5, '×', ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "ax.text(4, 7.3, '$f_t$', ha='center', fontsize=11)\n",
    "ax.text(4, 6.8, 'Forget', ha='center', fontsize=10)\n",
    "\n",
    "# Input gate (×)\n",
    "input_gate = plt.Circle((7, 8.5), 0.5, color='green', alpha=0.8)\n",
    "ax.add_patch(input_gate)\n",
    "ax.text(7, 8.5, '×', ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "ax.text(7, 7.3, '$i_t$', ha='center', fontsize=11)\n",
    "ax.text(7, 6.8, 'Input', ha='center', fontsize=10)\n",
    "\n",
    "# Addition\n",
    "add_circle = plt.Circle((5.5, 8.5), 0.4, color='purple', alpha=0.8)\n",
    "ax.add_patch(add_circle)\n",
    "ax.text(5.5, 8.5, '+', ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "\n",
    "# Output gate (×)\n",
    "output_gate = plt.Circle((10, 4), 0.5, color='blue', alpha=0.8)\n",
    "ax.add_patch(output_gate)\n",
    "ax.text(10, 4, '×', ha='center', va='center', fontsize=16, color='white', fontweight='bold')\n",
    "ax.text(10, 2.8, '$o_t$', ha='center', fontsize=11)\n",
    "ax.text(10, 2.3, 'Output', ha='center', fontsize=10)\n",
    "\n",
    "# Tanh for candidate\n",
    "tanh1 = plt.Rectangle((6.3, 5.5), 1.4, 0.8, color='yellow', alpha=0.8)\n",
    "ax.add_patch(tanh1)\n",
    "ax.text(7, 5.9, 'tanh', ha='center', va='center', fontsize=10)\n",
    "ax.text(7, 4.8, '$\\\\tilde{C}_t$', ha='center', fontsize=11)\n",
    "\n",
    "# Tanh for output\n",
    "tanh2 = plt.Rectangle((9.3, 6.5), 1.4, 0.8, color='yellow', alpha=0.8)\n",
    "ax.add_patch(tanh2)\n",
    "ax.text(10, 6.9, 'tanh', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Sigma boxes for gates\n",
    "for x_pos, name in [(4, 'σ'), (7, 'σ'), (10, 'σ')]:\n",
    "    sigma_box = plt.Rectangle((x_pos-0.4, 3.6), 0.8, 0.8, color='lightgray', alpha=0.8)\n",
    "    ax.add_patch(sigma_box)\n",
    "    ax.text(x_pos, 4, name, ha='center', va='center', fontsize=12)\n",
    "\n",
    "# Connections (simplified)\n",
    "# From concat to gates\n",
    "ax.plot([3, 4], [3, 3.6], 'k-', linewidth=1)\n",
    "ax.plot([3, 7], [3, 3.6], 'k-', linewidth=1)\n",
    "ax.plot([3, 7], [3, 5.5], 'k-', linewidth=1)\n",
    "ax.plot([3, 10], [3, 3.6], 'k-', linewidth=1)\n",
    "\n",
    "# Gates to operations\n",
    "ax.plot([4, 4], [4.4, 8], 'k-', linewidth=1)\n",
    "ax.plot([7, 7], [4.4, 8], 'k-', linewidth=1)\n",
    "ax.plot([7, 7], [6.3, 8], 'k-', linewidth=1)\n",
    "ax.plot([10, 10], [4.5, 6.5], 'k-', linewidth=1)\n",
    "ax.plot([10, 12], [4.5, 4], 'k-', linewidth=1)\n",
    "\n",
    "# Cell state connections\n",
    "ax.plot([10, 10], [7.3, 8.5], 'b-', linewidth=2)\n",
    "\n",
    "# Equations on the side\n",
    "equations = [\n",
    "    r'$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$',\n",
    "    r'$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$',\n",
    "    r'$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$',\n",
    "    r'$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$',\n",
    "    r'$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$',\n",
    "    r'$h_t = o_t \\odot \\tanh(C_t)$'\n",
    "]\n",
    "\n",
    "for i, eq in enumerate(equations):\n",
    "    ax.text(15, 9 - i*1.2, eq, fontsize=11, va='center')\n",
    "\n",
    "ax.set_xlim(-1, 25)\n",
    "ax.set_ylim(-0.5, 11)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complete LSTM Network\n",
    "\n",
    "### 3.1 Stacking LSTM Cells Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork:\n",
    "    \"\"\"\n",
    "    Complete LSTM network for sequence processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.lstm_cell = LSTMCell(input_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        scale = np.sqrt(2.0 / (hidden_dim + output_dim))\n",
    "        self.W_y = np.random.randn(output_dim, hidden_dim) * scale\n",
    "        self.b_y = np.zeros((output_dim, 1))\n",
    "        \n",
    "        # Cache for backprop\n",
    "        self.caches = []\n",
    "    \n",
    "    def forward(self, inputs, h_prev=None, c_prev=None):\n",
    "        \"\"\"\n",
    "        Forward pass through entire sequence.\n",
    "        \n",
    "        inputs: list of input vectors\n",
    "        \"\"\"\n",
    "        if h_prev is None:\n",
    "            h_prev = np.zeros((self.hidden_dim, 1))\n",
    "        if c_prev is None:\n",
    "            c_prev = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        self.caches = []\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        \n",
    "        h_t, c_t = h_prev, c_prev\n",
    "        \n",
    "        for t, x_t in enumerate(inputs):\n",
    "            h_t, c_t, cache = self.lstm_cell.forward(x_t, h_t, c_t)\n",
    "            \n",
    "            # Output\n",
    "            y_t = self.W_y @ h_t + self.b_y\n",
    "            \n",
    "            self.caches.append(cache)\n",
    "            outputs.append(y_t)\n",
    "            hidden_states.append(h_t)\n",
    "            cell_states.append(c_t)\n",
    "        \n",
    "        return outputs, hidden_states, cell_states\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def compute_loss(self, outputs, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        loss = 0\n",
    "        self.probs = []\n",
    "        \n",
    "        for y_t, target in zip(outputs, targets):\n",
    "            probs = self.softmax(y_t)\n",
    "            self.probs.append(probs)\n",
    "            loss += -np.log(probs[target, 0] + 1e-10)\n",
    "        \n",
    "        return loss / len(outputs)\n",
    "\n",
    "# Test LSTM network\n",
    "print(\"LSTM Network Test\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "input_dim = 10\n",
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    "seq_length = 20\n",
    "\n",
    "lstm_net = LSTMNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Create sequence\n",
    "inputs = [np.random.randn(input_dim, 1) for _ in range(seq_length)]\n",
    "targets = [np.random.randint(0, output_dim) for _ in range(seq_length)]\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states, cell_states = lstm_net.forward(inputs)\n",
    "loss = lstm_net.compute_loss(outputs, targets)\n",
    "\n",
    "print(f\"Sequence length: {seq_length}\")\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Number of outputs: {len(outputs)}\")\n",
    "print(f\"Output shape: {outputs[0].shape}\")\n",
    "\n",
    "# Analyze gate behavior\n",
    "forget_gates = [cache['f_t'].mean() for cache in lstm_net.caches]\n",
    "input_gates = [cache['i_t'].mean() for cache in lstm_net.caches]\n",
    "output_gates = [cache['o_t'].mean() for cache in lstm_net.caches]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(forget_gates, 'r-', label='Forget Gate', linewidth=2)\n",
    "plt.plot(input_gates, 'g-', label='Input Gate', linewidth=2)\n",
    "plt.plot(output_gates, 'b-', label='Output Gate', linewidth=2)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Mean Gate Value')\n",
    "plt.title('LSTM Gate Activations Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRU: Gated Recurrent Unit\n",
    "\n",
    "### 4.1 Simplified Gating\n",
    "\n",
    "GRU (Cho et al., 2014) simplifies LSTM:\n",
    "- Combines forget and input gates into **update gate**\n",
    "- Merges cell state and hidden state\n",
    "- Fewer parameters, similar performance\n",
    "\n",
    "### 4.2 GRU Equations\n",
    "\n",
    "**Reset Gate**: How much of past to forget\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$$\n",
    "\n",
    "**Update Gate**: How much to update state\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$$\n",
    "\n",
    "**Candidate Hidden State**: New state proposal\n",
    "$$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t] + b_h)$$\n",
    "\n",
    "**Final Hidden State**: Interpolation between old and new\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "**Key Insight**: The update gate creates a direct connection to past:\n",
    "- When $z_t \\approx 0$: $h_t \\approx h_{t-1}$ (copy forward)\n",
    "- When $z_t \\approx 1$: $h_t \\approx \\tilde{h}_t$ (use new info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell:\n",
    "    \"\"\"\n",
    "    GRU Cell implementation from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        concat_dim = input_dim + hidden_dim\n",
    "        scale = np.sqrt(2.0 / (concat_dim + hidden_dim))\n",
    "        \n",
    "        # Reset gate\n",
    "        self.W_r = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_r = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Update gate\n",
    "        self.W_z = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_z = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        self.W_h = np.random.randn(hidden_dim, concat_dim) * scale\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, x_t, h_prev):\n",
    "        \"\"\"\n",
    "        Forward pass through GRU cell.\n",
    "        \"\"\"\n",
    "        concat = np.vstack([h_prev, x_t])\n",
    "        \n",
    "        # Reset gate\n",
    "        r_t = self.sigmoid(self.W_r @ concat + self.b_r)\n",
    "        \n",
    "        # Update gate\n",
    "        z_t = self.sigmoid(self.W_z @ concat + self.b_z)\n",
    "        \n",
    "        # Candidate hidden state\n",
    "        reset_concat = np.vstack([r_t * h_prev, x_t])\n",
    "        h_tilde = np.tanh(self.W_h @ reset_concat + self.b_h)\n",
    "        \n",
    "        # Final hidden state\n",
    "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
    "        \n",
    "        cache = {\n",
    "            'x_t': x_t,\n",
    "            'h_prev': h_prev,\n",
    "            'r_t': r_t,\n",
    "            'z_t': z_t,\n",
    "            'h_tilde': h_tilde,\n",
    "            'h_t': h_t\n",
    "        }\n",
    "        \n",
    "        return h_t, cache\n",
    "\n",
    "# Test GRU cell\n",
    "print(\"GRU Cell Implementation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "gru_cell = GRUCell(input_dim, hidden_dim)\n",
    "\n",
    "h_prev = np.zeros((hidden_dim, 1))\n",
    "x_t = np.random.randn(input_dim, 1)\n",
    "\n",
    "h_t, cache = gru_cell.forward(x_t, h_prev)\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"\\nGate values (mean):\")\n",
    "print(f\"  Reset gate: {cache['r_t'].mean():.4f}\")\n",
    "print(f\"  Update gate: {cache['z_t'].mean():.4f}\")\n",
    "\n",
    "# Compare parameter counts\n",
    "lstm_params = 4 * hidden_dim * (input_dim + hidden_dim) + 4 * hidden_dim\n",
    "gru_params = 3 * hidden_dim * (input_dim + hidden_dim) + 3 * hidden_dim\n",
    "\n",
    "print(f\"\\nParameter comparison:\")\n",
    "print(f\"  LSTM parameters: {lstm_params:,}\")\n",
    "print(f\"  GRU parameters: {gru_params:,}\")\n",
    "print(f\"  GRU reduction: {(1 - gru_params/lstm_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GRU architecture\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Main cell\n",
    "cell_rect = plt.Rectangle((2, 2), 8, 6, fill=False, edgecolor='black', linewidth=2)\n",
    "ax.add_patch(cell_rect)\n",
    "ax.text(6, 8.5, 'GRU Cell', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Hidden state line\n",
    "ax.plot([0, 12], [6, 6], 'g-', linewidth=3)\n",
    "ax.text(0.5, 6.5, '$h_{t-1}$', fontsize=12, fontweight='bold')\n",
    "ax.text(11.5, 6.5, '$h_t$', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Input\n",
    "ax.plot([6, 6], [0, 2], 'orange', linewidth=2)\n",
    "ax.text(6, 0.5, '$x_t$', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Update gate multiply\n",
    "update_mult1 = plt.Circle((4, 6), 0.4, color='purple', alpha=0.8)\n",
    "ax.add_patch(update_mult1)\n",
    "ax.text(4, 6, '×', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "ax.text(4, 4.8, '$1-z_t$', ha='center', fontsize=10)\n",
    "\n",
    "# Update gate multiply 2\n",
    "update_mult2 = plt.Circle((8, 6), 0.4, color='green', alpha=0.8)\n",
    "ax.add_patch(update_mult2)\n",
    "ax.text(8, 6, '×', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "ax.text(8, 4.8, '$z_t$', ha='center', fontsize=10)\n",
    "\n",
    "# Addition\n",
    "add_circle = plt.Circle((10, 6), 0.3, color='blue', alpha=0.8)\n",
    "ax.add_patch(add_circle)\n",
    "ax.text(10, 6, '+', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "\n",
    "# Candidate hidden\n",
    "tanh_box = plt.Rectangle((7.3, 3.5), 1.4, 0.8, color='yellow', alpha=0.8)\n",
    "ax.add_patch(tanh_box)\n",
    "ax.text(8, 3.9, 'tanh', ha='center', va='center', fontsize=10)\n",
    "ax.text(8, 2.8, '$\\\\tilde{h}_t$', ha='center', fontsize=11)\n",
    "\n",
    "# Reset gate effect\n",
    "reset_mult = plt.Circle((3, 4), 0.4, color='red', alpha=0.8)\n",
    "ax.add_patch(reset_mult)\n",
    "ax.text(3, 4, '×', ha='center', va='center', fontsize=14, color='white', fontweight='bold')\n",
    "ax.text(3, 2.8, '$r_t$', ha='center', fontsize=11)\n",
    "ax.text(3, 2.3, 'Reset', ha='center', fontsize=10)\n",
    "\n",
    "# Sigma boxes\n",
    "sigma1 = plt.Rectangle((2.6, 5), 0.8, 0.6, color='lightgray', alpha=0.8)\n",
    "ax.add_patch(sigma1)\n",
    "ax.text(3, 5.3, 'σ', ha='center', va='center', fontsize=10)\n",
    "\n",
    "sigma2 = plt.Rectangle((7.6, 5), 0.8, 0.6, color='lightgray', alpha=0.8)\n",
    "ax.add_patch(sigma2)\n",
    "ax.text(8, 5.3, 'σ', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Equations\n",
    "equations = [\n",
    "    r'$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$',\n",
    "    r'$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$',\n",
    "    r'$\\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\odot h_{t-1}, x_t])$',\n",
    "    r'$h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$'\n",
    "]\n",
    "\n",
    "for i, eq in enumerate(equations):\n",
    "    ax.text(13, 7 - i*1.2, eq, fontsize=11, va='center')\n",
    "\n",
    "ax.set_xlim(-1, 22)\n",
    "ax.set_ylim(-0.5, 9)\n",
    "ax.axis('off')\n",
    "ax.set_title('GRU: Simplified Gating (vs LSTM)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch LSTM and GRU\n",
    "\n",
    "### 5.1 Built-in Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch LSTM example\n",
    "\n",
    "class SequenceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence classifier using LSTM/GRU.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                 num_layers=1, rnn_type='LSTM', bidirectional=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(\n",
    "                embedding_dim, hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout if num_layers > 1 else 0\n",
    "            )\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(\n",
    "                embedding_dim, hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout if num_layers > 1 else 0\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.RNN(\n",
    "                embedding_dim, hidden_dim,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout if num_layers > 1 else 0\n",
    "            )\n",
    "        \n",
    "        # Output\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        embedded = self.embedding(x)  # [batch, seq, embed]\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # RNN\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.rnn(embedded)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        # Use last hidden state(s)\n",
    "        if self.bidirectional:\n",
    "            # Concatenate forward and backward\n",
    "            hidden_fwd = hidden[-2]  # Last layer, forward\n",
    "            hidden_bwd = hidden[-1]  # Last layer, backward\n",
    "            hidden_final = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n",
    "        else:\n",
    "            hidden_final = hidden[-1]  # Last layer\n",
    "        \n",
    "        hidden_final = self.dropout(hidden_final)\n",
    "        logits = self.fc(hidden_final)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Compare architectures\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n",
    "\n",
    "models = {\n",
    "    'RNN': SequenceClassifier(vocab_size, embedding_dim, hidden_dim, num_classes, rnn_type='RNN'),\n",
    "    'LSTM': SequenceClassifier(vocab_size, embedding_dim, hidden_dim, num_classes, rnn_type='LSTM'),\n",
    "    'GRU': SequenceClassifier(vocab_size, embedding_dim, hidden_dim, num_classes, rnn_type='GRU'),\n",
    "    'BiLSTM': SequenceClassifier(vocab_size, embedding_dim, hidden_dim, num_classes, \n",
    "                                  rnn_type='LSTM', bidirectional=True),\n",
    "}\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"{name:10s}: {params:,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 4\n",
    "seq_len = 50\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "for name, model in models.items():\n",
    "    output = model(x)\n",
    "    print(f\"{name:10s} output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Long-Range Dependency Test\n",
    "\n",
    "### 6.1 Copying Task\n",
    "\n",
    "Test: Can the model remember information over long sequences?\n",
    "- Input: First character, then noise, then predict first character\n",
    "- Vanilla RNN fails for long sequences\n",
    "- LSTM/GRU should succeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-range dependency task: Copy first element to last position\n",
    "\n",
    "def create_copy_task_data(num_samples, seq_length, num_classes=10):\n",
    "    \"\"\"\n",
    "    Create data for copy task.\n",
    "    Input: [first_char, noise, noise, ..., noise]\n",
    "    Target: first_char (predict at last position)\n",
    "    \"\"\"\n",
    "    # First character to remember\n",
    "    first_chars = torch.randint(0, num_classes, (num_samples,))\n",
    "    \n",
    "    # Fill with noise (use special tokens > num_classes)\n",
    "    inputs = torch.randint(num_classes, num_classes + 5, (num_samples, seq_length))\n",
    "    \n",
    "    # Set first position to the character to remember\n",
    "    inputs[:, 0] = first_chars\n",
    "    \n",
    "    # Target is the first character\n",
    "    targets = first_chars\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "def train_copy_task(model_class, seq_length, num_epochs=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train model on copy task and return final accuracy.\n",
    "    \"\"\"\n",
    "    num_classes = 10\n",
    "    vocab_size = num_classes + 5  # Extra for noise tokens\n",
    "    \n",
    "    model = model_class(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=32,\n",
    "        hidden_dim=64,\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training data\n",
    "    train_inputs, train_targets = create_copy_task_data(500, seq_length, num_classes)\n",
    "    train_inputs = train_inputs.to(device)\n",
    "    train_targets = train_targets.to(device)\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(train_inputs)\n",
    "        loss = criterion(output, train_targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    test_inputs, test_targets = create_copy_task_data(200, seq_length, num_classes)\n",
    "    test_inputs = test_inputs.to(device)\n",
    "    test_targets = test_targets.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(test_inputs)\n",
    "        predictions = output.argmax(dim=1)\n",
    "        accuracy = (predictions == test_targets).float().mean().item()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Test different sequence lengths\n",
    "print(\"Long-Range Dependency Test: Copying Task\")\n",
    "print(\"=\"*60)\n",
    "print(\"Task: Remember first element and predict it at the end\")\n",
    "print()\n",
    "\n",
    "seq_lengths = [10, 25, 50, 100]\n",
    "results = {}\n",
    "\n",
    "for rnn_type in ['RNN', 'GRU', 'LSTM']:\n",
    "    results[rnn_type] = []\n",
    "    print(f\"Testing {rnn_type}...\")\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Create model factory\n",
    "        def create_model(vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "            return SequenceClassifier(\n",
    "                vocab_size, embedding_dim, hidden_dim, num_classes,\n",
    "                rnn_type=rnn_type\n",
    "            )\n",
    "        \n",
    "        acc = train_copy_task(create_model, seq_len, num_epochs=150)\n",
    "        results[rnn_type].append(acc)\n",
    "        print(f\"  Seq length {seq_len}: {acc:.1%}\")\n",
    "    print()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for rnn_type, accs in results.items():\n",
    "    plt.plot(seq_lengths, accs, 'o-', linewidth=2, markersize=10, label=rnn_type)\n",
    "\n",
    "plt.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random guess')\n",
    "plt.xlabel('Sequence Length (Distance to Remember)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Long-Range Dependency Test: Copy First Element', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "for rnn_type, accs in results.items():\n",
    "    for i, (sl, acc) in enumerate(zip(seq_lengths, accs)):\n",
    "        plt.annotate(f'{acc:.0%}', (sl, acc), textcoords=\"offset points\",\n",
    "                    xytext=(0, 10), ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"- LSTM and GRU maintain high accuracy for long sequences\")\n",
    "print(\"- Vanilla RNN struggles as sequence length increases\")\n",
    "print(\"- This demonstrates the power of gating mechanisms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sentiment Analysis with LSTM\n",
    "\n",
    "### 7.1 Real-World Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment analysis example\n",
    "\n",
    "# Create synthetic sentiment dataset\n",
    "positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'best', 'happy', 'perfect']\n",
    "negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'poor', 'disappointing', 'sad', 'wrong']\n",
    "neutral_words = ['the', 'a', 'is', 'was', 'it', 'this', 'that', 'movie', 'film', 'story']\n",
    "\n",
    "all_words = positive_words + negative_words + neutral_words\n",
    "word2idx = {word: i for i, word in enumerate(all_words)}\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "def generate_sentiment_data(num_samples=500):\n",
    "    \"\"\"Generate synthetic sentiment data.\"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Randomly choose sentiment\n",
    "        if np.random.rand() > 0.5:\n",
    "            # Positive\n",
    "            sentiment_words = np.random.choice(positive_words, size=np.random.randint(2, 4))\n",
    "            label = 1\n",
    "        else:\n",
    "            # Negative\n",
    "            sentiment_words = np.random.choice(negative_words, size=np.random.randint(2, 4))\n",
    "            label = 0\n",
    "        \n",
    "        # Add neutral words\n",
    "        neutral = np.random.choice(neutral_words, size=np.random.randint(3, 7))\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        sentence = list(neutral) + list(sentiment_words)\n",
    "        np.random.shuffle(sentence)\n",
    "        \n",
    "        # Convert to indices\n",
    "        indices = [word2idx[w] for w in sentence]\n",
    "        \n",
    "        sentences.append(indices)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "def pad_sequences(sequences, max_len=None):\n",
    "    \"\"\"Pad sequences to same length.\"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded = np.zeros((len(sequences), max_len), dtype=np.int64)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    \n",
    "    return padded\n",
    "\n",
    "# Generate data\n",
    "train_sentences, train_labels = generate_sentiment_data(800)\n",
    "test_sentences, test_labels = generate_sentiment_data(200)\n",
    "\n",
    "# Pad\n",
    "max_len = 15\n",
    "X_train = torch.tensor(pad_sequences(train_sentences, max_len))\n",
    "y_train = torch.tensor(train_labels)\n",
    "X_test = torch.tensor(pad_sequences(test_sentences, max_len))\n",
    "y_test = torch.tensor(test_labels)\n",
    "\n",
    "print(\"Sentiment Analysis Dataset\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Example sentences\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "print(\"\\nExample sentences:\")\n",
    "for i in range(3):\n",
    "    words = [idx2word.get(idx, 'PAD') for idx in train_sentences[i]]\n",
    "    label = 'Positive' if train_labels[i] == 1 else 'Negative'\n",
    "    print(f\"  {' '.join(words)} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sentiment classifier\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        # Concatenate final forward and backward hidden states\n",
    "        hidden_cat = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        return self.fc(self.dropout(hidden_cat))\n",
    "\n",
    "# Create model\n",
    "model = SentimentLSTM(vocab_size, embedding_dim=32, hidden_dim=64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training Sentiment LSTM...\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train.to(device), y_train.to(device))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        predictions = output.argmax(dim=1)\n",
    "        correct += (predictions == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "    train_accs.append(correct / total)\n",
    "    \n",
    "    # Test accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test.to(device))\n",
    "        test_preds = test_output.argmax(dim=1)\n",
    "        test_acc = (test_preds == y_test.to(device)).float().mean().item()\n",
    "        test_accs.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Train Acc: {train_accs[-1]:.1%}, Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, label='Train')\n",
    "axes[1].plot(test_accs, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sentiment Analysis Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accs[-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Techniques\n",
    "\n",
    "### 8.1 Stacked (Deep) LSTMs\n",
    "\n",
    "Multiple LSTM layers for hierarchical feature learning:\n",
    "\n",
    "```python\n",
    "nn.LSTM(input_size, hidden_size, num_layers=3)\n",
    "```\n",
    "\n",
    "- First layer: Low-level patterns\n",
    "- Higher layers: Abstract representations\n",
    "- Dropout between layers prevents overfitting\n",
    "\n",
    "### 8.2 Attention Mechanisms (Preview)\n",
    "\n",
    "Instead of using only final hidden state, attend to all positions:\n",
    "\n",
    "$$\\text{context} = \\sum_t \\alpha_t h_t$$\n",
    "\n",
    "Where attention weights $\\alpha_t$ are learned.\n",
    "\n",
    "This will be covered in detail on Day 15!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTM example\n",
    "\n",
    "class DeepLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer LSTM with dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=3, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Combine final states from all layers\n",
    "        # hidden shape: [num_layers * 2, batch, hidden]\n",
    "        hidden_cat = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        normalized = self.layer_norm(hidden_cat)\n",
    "        \n",
    "        return self.fc(self.dropout(normalized))\n",
    "\n",
    "# Compare single vs multi-layer\n",
    "print(\"Deep LSTM Architecture:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "single_layer = SentimentLSTM(vocab_size, 32, 64)\n",
    "multi_layer = DeepLSTM(vocab_size, 32, 64, 2, num_layers=3)\n",
    "\n",
    "print(f\"Single layer LSTM params: {sum(p.numel() for p in single_layer.parameters()):,}\")\n",
    "print(f\"3-layer LSTM params: {sum(p.numel() for p in multi_layer.parameters()):,}\")\n",
    "\n",
    "print(\"\\n3-Layer LSTM Structure:\")\n",
    "print(multi_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention mechanism preview\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM with simple attention mechanism.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, seq, embed]\n",
    "        \n",
    "        # LSTM output at each position\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch, seq, hidden*2]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention(lstm_out).squeeze(-1)  # [batch, seq]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)  # [batch, seq]\n",
    "        \n",
    "        # Weighted sum (context vector)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), lstm_out)  # [batch, 1, hidden*2]\n",
    "        context = context.squeeze(1)  # [batch, hidden*2]\n",
    "        \n",
    "        # Classify\n",
    "        return self.fc(context), attention_weights\n",
    "\n",
    "# Demo attention\n",
    "print(\"LSTM with Attention:\")\n",
    "attention_model = AttentionLSTM(vocab_size, 32, 64, 2).to(device)\n",
    "\n",
    "# Test with sample\n",
    "sample_x = X_test[:5].to(device)\n",
    "with torch.no_grad():\n",
    "    output, attention_weights = attention_model(sample_x)\n",
    "\n",
    "# Visualize attention for one sample\n",
    "sample_idx = 0\n",
    "weights = attention_weights[sample_idx].cpu().numpy()\n",
    "words = [idx2word.get(idx, 'PAD') for idx in test_sentences[sample_idx]]\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.bar(range(len(weights)), weights)\n",
    "plt.xticks(range(len(words)), words, rotation=45, ha='right')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.title(f'Attention Weights\\nLabel: {\"Positive\" if test_labels[sample_idx] == 1 else \"Negative\"}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAttention allows the model to focus on relevant words!\")\n",
    "print(\"Notice: sentiment words should have higher attention weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LSTM vs GRU: When to Use What\n",
    "\n",
    "### 9.1 Comparison Table\n",
    "\n",
    "| Aspect | LSTM | GRU |\n",
    "|--------|------|-----|\n",
    "| Gates | 3 (forget, input, output) | 2 (reset, update) |\n",
    "| Parameters | More (~33% more) | Fewer |\n",
    "| Training speed | Slower | Faster |\n",
    "| Memory | More | Less |\n",
    "| Performance | Often slightly better | Often comparable |\n",
    "| Long sequences | Better | Good |\n",
    "| Small datasets | May overfit | Less prone |\n",
    "\n",
    "### 9.2 Practical Guidelines\n",
    "\n",
    "**Choose LSTM when:**\n",
    "- Very long sequences (>200 tokens)\n",
    "- Complex temporal patterns\n",
    "- Plenty of training data\n",
    "- Need fine-grained control over memory\n",
    "\n",
    "**Choose GRU when:**\n",
    "- Moderate sequence lengths\n",
    "- Limited computational resources\n",
    "- Smaller datasets\n",
    "- Need faster training/inference\n",
    "\n",
    "**In practice:**\n",
    "- Try both and compare!\n",
    "- Performance difference often minimal\n",
    "- GRU is becoming more popular due to simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Architecture complexity\n",
    "ax = axes[0, 0]\n",
    "models_compare = ['Vanilla RNN', 'GRU', 'LSTM']\n",
    "gates = [0, 2, 3]\n",
    "params_mult = [1, 3, 4]  # Relative parameter count\n",
    "\n",
    "x = np.arange(len(models_compare))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, gates, width, label='Number of Gates', color='steelblue', alpha=0.7)\n",
    "ax.bar(x + width/2, params_mult, width, label='Relative Parameters', color='coral', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_compare)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Architecture Complexity')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Memory capability comparison (from our tests)\n",
    "ax = axes[0, 1]\n",
    "for rnn_type, accs in results.items():\n",
    "    ax.plot(seq_lengths, accs, 'o-', linewidth=2, markersize=8, label=rnn_type)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Accuracy on Copy Task')\n",
    "ax.set_title('Long-Range Dependency (Copy Task)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# 3. Training time comparison (simulated)\n",
    "ax = axes[1, 0]\n",
    "seq_len_train = [10, 50, 100, 200]\n",
    "rnn_time = [1, 5, 10, 20]\n",
    "gru_time = [1.5, 7.5, 15, 30]\n",
    "lstm_time = [2, 10, 20, 40]\n",
    "\n",
    "ax.plot(seq_len_train, rnn_time, 's-', linewidth=2, label='RNN', markersize=8)\n",
    "ax.plot(seq_len_train, gru_time, 'o-', linewidth=2, label='GRU', markersize=8)\n",
    "ax.plot(seq_len_train, lstm_time, '^-', linewidth=2, label='LSTM', markersize=8)\n",
    "ax.set_xlabel('Sequence Length')\n",
    "ax.set_ylabel('Relative Training Time')\n",
    "ax.set_title('Training Time Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Use case recommendations\n",
    "ax = axes[1, 1]\n",
    "use_cases = [\n",
    "    ('Short sequences\\n(<50 tokens)', 'GRU or RNN'),\n",
    "    ('Medium sequences\\n(50-200 tokens)', 'GRU or LSTM'),\n",
    "    ('Long sequences\\n(>200 tokens)', 'LSTM'),\n",
    "    ('Limited resources', 'GRU'),\n",
    "    ('Maximum accuracy', 'LSTM (try both)')\n",
    "]\n",
    "\n",
    "y_pos = np.arange(len(use_cases))\n",
    "ax.barh(y_pos, [1]*len(use_cases), color='lightgray', alpha=0.5)\n",
    "for i, (case, recommendation) in enumerate(use_cases):\n",
    "    ax.text(0.05, i, f\"{case}: {recommendation}\", va='center', fontsize=11)\n",
    "\n",
    "ax.set_yticks([])\n",
    "ax.set_xticks([])\n",
    "ax.set_title('Use Case Recommendations')\n",
    "ax.set_xlim([0, 1])\n",
    "\n",
    "plt.suptitle('LSTM vs GRU: Comprehensive Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Gating mechanisms** solve the vanishing gradient problem through:\n",
    "   - Sigmoid gates controlling information flow\n",
    "   - Additive (not multiplicative) state updates\n",
    "   - Direct gradient pathways through time\n",
    "\n",
    "2. **LSTM Architecture:**\n",
    "   - Separate cell state (long-term) and hidden state (short-term)\n",
    "   - Three gates: Forget, Input, Output\n",
    "   - Most powerful for long sequences\n",
    "\n",
    "3. **GRU Architecture:**\n",
    "   - Simplified LSTM with two gates: Reset, Update\n",
    "   - Fewer parameters, faster training\n",
    "   - Often comparable performance\n",
    "\n",
    "4. **Practical Applications:**\n",
    "   - Sentiment analysis\n",
    "   - Language modeling\n",
    "   - Sequence classification\n",
    "   - Time series prediction\n",
    "\n",
    "5. **Best Practices:**\n",
    "   - Initialize forget gate bias to 1\n",
    "   - Use gradient clipping\n",
    "   - Apply dropout between layers\n",
    "   - Consider bidirectional processing\n",
    "\n",
    "### Next Steps (Day 15)\n",
    "\n",
    "- **Attention mechanisms**: Beyond simple recurrence\n",
    "- **Transformers**: Self-attention and parallel processing\n",
    "- This leads to modern NLP architectures (BERT, GPT)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: LSTM Backpropagation\n",
    "Implement the backward pass for the LSTM cell, computing gradients for all gates.\n",
    "\n",
    "### Exercise 2: Language Model\n",
    "Train an LSTM language model on a small corpus and generate text. Compare with vanilla RNN.\n",
    "\n",
    "### Exercise 3: Time Series Prediction\n",
    "Use LSTM to predict stock prices or weather patterns from historical data.\n",
    "\n",
    "### Exercise 4: Named Entity Recognition\n",
    "Implement sequence labeling with BiLSTM for NER task.\n",
    "\n",
    "### Exercise 5: Variational Dropout\n",
    "Implement variational dropout (same mask across time steps) and compare with standard dropout.\n",
    "\n",
    "### Exercise 6: Peephole Connections\n",
    "Add peephole connections to LSTM (gates also see cell state).\n",
    "\n",
    "### Exercise 7: Seq2Seq Model\n",
    "Implement encoder-decoder LSTM for simple translation or text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starter code for Exercise 3: Time Series Prediction\n",
    "\n",
    "def create_sine_wave_data(num_samples=1000, seq_length=50, pred_length=10):\n",
    "    \"\"\"\n",
    "    Create sine wave prediction task.\n",
    "    Input: seq_length points of sine wave\n",
    "    Target: next pred_length points\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Random starting point\n",
    "        start = np.random.uniform(0, 2*np.pi)\n",
    "        # Generate sequence\n",
    "        t = np.linspace(start, start + 4*np.pi, seq_length + pred_length)\n",
    "        wave = np.sin(t)\n",
    "        \n",
    "        X.append(wave[:seq_length])\n",
    "        y.append(wave[seq_length:])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate data\n",
    "X_sine, y_sine = create_sine_wave_data()\n",
    "print(f\"Input shape: {X_sine.shape}\")\n",
    "print(f\"Target shape: {y_sine.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "sample_idx = 0\n",
    "plt.plot(range(50), X_sine[sample_idx], 'b-', linewidth=2, label='Input')\n",
    "plt.plot(range(50, 60), y_sine[sample_idx], 'r-', linewidth=2, label='Target')\n",
    "plt.axvline(x=49.5, color='gray', linestyle='--')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sine Wave Prediction Task')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nExercise: Build LSTM model to predict future sine wave values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Hochreiter, S., & Schmidhuber, J. (1997). \"Long Short-Term Memory.\" Neural Computation.\n",
    "2. Cho, K., et al. (2014). \"Learning Phrase Representations using RNN Encoder-Decoder.\" EMNLP.\n",
    "3. Gers, F. A., et al. (2000). \"Learning to Forget: Continual Prediction with LSTM.\" Neural Computation.\n",
    "4. Greff, K., et al. (2017). \"LSTM: A Search Space Odyssey.\" IEEE Transactions on Neural Networks.\n",
    "5. Jozefowicz, R., et al. (2015). \"An Empirical Exploration of Recurrent Network Architectures.\" ICML.\n",
    "6. Chung, J., et al. (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks.\" NeurIPS Workshop.\n",
    "7. Olah, C. (2015). \"Understanding LSTM Networks.\" Blog post."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
