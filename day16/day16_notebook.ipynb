{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 16: Attention Mechanisms\n",
    "\n",
    "Learn attention mechanisms that revolutionized NLP and paved the way for Transformers.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the information bottleneck problem in sequence models\n",
    "- Implement attention mechanisms (Bahdanau and Luong)\n",
    "- Apply attention to seq2seq models\n",
    "- Visualize and analyze attention patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Attention\n",
    "\n",
    "### The Problem: Information Bottleneck\n",
    "In seq2seq models, the encoder compresses the entire input sequence into a single context vector.\n",
    "This becomes problematic for long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the bottleneck problem\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Without attention\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-1, 10)\n",
    "ax.set_ylim(-1, 5)\n",
    "# Draw encoder boxes\n",
    "for i in range(5):\n",
    "    ax.add_patch(plt.Rectangle((i, 3), 0.8, 0.8, fill=True, color='lightblue', edgecolor='black'))\n",
    "    ax.text(i+0.4, 3.4, f'x{i}', ha='center', va='center')\n",
    "# Draw bottleneck\n",
    "ax.add_patch(plt.Rectangle((2, 1.5), 1, 0.8, fill=True, color='red', edgecolor='black', linewidth=2))\n",
    "ax.text(2.5, 1.9, 'c', ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "# Draw decoder boxes\n",
    "for i in range(3):\n",
    "    ax.add_patch(plt.Rectangle((2+i*0.5, 0), 0.8, 0.8, fill=True, color='lightgreen', edgecolor='black'))\n",
    "    ax.text(2.4+i*0.5, 0.4, f'y{i}', ha='center', va='center')\n",
    "ax.set_title('Without Attention (Bottleneck)', fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# With attention\n",
    "ax = axes[1]\n",
    "ax.set_xlim(-1, 10)\n",
    "ax.set_ylim(-1, 5)\n",
    "# Draw encoder boxes\n",
    "for i in range(5):\n",
    "    ax.add_patch(plt.Rectangle((i, 3), 0.8, 0.8, fill=True, color='lightblue', edgecolor='black'))\n",
    "    ax.text(i+0.4, 3.4, f'x{i}', ha='center', va='center')\n",
    "# Draw attention arrows\n",
    "for i in range(5):\n",
    "    ax.arrow(i+0.4, 2.9, 2, -1.5, head_width=0.2, head_length=0.1, fc='orange', ec='orange', alpha=0.6)\n",
    "# Draw decoder boxes\n",
    "for i in range(3):\n",
    "    ax.add_patch(plt.Rectangle((2+i*0.5, 0), 0.8, 0.8, fill=True, color='lightgreen', edgecolor='black'))\n",
    "    ax.text(2.4+i*0.5, 0.4, f'y{i}', ha='center', va='center')\n",
    "ax.set_title('With Attention (Dynamic Context)', fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_bottleneck.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Attention allows the decoder to focus on different parts of the input at each step!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n    \"\"\"Additive (Bahdanau) Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Linear layers for attention\n",
    "        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, 1, hidden_size) - decoder hidden state\n",
    "            keys: (batch_size, seq_len, hidden_size) - encoder hidden states\n",
    "            values: (batch_size, seq_len, hidden_size) - encoder hidden states\n",
    "            mask: (batch_size, seq_len) - padding mask\n",
    "        \n",
    "        Returns:\n",
    "            context: (batch_size, 1, hidden_size) - context vector\n",
    "            attention_weights: (batch_size, 1, seq_len) - attention weights\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        Q = self.W_q(query)  # (batch_size, 1, hidden_size)\n",
    "        K = self.W_k(keys)   # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Score = v^T * tanh(Q + K)\n",
    "        scores = self.v(torch.tanh(Q + K))  # (batch_size, seq_len, 1)\n",
    "        scores = scores.squeeze(-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Compute weighted sum of values\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), values)  # (batch_size, 1, hidden_size)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print('✓ BahdanauAttention (Additive) implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    \"\"\"Multiplicative (Luong) Attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "    \n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, 1, hidden_size)\n",
    "            keys: (batch_size, seq_len, hidden_size)\n",
    "            values: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        Returns:\n",
    "            context: (batch_size, 1, hidden_size)\n",
    "            attention_weights: (batch_size, 1, seq_len)\n",
    "        \"\"\"\n",
    "        # Score = Q * W * K^T\n",
    "        Q = self.W(query)  # (batch_size, 1, hidden_size)\n",
    "        scores = torch.bmm(Q, keys.transpose(1, 2))  # (batch_size, 1, seq_len)\n",
    "        scores = scores.squeeze(1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Compute context\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), values)  # (batch_size, 1, hidden_size)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print('✓ LuongAttention (Multiplicative) implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_name": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention (used in Transformers)\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.scale = np.sqrt(hidden_size)\n",
    "    \n",
    "    def forward(self, query, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, ..., hidden_size)\n",
    "            keys: (batch_size, seq_len, hidden_size)\n",
    "            values: (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # Compute scaled dot product\n",
    "        scores = torch.matmul(query, keys.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Compute context\n",
    "        context = torch.matmul(attention_weights, values)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print('✓ ScaledDotProductAttention implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Test Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample inputs\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "hidden_size = 64\n",
    "\n",
    "query = torch.randn(batch_size, 1, hidden_size).to(device)\n",
    "keys = torch.randn(batch_size, seq_len, hidden_size).to(device)\n",
    "values = torch.randn(batch_size, seq_len, hidden_size).to(device)\n",
    "\n",
    "# Test each attention mechanism\n",
    "bahdanau = BahdanauAttention(hidden_size).to(device)\n",
    "luong = LuongAttention(hidden_size).to(device)\n",
    "scaled = ScaledDotProductAttention(hidden_size).to(device)\n",
    "\n",
    "# Forward pass\n",
    "context_b, weights_b = bahdanau(query, keys, values)\n",
    "context_l, weights_l = luong(query, keys, values)\n",
    "context_s, weights_s = scaled(query, keys, values)\n",
    "\n",
    "print(f'Context shape: {context_b.shape}')\n",
    "print(f'Attention weights shape: {weights_b.shape}')\n",
    "print(f'Attention weights sum: {weights_b.sum(dim=1)}')\n",
    "print('✓ All attention mechanisms working!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualize Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights from different mechanisms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Bahdanau\n",
    "sns.heatmap(weights_b[0].detach().cpu().numpy().reshape(1, -1), \n",
    "            ax=axes[0], cbar=True, cmap='Blues')\n",
    "axes[0].set_title('Bahdanau Attention')\n",
    "axes[0].set_xlabel('Source Sequence Position')\n",
    "\n",
    "# Luong\n",
    "sns.heatmap(weights_l[0].detach().cpu().numpy().reshape(1, -1), \n",
    "            ax=axes[1], cbar=True, cmap='Greens')\n",
    "axes[1].set_title('Luong Attention')\n",
    "axes[1].set_xlabel('Source Sequence Position')\n",
    "\n",
    "# Scaled Dot-Product\n",
    "sns.heatmap(weights_s[0].detach().cpu().numpy().reshape(1, -1), \n",
    "            ax=axes[2], cbar=True, cmap='Reds')\n",
    "axes[2].set_title('Scaled Dot-Product Attention')\n",
    "axes[2].set_xlabel('Source Sequence Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('✓ Attention patterns visualized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of attention mechanisms\n",
    "print(\"=\" * 60)\n",
    "print(\"ATTENTION MECHANISMS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. BAHDANAU (ADDITIVE) ATTENTION\")\n",
    "print(\"   - Score = v^T * tanh(W_q*q + W_k*k)\")\n",
    "print(\"   - Computational cost: O(seq_len * hidden_size)\")\n",
    "print(\"   - Good for shorter sequences\")\n",
    "\n",
    "print(\"\\n2. LUONG (MULTIPLICATIVE) ATTENTION\")\n",
    "print(\"   - Score = q^T * W * k\")\n",
    "print(\"   - Computational cost: O(seq_len * hidden_size)\")\n",
    "print(\"   - More efficient matrix operations\")\n",
    "\n",
    "print(\"\\n3. SCALED DOT-PRODUCT ATTENTION\")\n",
    "print(\"   - Score = (q * k^T) / sqrt(d_k)\")\n",
    "print(\"   - Scaling prevents gradient vanishing\")\n",
    "print(\"   - Foundation of Transformer attention\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✓ Attention solves the information bottleneck problem\")\n",
    "print(\"✓ Decoder can focus on relevant input parts\")\n",
    "print(\"✓ Different mechanisms have different computational costs\")\n",
    "print(\"✓ Scaled dot-product is most efficient (matrix operations)\")\n",
    "print(\"✓ Attention weights are interpretable and visualizable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
