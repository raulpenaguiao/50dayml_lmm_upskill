# Day 16: Attention Mechanisms - Further Reading

This day explores attention mechanismsâ€”a technique that allows models to focus on relevant parts of the input when making decisions, dramatically improving performance on sequence-to-sequence tasks.

## References

1. **Neural Machine Translation by Jointly Learning to Align and Translate**
   - [Bahdanau et al.: Attention Paper](https://arxiv.org/abs/1409.0473)
   - Seminal paper introducing attention mechanism to solve information bottleneck in seq2seq models, enabling better translation.

2. **Effective Approaches to Attention-based Neural Machine Translation**
   - [Luong et al.: Multiplicative Attention](https://arxiv.org/abs/1508.04025)
   - Paper introducing multiplicative (Luong) attention and comparing with additive (Bahdanau) attention with empirical results.

3. **Understanding Attention Visualization**
   - [Attention Mechanism Explained](https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f)
   - Intuitive explanation with visualizations showing how attention learns to align source and target sequences.

4. **PyTorch Attention Implementation**
   - [PyTorch: Attention Patterns](https://pytorch.org/docs/stable/nn.html#multiheadattention)
   - Official documentation with implementations of attention layers for practical use in neural networks.

5. **Attention in NLP: A Survey**
   - [A Thorough Examination of the CNN/Daily Mail Dataset](https://arxiv.org/abs/1602.00563)
   - Paper analyzing attention mechanisms on NLP tasks with insights into what attention learns to focus on.

---

**Tip:** Start with reference #3 for intuitive understanding, then #1-2 for theoretical foundations, and #4 for PyTorch implementation.
