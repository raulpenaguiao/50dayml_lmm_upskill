{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11: Introduction to Natural Language Processing\n",
    "\n",
    "## Phase 2: Deep Learning & NLP Basics (Days 11-20)\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Mathematical Prerequisites:**\n",
    "- Information theory (entropy, mutual information)\n",
    "- Probability (distributions, conditional probability)\n",
    "- Linear algebra (vector spaces, similarity measures)\n",
    "- Statistics (frequency analysis, n-grams)\n",
    "\n",
    "---\n",
    "\n",
    "## Welcome to NLP!\n",
    "\n",
    "After mastering CNNs and image classification in Phase 1, we now transition to **Natural Language Processing (NLP)** - the art and science of making machines understand human language.\n",
    "\n",
    "### Phase 2 Roadmap:\n",
    "- **Day 11:** NLP Fundamentals (TODAY)\n",
    "- **Day 12:** Word Embeddings (Word2Vec, GloVe)\n",
    "- **Day 13:** Recurrent Neural Networks (RNNs)\n",
    "- **Day 14:** Long Short-Term Memory (LSTMs)\n",
    "- **Day 15:** Sequence-to-Sequence Models\n",
    "- **Day 16:** Attention Mechanisms\n",
    "- **Day 17:** Introduction to Transformers\n",
    "- **Day 18:** Text Classification\n",
    "- **Day 19:** Character-Level Language Models\n",
    "- **Day 20:** Project 2 - Text Generation\n",
    "\n",
    "---\n",
    "\n",
    "## Today's Objectives\n",
    "\n",
    "1. Understand the unique challenges of NLP\n",
    "2. Text preprocessing pipeline (cleaning, normalization)\n",
    "3. Tokenization strategies (word, subword, character)\n",
    "4. Vocabulary building and OOV handling\n",
    "5. Text representation (Bag of Words, TF-IDF)\n",
    "6. N-gram language models\n",
    "7. Evaluation metrics for language models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The NLP Challenge\n",
    "\n",
    "### 1.1 Why is NLP Hard?\n",
    "\n",
    "**Images vs Text:**\n",
    "- Images: Fixed-size grid of continuous pixel values\n",
    "- Text: Variable-length sequences of discrete symbols\n",
    "\n",
    "**Key Challenges:**\n",
    "\n",
    "1. **Ambiguity:** \"I saw her duck\" (animal or verb?)\n",
    "2. **Context dependence:** \"Apple\" (fruit or company?)\n",
    "3. **Variable length:** Sentences can be 1 word or 1000 words\n",
    "4. **Discrete symbols:** Words are categorical, not continuous\n",
    "5. **Vocabulary explosion:** Millions of possible words\n",
    "6. **Out-of-vocabulary (OOV):** New words constantly appear\n",
    "7. **Long-range dependencies:** \"The cat, which was sitting on the mat, was sleeping\"\n",
    "\n",
    "### 1.2 The NLP Pipeline\n",
    "\n",
    "```\n",
    "Raw Text → Preprocessing → Tokenization → Numericalization → Model → Output\n",
    "```\n",
    "\n",
    "Today we focus on the first three steps: **Preprocessing**, **Tokenization**, and **Numericalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import string\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Scikit-learn for vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"NLP environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing\n",
    "\n",
    "### 2.1 Sample Dataset\n",
    "\n",
    "We'll use sample text data to demonstrate preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "sample_texts = [\n",
    "    \"Natural Language Processing (NLP) is a field of Artificial Intelligence!\",\n",
    "    \"It's focused on enabling computers to understand human language.\",\n",
    "    \"NLP applications include: chatbots, translation, sentiment analysis, etc.\",\n",
    "    \"Deep learning has revolutionized NLP in the 2010s and 2020s.\",\n",
    "    \"The transformer architecture (2017) was a breakthrough moment!!!\",\n",
    "    \"Models like BERT, GPT-3, and ChatGPT show remarkable capabilities.\",\n",
    "    \"But challenges remain: bias, hallucination, reasoning, etc.\",\n",
    "    \"The future of NLP is exciting... and a bit scary too :)\",\n",
    "]\n",
    "\n",
    "print(\"Sample corpus:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Cleaning\n",
    "\n",
    "Common preprocessing steps:\n",
    "1. Lowercasing\n",
    "2. Removing punctuation\n",
    "3. Removing numbers\n",
    "4. Removing extra whitespace\n",
    "5. Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def lowercase(self, text):\n",
    "        \"\"\"Convert to lowercase.\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"Remove punctuation marks.\"\"\"\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    def remove_numbers(self, text):\n",
    "        \"\"\"Remove digits.\"\"\"\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    def remove_extra_whitespace(self, text):\n",
    "        \"\"\"Remove extra whitespace.\"\"\"\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_special_chars(self, text):\n",
    "        \"\"\"Remove special characters (keep letters and spaces).\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove common stop words.\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "    \n",
    "    def stem(self, tokens):\n",
    "        \"\"\"Apply stemming (reduce to root form).\"\"\"\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        \"\"\"Apply lemmatization (reduce to dictionary form).\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Apply full cleaning pipeline.\"\"\"\n",
    "        text = self.lowercase(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        return text\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Demonstrate preprocessing\n",
    "sample_text = sample_texts[0]\n",
    "print(\"Original text:\")\n",
    "print(f\"  {sample_text}\")\n",
    "print(\"\\nStep-by-step preprocessing:\")\n",
    "\n",
    "# Step 1: Lowercase\n",
    "text = preprocessor.lowercase(sample_text)\n",
    "print(f\"1. Lowercase: {text}\")\n",
    "\n",
    "# Step 2: Remove punctuation\n",
    "text = preprocessor.remove_punctuation(text)\n",
    "print(f\"2. Remove punctuation: {text}\")\n",
    "\n",
    "# Step 3: Remove extra whitespace\n",
    "text = preprocessor.remove_extra_whitespace(text)\n",
    "print(f\"3. Remove whitespace: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stemming vs Lemmatization\n",
    "\n",
    "**Stemming:** Chop off word endings (fast but crude)\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"better\" (no change)\n",
    "\n",
    "**Lemmatization:** Use dictionary to find root form (slower but accurate)\n",
    "- \"running\" → \"run\"\n",
    "- \"better\" → \"good\"\n",
    "\n",
    "**When to use:**\n",
    "- Stemming: When speed matters, exact forms less important (search engines)\n",
    "- Lemmatization: When linguistic accuracy matters (text classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare stemming vs lemmatization\n",
    "test_words = ['running', 'runs', 'ran', 'better', 'studies', 'studying', 'mice', 'feet']\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Original': test_words,\n",
    "    'Stemmed': preprocessor.stem(test_words),\n",
    "    'Lemmatized': preprocessor.lemmatize(test_words)\n",
    "})\n",
    "\n",
    "print(\"Stemming vs Lemmatization:\")\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Tokenization\n",
    "\n",
    "### 3.1 What is Tokenization?\n",
    "\n",
    "**Tokenization** splits text into meaningful units (tokens).\n",
    "\n",
    "**Types:**\n",
    "1. **Word tokenization:** Split by words\n",
    "2. **Sentence tokenization:** Split by sentences\n",
    "3. **Subword tokenization:** Split into subword units (BPE, WordPiece)\n",
    "4. **Character tokenization:** Split into characters\n",
    "\n",
    "### 3.2 Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different tokenization approaches\n",
    "text = \"It's a beautiful day! Don't you think? I'd love to go outside.\"\n",
    "\n",
    "print(f\"Original: {text}\\n\")\n",
    "\n",
    "# 1. Naive split by space\n",
    "naive_tokens = text.split()\n",
    "print(f\"Naive (split by space):\")\n",
    "print(f\"  {naive_tokens}\\n\")\n",
    "\n",
    "# 2. NLTK word_tokenize\n",
    "nltk_tokens = word_tokenize(text)\n",
    "print(f\"NLTK word_tokenize:\")\n",
    "print(f\"  {nltk_tokens}\\n\")\n",
    "\n",
    "# 3. Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentence tokenization:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"  {i+1}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Building a Vocabulary\n",
    "\n",
    "A **vocabulary** maps tokens to integer IDs.\n",
    "\n",
    "**Key considerations:**\n",
    "- Vocabulary size (memory vs coverage trade-off)\n",
    "- Special tokens: `<PAD>`, `<UNK>`, `<SOS>`, `<EOS>`\n",
    "- Handling out-of-vocabulary (OOV) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary for mapping tokens to indices.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_freq=1):\n",
    "        self.min_freq = min_freq\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.SOS_TOKEN = '<SOS>'\n",
    "        self.EOS_TOKEN = '<EOS>'\n",
    "        \n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        self.token_freqs = Counter()\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self._add_special_tokens()\n",
    "    \n",
    "    def _add_special_tokens(self):\n",
    "        \"\"\"Add special tokens to vocabulary.\"\"\"\n",
    "        for token in [self.PAD_TOKEN, self.UNK_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN]:\n",
    "            self._add_token(token)\n",
    "    \n",
    "    def _add_token(self, token):\n",
    "        \"\"\"Add single token to vocabulary.\"\"\"\n",
    "        if token not in self.token2idx:\n",
    "            idx = len(self.token2idx)\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "    \n",
    "    def build_vocab(self, texts, tokenizer=None):\n",
    "        \"\"\"Build vocabulary from list of texts.\"\"\"\n",
    "        if tokenizer is None:\n",
    "            tokenizer = lambda x: x.lower().split()\n",
    "        \n",
    "        # Count token frequencies\n",
    "        for text in texts:\n",
    "            tokens = tokenizer(text)\n",
    "            self.token_freqs.update(tokens)\n",
    "        \n",
    "        # Add tokens with frequency >= min_freq\n",
    "        for token, freq in self.token_freqs.most_common():\n",
    "            if freq >= self.min_freq:\n",
    "                self._add_token(token)\n",
    "        \n",
    "        print(f\"Vocabulary built: {len(self)} tokens\")\n",
    "    \n",
    "    def encode(self, text, tokenizer=None):\n",
    "        \"\"\"Convert text to list of indices.\"\"\"\n",
    "        if tokenizer is None:\n",
    "            tokenizer = lambda x: x.lower().split()\n",
    "        \n",
    "        tokens = tokenizer(text)\n",
    "        indices = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.token2idx:\n",
    "                indices.append(self.token2idx[token])\n",
    "            else:\n",
    "                indices.append(self.token2idx[self.UNK_TOKEN])\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of indices back to text.\"\"\"\n",
    "        tokens = [self.idx2token.get(idx, self.UNK_TOKEN) for idx in indices]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token2idx)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.token2idx.get(token, self.token2idx[self.UNK_TOKEN])\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(min_freq=1)\n",
    "vocab.build_vocab(sample_texts)\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nFirst 20 tokens:\")\n",
    "for i in range(min(20, len(vocab))):\n",
    "    print(f\"  {i}: {vocab.idx2token[i]}\")\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_text = \"NLP is a field of artificial intelligence\"\n",
    "encoded = vocab.encode(test_text)\n",
    "decoded = vocab.decode(encoded)\n",
    "\n",
    "print(f\"\\nTest text: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Subword Tokenization\n",
    "\n",
    "**Problem with word tokenization:**\n",
    "- Large vocabulary (100K+ words)\n",
    "- OOV problem for rare/new words\n",
    "- No sharing between similar words (\"run\", \"running\", \"runner\")\n",
    "\n",
    "**Subword tokenization** breaks words into smaller units:\n",
    "- \"unhappiness\" → \"un\", \"happi\", \"ness\"\n",
    "- \"transformer\" → \"trans\", \"former\"\n",
    "\n",
    "**Popular methods:**\n",
    "1. **Byte Pair Encoding (BPE)** - Used in GPT\n",
    "2. **WordPiece** - Used in BERT\n",
    "3. **SentencePiece** - Language-agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"Simple Byte Pair Encoding implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = []\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def get_pairs(self, word):\n",
    "        \"\"\"Get all pairs of consecutive symbols.\"\"\"\n",
    "        pairs = Counter()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs[(prev_char, char)] += 1\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair(self, word, pair):\n",
    "        \"\"\"Merge a pair in the word.\"\"\"\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                new_word.append(pair[0] + pair[1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "        return new_word\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        \"\"\"Learn BPE merges from corpus.\"\"\"\n",
    "        # Tokenize into characters with word boundaries\n",
    "        word_freqs = Counter()\n",
    "        for text in corpus:\n",
    "            for word in text.lower().split():\n",
    "                word_freqs[tuple(word) + ('</w>',)] += 1\n",
    "        \n",
    "        # Initialize vocabulary with characters\n",
    "        self.vocab = set()\n",
    "        for word in word_freqs:\n",
    "            for char in word:\n",
    "                self.vocab.add(char)\n",
    "        \n",
    "        print(f\"Initial vocab size (characters): {len(self.vocab)}\")\n",
    "        \n",
    "        # Iteratively merge most frequent pairs\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            # Count all pairs\n",
    "            pair_freqs = Counter()\n",
    "            for word, freq in word_freqs.items():\n",
    "                pairs = self.get_pairs(word)\n",
    "                for pair, count in pairs.items():\n",
    "                    pair_freqs[pair] += count * freq\n",
    "            \n",
    "            if not pair_freqs:\n",
    "                break\n",
    "            \n",
    "            # Get most frequent pair\n",
    "            best_pair = pair_freqs.most_common(1)[0][0]\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            # Add merged token to vocab\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab.add(new_token)\n",
    "            \n",
    "            # Apply merge to all words\n",
    "            new_word_freqs = Counter()\n",
    "            for word, freq in word_freqs.items():\n",
    "                new_word = self.merge_pair(list(word), best_pair)\n",
    "                new_word_freqs[tuple(new_word)] = freq\n",
    "            \n",
    "            word_freqs = new_word_freqs\n",
    "        \n",
    "        print(f\"Final vocab size: {len(self.vocab)}\")\n",
    "        print(f\"Number of merges: {len(self.merges)}\")\n",
    "    \n",
    "    def tokenize(self, word):\n",
    "        \"\"\"Tokenize a single word using learned merges.\"\"\"\n",
    "        word = list(word) + ['</w>']\n",
    "        \n",
    "        for pair in self.merges:\n",
    "            word = self.merge_pair(word, pair)\n",
    "        \n",
    "        return word\n",
    "\n",
    "# Train BPE\n",
    "bpe = SimpleBPE(vocab_size=50)\n",
    "bpe.fit(sample_texts)\n",
    "\n",
    "# Test tokenization\n",
    "test_words = ['natural', 'processing', 'transformer', 'artificial']\n",
    "print(\"\\nBPE Tokenization:\")\n",
    "for word in test_words:\n",
    "    tokens = bpe.tokenize(word.lower())\n",
    "    print(f\"  {word} → {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Character-Level Tokenization\n",
    "\n",
    "**Advantages:**\n",
    "- No OOV problem (closed vocabulary)\n",
    "- Very small vocabulary (~100 chars)\n",
    "- Can handle any word, including misspellings\n",
    "\n",
    "**Disadvantages:**\n",
    "- Very long sequences\n",
    "- Harder to learn semantic meaning\n",
    "- Slower processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Character-level tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self._add_char(self.PAD_TOKEN)\n",
    "        self._add_char(self.UNK_TOKEN)\n",
    "    \n",
    "    def _add_char(self, char):\n",
    "        if char not in self.char2idx:\n",
    "            idx = len(self.char2idx)\n",
    "            self.char2idx[char] = idx\n",
    "            self.idx2char[idx] = char\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build character vocabulary from texts.\"\"\"\n",
    "        for text in texts:\n",
    "            for char in text:\n",
    "                self._add_char(char)\n",
    "        \n",
    "        print(f\"Character vocabulary size: {len(self)}\")\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to character indices.\"\"\"\n",
    "        return [self.char2idx.get(c, self.char2idx[self.UNK_TOKEN]) for c in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert indices back to text.\"\"\"\n",
    "        return ''.join([self.idx2char.get(i, self.UNK_TOKEN) for i in indices])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "# Build character tokenizer\n",
    "char_tokenizer = CharacterTokenizer()\n",
    "char_tokenizer.fit(sample_texts)\n",
    "\n",
    "# Test\n",
    "test_text = \"Hello NLP!\"\n",
    "char_encoded = char_tokenizer.encode(test_text)\n",
    "char_decoded = char_tokenizer.decode(char_encoded)\n",
    "\n",
    "print(f\"\\nTest: {test_text}\")\n",
    "print(f\"Encoded: {char_encoded}\")\n",
    "print(f\"Decoded: {char_decoded}\")\n",
    "print(f\"Sequence length: {len(char_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Text Representation\n",
    "\n",
    "### 4.1 Bag of Words (BoW)\n",
    "\n",
    "Represent text as vector of word counts.\n",
    "\n",
    "**Example:**\n",
    "- Vocabulary: [\"the\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "- Text: \"the cat sat on the mat\"\n",
    "- BoW: [2, 1, 1, 1, 1] (count of each word)\n",
    "\n",
    "**Properties:**\n",
    "- Ignores word order (hence \"bag\")\n",
    "- High-dimensional (vocab_size dimensions)\n",
    "- Sparse vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words with sklearn\n",
    "bow_vectorizer = CountVectorizer(max_features=50)\n",
    "bow_matrix = bow_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"Bag of Words Representation:\")\n",
    "print(f\"Matrix shape: {bow_matrix.shape} (documents × vocabulary)\")\n",
    "\n",
    "# Show vocabulary\n",
    "vocab_bow = bow_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nVocabulary ({len(vocab_bow)} words):\")\n",
    "print(vocab_bow)\n",
    "\n",
    "# Show BoW for first document\n",
    "print(f\"\\nFirst document: {sample_texts[0]}\")\n",
    "print(f\"BoW vector: {bow_matrix[0].toarray()[0]}\")\n",
    "\n",
    "# Visualize BoW matrix\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.heatmap(bow_matrix.toarray(), annot=False, cmap='YlOrRd', ax=ax,\n",
    "            xticklabels=vocab_bow, yticklabels=[f'Doc {i+1}' for i in range(len(sample_texts))])\n",
    "ax.set_title('Bag of Words Matrix (Document-Term Matrix)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Words', fontsize=12)\n",
    "ax.set_ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "Weight words by their importance.\n",
    "\n",
    "**Term Frequency (TF):**\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{total words in } d}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (IDF):**\n",
    "$$\n",
    "IDF(t) = \\log\\frac{N}{|\\{d : t \\in d\\}|}\n",
    "$$\n",
    "where $N$ is total number of documents.\n",
    "\n",
    "**TF-IDF:**\n",
    "$$\n",
    "TF\\text{-}IDF(t, d) = TF(t, d) \\cdot IDF(t)\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- High TF-IDF: Word appears frequently in document but rarely in corpus (important word)\n",
    "- Low TF-IDF: Word appears rarely or appears in many documents (common word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=50)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "print(\"TF-IDF Representation:\")\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "vocab_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Show TF-IDF for first document\n",
    "print(f\"\\nFirst document: {sample_texts[0]}\")\n",
    "tfidf_scores = tfidf_matrix[0].toarray()[0]\n",
    "print(f\"TF-IDF vector (first 10):\")\n",
    "for i in range(min(10, len(vocab_tfidf))):\n",
    "    if tfidf_scores[i] > 0:\n",
    "        print(f\"  {vocab_tfidf[i]}: {tfidf_scores[i]:.4f}\")\n",
    "\n",
    "# Visualize TF-IDF matrix\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.heatmap(tfidf_matrix.toarray(), annot=False, cmap='YlOrRd', ax=ax,\n",
    "            xticklabels=vocab_tfidf, yticklabels=[f'Doc {i+1}' for i in range(len(sample_texts))])\n",
    "ax.set_title('TF-IDF Matrix (Weighted Document-Term Matrix)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Words', fontsize=12)\n",
    "ax.set_ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare BoW vs TF-IDF\n",
    "print(\"\\nBoW vs TF-IDF for 'nlp':\")\n",
    "if 'nlp' in vocab_bow:\n",
    "    bow_idx = list(vocab_bow).index('nlp')\n",
    "    print(f\"  BoW counts: {bow_matrix[:, bow_idx].toarray().flatten()}\")\n",
    "if 'nlp' in vocab_tfidf:\n",
    "    tfidf_idx = list(vocab_tfidf).index('nlp')\n",
    "    print(f\"  TF-IDF scores: {tfidf_matrix[:, tfidf_idx].toarray().flatten().round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: N-gram Language Models\n",
    "\n",
    "### 5.1 N-gram Theory\n",
    "\n",
    "An **n-gram** is a sequence of n consecutive words.\n",
    "\n",
    "**Examples:**\n",
    "- Unigram (n=1): \"the\", \"cat\", \"sat\"\n",
    "- Bigram (n=2): \"the cat\", \"cat sat\"\n",
    "- Trigram (n=3): \"the cat sat\"\n",
    "\n",
    "**N-gram Language Model:**\n",
    "Estimate probability of next word given previous n-1 words.\n",
    "\n",
    "$$\n",
    "P(w_n | w_1, ..., w_{n-1}) \\approx P(w_n | w_{n-N+1}, ..., w_{n-1})\n",
    "$$\n",
    "\n",
    "**Markov Assumption:** Only the last n-1 words matter.\n",
    "\n",
    "**Estimation:**\n",
    "$$\n",
    "P(w_n | w_{n-N+1}, ..., w_{n-1}) = \\frac{C(w_{n-N+1}, ..., w_n)}{C(w_{n-N+1}, ..., w_{n-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    \"\"\"N-gram language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngram_counts = Counter()\n",
    "        self.context_counts = Counter()\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def train(self, texts):\n",
    "        \"\"\"Train the n-gram model.\"\"\"\n",
    "        for text in texts:\n",
    "            tokens = ['<s>'] * (self.n - 1) + text.lower().split() + ['</s>']\n",
    "            self.vocab.update(tokens)\n",
    "            \n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i+self.n])\n",
    "                context = tuple(tokens[i:i+self.n-1])\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "        \n",
    "        print(f\"{self.n}-gram model trained\")\n",
    "        print(f\"  Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"  Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "    \n",
    "    def probability(self, word, context):\n",
    "        \"\"\"Estimate P(word | context) using MLE.\"\"\"\n",
    "        context = tuple(context[-(self.n-1):])\n",
    "        ngram = context + (word,)\n",
    "        \n",
    "        if self.context_counts[context] == 0:\n",
    "            return 1 / len(self.vocab)  # Uniform fallback\n",
    "        \n",
    "        return self.ngram_counts[ngram] / self.context_counts[context]\n",
    "    \n",
    "    def generate(self, start_tokens=None, max_length=20):\n",
    "        \"\"\"Generate text using the model.\"\"\"\n",
    "        if start_tokens is None:\n",
    "            tokens = ['<s>'] * (self.n - 1)\n",
    "        else:\n",
    "            tokens = ['<s>'] * (self.n - 1) + start_tokens\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            context = tuple(tokens[-(self.n-1):])\n",
    "            \n",
    "            # Get possible next words\n",
    "            candidates = []\n",
    "            probs = []\n",
    "            \n",
    "            for word in self.vocab:\n",
    "                ngram = context + (word,)\n",
    "                if self.ngram_counts[ngram] > 0:\n",
    "                    candidates.append(word)\n",
    "                    probs.append(self.ngram_counts[ngram])\n",
    "            \n",
    "            if not candidates:\n",
    "                break\n",
    "            \n",
    "            # Sample next word\n",
    "            probs = np.array(probs) / sum(probs)\n",
    "            next_word = np.random.choice(candidates, p=probs)\n",
    "            \n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            \n",
    "            tokens.append(next_word)\n",
    "        \n",
    "        # Remove start tokens\n",
    "        return ' '.join([t for t in tokens if t not in ['<s>', '</s>']])\n",
    "    \n",
    "    def perplexity(self, text):\n",
    "        \"\"\"Calculate perplexity on text.\"\"\"\n",
    "        tokens = ['<s>'] * (self.n - 1) + text.lower().split() + ['</s>']\n",
    "        \n",
    "        log_prob = 0\n",
    "        num_tokens = 0\n",
    "        \n",
    "        for i in range(self.n - 1, len(tokens)):\n",
    "            context = tokens[i-self.n+1:i]\n",
    "            word = tokens[i]\n",
    "            prob = self.probability(word, context)\n",
    "            \n",
    "            if prob > 0:\n",
    "                log_prob += np.log(prob)\n",
    "                num_tokens += 1\n",
    "        \n",
    "        if num_tokens == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        return np.exp(-log_prob / num_tokens)\n",
    "\n",
    "\n",
    "# Train bigram model\n",
    "bigram_model = NGramLanguageModel(n=2)\n",
    "bigram_model.train(sample_texts)\n",
    "\n",
    "# Train trigram model\n",
    "trigram_model = NGramLanguageModel(n=3)\n",
    "trigram_model.train(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize most common n-grams\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bigrams\n",
    "top_bigrams = bigram_model.ngram_counts.most_common(15)\n",
    "bigram_labels = [' '.join(bg) for bg, _ in top_bigrams]\n",
    "bigram_counts = [count for _, count in top_bigrams]\n",
    "\n",
    "axes[0].barh(bigram_labels, bigram_counts, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Count', fontsize=12)\n",
    "axes[0].set_title('Top 15 Bigrams', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Trigrams\n",
    "top_trigrams = trigram_model.ngram_counts.most_common(15)\n",
    "trigram_labels = [' '.join(tg) for tg, _ in top_trigrams]\n",
    "trigram_counts = [count for _, count in top_trigrams]\n",
    "\n",
    "axes[1].barh(trigram_labels, trigram_counts, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Count', fontsize=12)\n",
    "axes[1].set_title('Top 15 Trigrams', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Text Generation with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "print(\"Text Generation:\\n\")\n",
    "\n",
    "print(\"Bigram model:\")\n",
    "for i in range(3):\n",
    "    generated = bigram_model.generate(max_length=15)\n",
    "    print(f\"  {i+1}. {generated}\")\n",
    "\n",
    "print(\"\\nTrigram model:\")\n",
    "for i in range(3):\n",
    "    generated = trigram_model.generate(max_length=15)\n",
    "    print(f\"  {i+1}. {generated}\")\n",
    "\n",
    "print(\"\\nGenerate with context:\")\n",
    "context = ['nlp', 'is']\n",
    "for i in range(3):\n",
    "    generated = bigram_model.generate(start_tokens=context, max_length=10)\n",
    "    print(f\"  {i+1}. {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation Metrics for Language Models\n",
    "\n",
    "### 6.1 Perplexity\n",
    "\n",
    "**Perplexity** measures how well a language model predicts text.\n",
    "\n",
    "$$\n",
    "PP(W) = P(w_1, ..., w_N)^{-1/N} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^N \\log P(w_i | w_1, ..., w_{i-1})\\right)\n",
    "$$\n",
    "\n",
    "**Intuition:**\n",
    "- Lower perplexity = better model\n",
    "- Perplexity of uniform random model = vocabulary size\n",
    "- Can be interpreted as \"average branching factor\"\n",
    "\n",
    "**Example:**\n",
    "- PP = 10 means the model is as uncertain as choosing from 10 equally likely words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity\n",
    "test_texts = [\n",
    "    \"nlp is a field of artificial intelligence\",\n",
    "    \"deep learning has revolutionized nlp\",\n",
    "    \"the transformer architecture was a breakthrough\"\n",
    "]\n",
    "\n",
    "print(\"Perplexity Comparison:\\n\")\n",
    "print(f\"{'Text':<50} {'Bigram PP':>15} {'Trigram PP':>15}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for text in test_texts:\n",
    "    bigram_pp = bigram_model.perplexity(text)\n",
    "    trigram_pp = trigram_model.perplexity(text)\n",
    "    print(f\"{text[:48]:<50} {bigram_pp:>15.2f} {trigram_pp:>15.2f}\")\n",
    "\n",
    "# Test on out-of-domain text\n",
    "ood_text = \"quantum computing uses qubits instead of bits\"\n",
    "print(f\"\\nOut-of-domain text: {ood_text}\")\n",
    "print(f\"  Bigram perplexity: {bigram_model.perplexity(ood_text):.2f}\")\n",
    "print(f\"  Trigram perplexity: {trigram_model.perplexity(ood_text):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Practical Exercise - Complete NLP Pipeline\n",
    "\n",
    "Let's build a complete preprocessing and analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPPipeline:\n",
    "    \"\"\"Complete NLP preprocessing and analysis pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.preprocessor = TextPreprocessor()\n",
    "        self.vocab = None\n",
    "        self.tfidf = TfidfVectorizer(max_features=100)\n",
    "    \n",
    "    def preprocess_corpus(self, texts):\n",
    "        \"\"\"Preprocess entire corpus.\"\"\"\n",
    "        cleaned_texts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Clean\n",
    "            clean = self.preprocessor.clean_text(text)\n",
    "            \n",
    "            # Tokenize\n",
    "            tokens = word_tokenize(clean)\n",
    "            \n",
    "            # Remove stopwords and lemmatize\n",
    "            tokens = self.preprocessor.remove_stopwords(tokens)\n",
    "            tokens = self.preprocessor.lemmatize(tokens)\n",
    "            \n",
    "            cleaned_texts.append(' '.join(tokens))\n",
    "        \n",
    "        return cleaned_texts\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        \"\"\"Build vocabulary from texts.\"\"\"\n",
    "        self.vocab = Vocabulary(min_freq=1)\n",
    "        self.vocab.build_vocab(texts)\n",
    "        return self.vocab\n",
    "    \n",
    "    def compute_tfidf(self, texts):\n",
    "        \"\"\"Compute TF-IDF representation.\"\"\"\n",
    "        return self.tfidf.fit_transform(texts)\n",
    "    \n",
    "    def analyze_corpus(self, texts):\n",
    "        \"\"\"Comprehensive corpus analysis.\"\"\"\n",
    "        print(\"Corpus Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Basic stats\n",
    "        num_docs = len(texts)\n",
    "        avg_length = np.mean([len(text.split()) for text in texts])\n",
    "        total_words = sum(len(text.split()) for text in texts)\n",
    "        \n",
    "        print(f\"Number of documents: {num_docs}\")\n",
    "        print(f\"Average document length: {avg_length:.2f} words\")\n",
    "        print(f\"Total words: {total_words}\")\n",
    "        \n",
    "        # Vocabulary stats\n",
    "        all_words = ' '.join(texts).split()\n",
    "        word_counts = Counter(all_words)\n",
    "        vocab_size = len(word_counts)\n",
    "        \n",
    "        print(f\"Unique words: {vocab_size}\")\n",
    "        print(f\"Type-token ratio: {vocab_size/total_words:.4f}\")\n",
    "        \n",
    "        # Most common words\n",
    "        print(f\"\\nTop 10 most common words:\")\n",
    "        for word, count in word_counts.most_common(10):\n",
    "            print(f\"  {word}: {count}\")\n",
    "        \n",
    "        return word_counts\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = NLPPipeline()\n",
    "\n",
    "# Preprocess\n",
    "cleaned_corpus = pipeline.preprocess_corpus(sample_texts)\n",
    "print(\"Preprocessed corpus:\")\n",
    "for i, text in enumerate(cleaned_corpus):\n",
    "    print(f\"{i+1}. {text}\")\n",
    "\n",
    "print()\n",
    "# Analyze\n",
    "word_counts = pipeline.analyze_corpus(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word frequencies\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "top_words = word_counts.most_common(20)\n",
    "words = [w for w, _ in top_words]\n",
    "counts = [c for _, c in top_words]\n",
    "\n",
    "bars = ax.barh(words, counts, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_xlabel('Frequency', fontsize=12)\n",
    "ax.set_title('Word Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "            str(count), ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Key Takeaways and Next Steps\n",
    "\n",
    "### 8.1 Summary\n",
    "\n",
    "Today we covered the **foundations of NLP**:\n",
    "\n",
    "✅ **Why NLP is challenging:** Ambiguity, context, variable length, discrete symbols  \n",
    "✅ **Text preprocessing:** Cleaning, normalization, stopword removal, stemming/lemmatization  \n",
    "✅ **Tokenization:** Word-level, subword (BPE), character-level  \n",
    "✅ **Vocabulary building:** Token-to-index mapping, special tokens, OOV handling  \n",
    "✅ **Text representation:** Bag of Words, TF-IDF  \n",
    "✅ **N-gram language models:** Probability estimation, text generation  \n",
    "✅ **Evaluation:** Perplexity as a measure of model quality  \n",
    "\n",
    "### 8.2 Limitations of Traditional NLP\n",
    "\n",
    "**Bag of Words / TF-IDF:**\n",
    "- No word order\n",
    "- No semantic similarity (\"king\" and \"monarch\" are unrelated)\n",
    "- High-dimensional, sparse vectors\n",
    "\n",
    "**N-gram models:**\n",
    "- Limited context (only last n-1 words)\n",
    "- Data sparsity (many n-grams never seen)\n",
    "- Cannot capture long-range dependencies\n",
    "\n",
    "### 8.3 What's Next?\n",
    "\n",
    "**Tomorrow (Day 12):** Word Embeddings\n",
    "- Dense, continuous representations\n",
    "- Semantic similarity captured\n",
    "- Word2Vec, GloVe algorithms\n",
    "\n",
    "**Coming up:**\n",
    "- RNNs for sequence modeling (Day 13)\n",
    "- LSTMs for long-term dependencies (Day 14)\n",
    "- Attention mechanisms (Day 16)\n",
    "- Transformers (Day 17)\n",
    "\n",
    "### 8.4 Mathematical Foundation for Phase 2\n",
    "\n",
    "**Key concepts you'll need:**\n",
    "- **Vector spaces:** Word embeddings live in high-dimensional space\n",
    "- **Similarity measures:** Cosine similarity, dot product\n",
    "- **Probability:** Language modeling, next word prediction\n",
    "- **Backpropagation through time:** For RNNs\n",
    "- **Matrix operations:** Attention is just weighted matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing **Day 11** and entering **Phase 2: NLP**!\n",
    "\n",
    "You've learned:\n",
    "\n",
    "✅ The unique challenges of natural language processing  \n",
    "✅ Complete text preprocessing pipeline  \n",
    "✅ Multiple tokenization strategies (word, subword, character)  \n",
    "✅ Vocabulary building and numericalization  \n",
    "✅ Text representations (BoW, TF-IDF)  \n",
    "✅ N-gram language models and text generation  \n",
    "✅ Perplexity as an evaluation metric  \n",
    "\n",
    "**Key Insight:** Traditional NLP methods have significant limitations (no semantics, limited context) that neural approaches will solve.\n",
    "\n",
    "**Time spent:** ~3-4 hours\n",
    "\n",
    "**Next:** Day 12 - Word Embeddings (Word2Vec, GloVe) - Dense semantic representations that revolutionized NLP!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
