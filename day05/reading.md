# Day 05: Neural Networks from First Principles (NumPy Implementation) - Further Reading

This day implements neural networks from scratch using NumPy, requiring deep understanding of linear algebra, calculus, and automatic differentiationâ€”the mathematical foundations of all deep learning frameworks.

## References

1. **3Blue1Brown - Essence of Linear Algebra**
   - [3Blue1Brown: Linear Algebra Series](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)
   - Visual explanation of vectors, matrices, transformations, and eigenvalues essential for understanding neural network mathematics.

2. **NumPy Broadcasting and Vectorization**
   - [NumPy Broadcasting Documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html)
   - Essential guide to efficient vectorized operations in NumPy for high-performance numerical computing.

3. **Automatic Differentiation Concepts**
   - [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)
   - Practical guide to automatic differentiation (reverse-mode) concepts with implementations and intuitions.

4. **Efficient Backpropagation in Deep Networks**
   - [LeCun et al.: Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
   - Classic paper on efficient backpropagation with practical tricks for numerical stability and computational efficiency.

5. **Building Neural Networks from Scratch**
   - [Neural Networks from Scratch in Python](https://nnfs.io/) (Free Online Book)
   - Step-by-step guide to implementing neural networks without frameworks, building deep mathematical understanding.

---

**Tip:** Start with reference #1 for mathematical foundation, then #2 for NumPy efficiency, and #3-4 for automatic differentiation concepts.
