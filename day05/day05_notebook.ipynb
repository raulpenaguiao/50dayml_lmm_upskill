{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: Building Neural Networks from First Principles (NumPy)\n",
    "\n",
    "**Time:** 4-5 hours (substantial implementation)\n",
    "\n",
    "**Mathematical Prerequisites:**\n",
    "- Matrix calculus (Jacobians, vector-Jacobian products)\n",
    "- Chain rule for compositions\n",
    "- Computational graphs and reverse-mode autodiff\n",
    "- Numerical methods and stability\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Today we implement a complete neural network library from scratch using only NumPy. This will deepen your understanding of:\n",
    "1. How automatic differentiation actually works\n",
    "2. Forward and backward propagation at a low level\n",
    "3. The mathematics behind gradient computation\n",
    "4. Why reverse-mode autodiff is O(n) not O(n²)\n",
    "\n",
    "By the end, you'll have a working NN library that achieves >90% accuracy on MNIST.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Mathematical Foundation\n",
    "\n",
    "### 1.1 Computational Graphs and the Chain Rule\n",
    "\n",
    "A neural network is a composition of functions:\n",
    "$$\n",
    "f(x; \\theta) = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "For a loss $L(f(x; \\theta), y)$, we need $\\frac{\\partial L}{\\partial \\theta}$ for gradient descent.\n",
    "\n",
    "**Chain Rule (Vector Calculus):**\n",
    "\n",
    "If $y = f(x)$ where $y \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, then:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\left(\\frac{\\partial y}{\\partial x}\\right)^T \\frac{\\partial L}{\\partial y}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial y}{\\partial x}$ is the Jacobian (m×n matrix).\n",
    "\n",
    "**Key Insight:** In reverse-mode autodiff, we propagate **vector-Jacobian products** (VJPs) backward. This is why backpropagation is O(n) not O(n²) - we never explicitly compute the full Jacobian.\n",
    "\n",
    "### 1.2 Forward and Backward Pass\n",
    "\n",
    "**Forward pass:** Compute outputs and cache intermediate values\n",
    "```python\n",
    "y = f(x)  # Store x for backward pass\n",
    "```\n",
    "\n",
    "**Backward pass:** Given $\\frac{\\partial L}{\\partial y}$ (gradient from next layer), compute:\n",
    "1. $\\frac{\\partial L}{\\partial x}$ to pass to previous layer\n",
    "2. $\\frac{\\partial L}{\\partial \\theta}$ to update parameters\n",
    "\n",
    "```python\n",
    "dL/dx = (dy/dx)^T @ (dL/dy)  # VJP\n",
    "dL/dθ = (dy/dθ)^T @ (dL/dy)  # Parameter gradient\n",
    "```\n",
    "\n",
    "### 1.3 Example: Linear Layer\n",
    "\n",
    "Forward: $y = Wx + b$ where $W \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$\n",
    "\n",
    "Gradients:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} x^T \\quad (m \\times n)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial y} \\quad (m \\times 1)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = W^T \\frac{\\partial L}{\\partial y} \\quad (n \\times 1)\n",
    "$$\n",
    "\n",
    "**For batched inputs** $X \\in \\mathbb{R}^{B \\times n}$:\n",
    "$$\n",
    "Y = XW^T + b\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{1}{B} \\left(\\frac{\\partial L}{\\partial Y}\\right)^T X\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{B} \\sum_{i=1}^B \\frac{\\partial L}{\\partial y_i}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} W\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# For comparison with PyTorch later\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Layer Abstraction\n",
    "\n",
    "We'll create a base `Layer` class that all layers inherit from. Each layer must implement:\n",
    "- `forward(x)`: Compute output and cache inputs\n",
    "- `backward(grad_output)`: Compute gradients w.r.t. inputs and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for all layers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = {}  # Parameters (weights, biases)\n",
    "        self.grads = {}   # Gradients of parameters\n",
    "        self.cache = {}   # Cache for backward pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass. Must be implemented by subclasses.\n",
    "        \n",
    "        Args:\n",
    "            x: Input array of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Output array of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Backward pass. Must be implemented by subclasses.\n",
    "        \n",
    "        Args:\n",
    "            grad_output: Gradient w.r.t. output (dL/dy), shape (batch_size, output_dim)\n",
    "        \n",
    "        Returns:\n",
    "            grad_input: Gradient w.r.t. input (dL/dx), shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return dictionary of parameters.\"\"\"\n",
    "        return self.params\n",
    "    \n",
    "    def get_grads(self):\n",
    "        \"\"\"Return dictionary of gradients.\"\"\"\n",
    "        return self.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear (Fully Connected) Layer\n",
    "\n",
    "**Forward:** $y = xW^T + b$\n",
    "\n",
    "**Backward:**\n",
    "- $\\frac{\\partial L}{\\partial W} = (\\frac{\\partial L}{\\partial y})^T x$\n",
    "- $\\frac{\\partial L}{\\partial b} = \\sum_{batch} \\frac{\\partial L}{\\partial y}$\n",
    "- $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \"\"\"Fully connected layer: y = xW^T + b\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features\n",
    "        output_dim: Number of output features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Xavier initialization: Var(W) = 2/(fan_in + fan_out)\n",
    "        limit = np.sqrt(6 / (input_dim + output_dim))\n",
    "        self.params['W'] = np.random.uniform(-limit, limit, (output_dim, input_dim))\n",
    "        self.params['b'] = np.zeros(output_dim)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
    "        self.grads['b'] = np.zeros_like(self.params['b'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, input_dim)\n",
    "        Returns:\n",
    "            y: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        self.cache['x'] = x\n",
    "        return x @ self.params['W'].T + self.params['b']\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grad_output: dL/dy, shape (batch_size, output_dim)\n",
    "        Returns:\n",
    "            grad_input: dL/dx, shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        x = self.cache['x']\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Gradient w.r.t. parameters\n",
    "        self.grads['W'] = (grad_output.T @ x) / batch_size\n",
    "        self.grads['b'] = np.mean(grad_output, axis=0)\n",
    "        \n",
    "        # Gradient w.r.t. input\n",
    "        grad_input = grad_output @ self.params['W']\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ReLU Activation\n",
    "\n",
    "**Forward:** $y = \\max(0, x)$\n",
    "\n",
    "**Backward:** \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\odot \\mathbb{1}_{x > 0}\n",
    "$$\n",
    "\n",
    "where $\\odot$ is element-wise multiplication.\n",
    "\n",
    "**Note on x=0:** Technically ReLU is not differentiable at 0. In practice, we set the gradient to 0 (or 1) at x=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"ReLU activation: y = max(0, x)\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        x = self.cache['x']\n",
    "        # Gradient is 1 where x > 0, else 0\n",
    "        grad_input = grad_output * (x > 0)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Sigmoid Activation\n",
    "\n",
    "**Forward:** $y = \\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "**Backward:** \n",
    "$$\n",
    "\\frac{\\partial \\sigma}{\\partial x} = \\sigma(x)(1 - \\sigma(x)) = y(1-y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\odot y(1-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    \"\"\"Sigmoid activation: y = 1 / (1 + exp(-x))\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip for numerical stability\n",
    "        self.cache['y'] = y\n",
    "        return y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        y = self.cache['y']\n",
    "        # dy/dx = y(1-y)\n",
    "        grad_input = grad_output * y * (1 - y)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Softmax with Cross-Entropy Loss\n",
    "\n",
    "**Why combine them?** Numerical stability. Computing softmax then log then cross-entropy can cause overflow/underflow.\n",
    "\n",
    "**Softmax:**\n",
    "$$\n",
    "\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "$$\n",
    "\n",
    "**Cross-Entropy Loss:**\n",
    "$$\n",
    "L = -\\log(\\text{softmax}(x_{\\text{true class}}))\n",
    "$$\n",
    "\n",
    "**Log-Sum-Exp Trick** for numerical stability:\n",
    "$$\n",
    "\\log\\sum_i e^{x_i} = a + \\log\\sum_i e^{x_i - a}\n",
    "$$\n",
    "where $a = \\max_i x_i$.\n",
    "\n",
    "**Gradient:**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\text{softmax}(x_i) - \\mathbb{1}_{i = \\text{true class}}\n",
    "$$\n",
    "\n",
    "This is a beautiful result: the gradient is just the difference between predicted probability and true label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Layer):\n",
    "    \"\"\"Combined Softmax + Cross-Entropy for numerical stability.\n",
    "    \n",
    "    Forward: Computes loss\n",
    "    Backward: Returns gradient w.r.t. logits\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (batch_size, num_classes)\n",
    "            labels: (batch_size,) integer labels\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        batch_size = logits.shape[0]\n",
    "        \n",
    "        # Log-sum-exp trick for numerical stability\n",
    "        logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "        logits_shifted = logits - logits_max\n",
    "        exp_logits = np.exp(logits_shifted)\n",
    "        sum_exp = np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        log_probs = logits_shifted - np.log(sum_exp)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        correct_log_probs = log_probs[np.arange(batch_size), labels]\n",
    "        loss = -np.mean(correct_log_probs)\n",
    "        \n",
    "        # Cache for backward\n",
    "        self.cache['probs'] = exp_logits / sum_exp\n",
    "        self.cache['labels'] = labels\n",
    "        self.cache['batch_size'] = batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            grad_logits: (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        probs = self.cache['probs']\n",
    "        labels = self.cache['labels']\n",
    "        batch_size = self.cache['batch_size']\n",
    "        \n",
    "        # Gradient: softmax(x) - one_hot(y)\n",
    "        grad_logits = probs.copy()\n",
    "        grad_logits[np.arange(batch_size), labels] -= 1\n",
    "        grad_logits /= batch_size\n",
    "        \n",
    "        return grad_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sequential Model Container\n",
    "\n",
    "We need a container to stack layers sequentially, similar to `nn.Sequential` in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Sequential container for layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, *layers):\n",
    "        self.layers = list(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all layers.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Backward pass through all layers in reverse.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "        return grad_output\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"Return all parameters and gradients from all layers.\"\"\"\n",
    "        params = []\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            params.append(layer.get_params())\n",
    "            grads.append(layer.get_grads())\n",
    "        return params, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Optimizer\n",
    "\n",
    "Implement SGD with momentum (from Day 4 theory).\n",
    "\n",
    "**Update rule:**\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1-\\beta)\\nabla L(\\theta_{t-1})\n",
    "$$\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\alpha v_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"SGD optimizer with momentum.\n",
    "    \n",
    "    Args:\n",
    "        lr: Learning rate\n",
    "        momentum: Momentum coefficient (0 = no momentum)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocities = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        \"\"\"Update parameters.\n",
    "        \n",
    "        Args:\n",
    "            params: List of parameter dictionaries\n",
    "            grads: List of gradient dictionaries\n",
    "        \"\"\"\n",
    "        # Initialize velocities on first call\n",
    "        if self.velocities is None:\n",
    "            self.velocities = []\n",
    "            for param_dict in params:\n",
    "                velocity_dict = {}\n",
    "                for key, value in param_dict.items():\n",
    "                    velocity_dict[key] = np.zeros_like(value)\n",
    "                self.velocities.append(velocity_dict)\n",
    "        \n",
    "        # Update parameters\n",
    "        for param_dict, grad_dict, velocity_dict in zip(params, grads, self.velocities):\n",
    "            for key in param_dict.keys():\n",
    "                # v = beta * v + (1-beta) * grad\n",
    "                velocity_dict[key] = self.momentum * velocity_dict[key] + (1 - self.momentum) * grad_dict[key]\n",
    "                # theta = theta - lr * v\n",
    "                param_dict[key] -= self.lr * velocity_dict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Numerical Gradient Checking\n",
    "\n",
    "Before training, we **must** verify our analytical gradients are correct.\n",
    "\n",
    "**Finite Difference Approximation:**\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "This is O(ε²) accurate (two-sided difference).\n",
    "\n",
    "**Relative Error:**\n",
    "$$\n",
    "\\text{error} = \\frac{||\\nabla_{\\text{analytical}} - \\nabla_{\\text{numerical}}||_2}{||\\nabla_{\\text{analytical}}||_2 + ||\\nabla_{\\text{numerical}}||_2}\n",
    "$$\n",
    "\n",
    "**Good threshold:** error < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_check(layer, x, grad_output, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Check analytical gradients against numerical gradients.\n",
    "    \n",
    "    Args:\n",
    "        layer: Layer to check\n",
    "        x: Input data\n",
    "        grad_output: Gradient from next layer\n",
    "        epsilon: Finite difference step size\n",
    "    \"\"\"\n",
    "    # Forward and backward pass\n",
    "    output = layer.forward(x)\n",
    "    analytical_grad = layer.backward(grad_output)\n",
    "    \n",
    "    # Numerical gradient w.r.t. input\n",
    "    numerical_grad = np.zeros_like(x)\n",
    "    \n",
    "    # Compute numerical gradient element by element\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    count = 0\n",
    "    while not it.finished and count < 10:  # Only check first 10 elements for speed\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        old_value = x[ix]\n",
    "        \n",
    "        # f(x + epsilon)\n",
    "        x[ix] = old_value + epsilon\n",
    "        out_plus = layer.forward(x)\n",
    "        loss_plus = np.sum(out_plus * grad_output)\n",
    "        \n",
    "        # f(x - epsilon)\n",
    "        x[ix] = old_value - epsilon\n",
    "        out_minus = layer.forward(x)\n",
    "        loss_minus = np.sum(out_minus * grad_output)\n",
    "        \n",
    "        # Numerical gradient\n",
    "        numerical_grad[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Restore\n",
    "        x[ix] = old_value\n",
    "        it.iternext()\n",
    "        count += 1\n",
    "    \n",
    "    # Compute relative error\n",
    "    numerator = np.linalg.norm(analytical_grad - numerical_grad)\n",
    "    denominator = np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad)\n",
    "    relative_error = numerator / (denominator + 1e-8)\n",
    "    \n",
    "    print(f\"Relative error: {relative_error:.2e}\")\n",
    "    if relative_error < 1e-7:\n",
    "        print(\"✓ Gradient check PASSED\")\n",
    "    elif relative_error < 1e-5:\n",
    "        print(\"⚠ Gradient check ACCEPTABLE (might be OK for some layers)\")\n",
    "    else:\n",
    "        print(\"✗ Gradient check FAILED\")\n",
    "    \n",
    "    return relative_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gradient Checking on Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Linear layer gradient...\")\n",
    "linear = Linear(5, 3)\n",
    "x_test = np.random.randn(2, 5)\n",
    "grad_out_test = np.random.randn(2, 3)\n",
    "numerical_gradient_check(linear, x_test, grad_out_test)\n",
    "\n",
    "print(\"\\nTesting ReLU gradient...\")\n",
    "relu = ReLU()\n",
    "x_test = np.random.randn(2, 5)\n",
    "grad_out_test = np.random.randn(2, 5)\n",
    "numerical_gradient_check(relu, x_test, grad_out_test)\n",
    "\n",
    "print(\"\\nTesting Sigmoid gradient...\")\n",
    "sigmoid = Sigmoid()\n",
    "x_test = np.random.randn(2, 5)\n",
    "grad_out_test = np.random.randn(2, 5)\n",
    "numerical_gradient_check(sigmoid, x_test, grad_out_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X, y = mnist['data'], mnist['target'].astype(int)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "y = np.array(y, dtype=np.int64)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Visualize a few examples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Build and Train Neural Network\n",
    "\n",
    "We'll build a simple 3-layer MLP:\n",
    "- Input: 784 (28×28 pixels)\n",
    "- Hidden 1: 128 neurons with ReLU\n",
    "- Hidden 2: 64 neurons with ReLU\n",
    "- Output: 10 classes\n",
    "\n",
    "**Goal:** Achieve >90% test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = Sequential(\n",
    "    Linear(784, 128),\n",
    "    ReLU(),\n",
    "    Linear(128, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 10)\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "loss_fn = SoftmaxCrossEntropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = SGD(lr=0.1, momentum=0.9)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(\"784 -> [Linear] -> 128 -> [ReLU] -> 128 -> [Linear] -> 64 -> [ReLU] -> 64 -> [Linear] -> 10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, X, y, loss_fn, optimizer, batch_size=128):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Get batch\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X_shuffled[start_idx:end_idx]\n",
    "        y_batch = y_shuffled[start_idx:end_idx]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model.forward(X_batch)\n",
    "        loss = loss_fn.forward(logits, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        grad_logits = loss_fn.backward()\n",
    "        model.backward(grad_logits)\n",
    "        \n",
    "        # Update parameters\n",
    "        params, grads = model.get_params_and_grads()\n",
    "        optimizer.step(params, grads)\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        correct += np.sum(preds == y_batch)\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = correct / (num_batches * batch_size)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, X, y, loss_fn, batch_size=128):\n",
    "    \"\"\"Evaluate on test set.\"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X[start_idx:end_idx]\n",
    "        y_batch = y[start_idx:end_idx]\n",
    "        \n",
    "        logits = model.forward(X_batch)\n",
    "        loss = loss_fn.forward(logits, y_batch)\n",
    "        \n",
    "        total_loss += loss\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds.extend(preds)\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = accuracy_score(y[:len(all_preds)], all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, X_train, y_train, loss_fn, optimizer, batch_size=128)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc, _ = evaluate(model, X_test, y_test, loss_fn, batch_size=128)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Final test accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax1.plot(test_losses, label='Test Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss (NumPy Implementation)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
    "ax2.plot(test_accs, label='Test Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy (NumPy Implementation)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on full test set\n",
    "_, _, test_preds = evaluate(model, X_test, y_test, loss_fn, batch_size=128)\n",
    "y_test_subset = y_test[:len(test_preds)]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_subset, test_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix (NumPy Implementation)')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i in range(10):\n",
    "    class_acc = cm[i, i] / np.sum(cm[i, :])\n",
    "    print(f\"Digit {i}: {class_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Comparison with PyTorch\n",
    "\n",
    "Let's compare our NumPy implementation with PyTorch in terms of:\n",
    "1. Performance (accuracy)\n",
    "2. Training speed\n",
    "3. Code complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train)\n",
    "y_train_torch = torch.LongTensor(y_train)\n",
    "X_test_torch = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "test_dataset = TensorDataset(X_test_torch, y_test_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# Define PyTorch model\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "pytorch_model = PyTorchMLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_torch = optim.SGD(pytorch_model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "print(\"PyTorch model architecture:\")\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PyTorch model\n",
    "print(\"\\nTraining PyTorch model...\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "pytorch_train_losses = []\n",
    "pytorch_test_losses = []\n",
    "pytorch_train_accs = []\n",
    "pytorch_test_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    pytorch_model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer_torch.zero_grad()\n",
    "        outputs = pytorch_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_torch.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    pytorch_train_losses.append(train_loss / len(train_loader))\n",
    "    pytorch_train_accs.append(correct / total)\n",
    "    \n",
    "    # Testing\n",
    "    pytorch_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = pytorch_model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    pytorch_test_losses.append(test_loss / len(test_loader))\n",
    "    pytorch_test_accs.append(correct / total)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {pytorch_train_losses[-1]:.4f}, Train Acc: {pytorch_train_accs[-1]:.4f} | \"\n",
    "          f\"Test Loss: {pytorch_test_losses[-1]:.4f}, Test Acc: {pytorch_test_accs[-1]:.4f}\")\n",
    "\n",
    "pytorch_time = time.time() - start_time\n",
    "print(f\"\\nPyTorch training completed in {pytorch_time:.2f} seconds\")\n",
    "print(f\"Final test accuracy: {pytorch_test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0, 0].plot(train_losses, label='NumPy Train', marker='o', linestyle='--')\n",
    "axes[0, 0].plot(pytorch_train_losses, label='PyTorch Train', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(test_losses, label='NumPy Test', marker='o', linestyle='--')\n",
    "axes[0, 1].plot(pytorch_test_losses, label='PyTorch Test', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Test Loss Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1, 0].plot(train_accs, label='NumPy Train', marker='o', linestyle='--')\n",
    "axes[1, 0].plot(pytorch_train_accs, label='PyTorch Train', marker='s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('Training Accuracy Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(test_accs, label='NumPy Test', marker='o', linestyle='--')\n",
    "axes[1, 1].plot(pytorch_test_accs, label='PyTorch Test', marker='s')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_title('Test Accuracy Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<30} {'NumPy':>15} {'PyTorch':>15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Final Test Accuracy':<30} {test_accs[-1]:>15.4f} {pytorch_test_accs[-1]:>15.4f}\")\n",
    "print(f\"{'Final Test Loss':<30} {test_losses[-1]:>15.4f} {pytorch_test_losses[-1]:>15.4f}\")\n",
    "print(f\"{'Training Time (seconds)':<30} {end_time - start_time:>15.2f} {pytorch_time:>15.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSpeed ratio (NumPy/PyTorch): {(end_time - start_time) / pytorch_time:.2f}x\")\n",
    "print(f\"Accuracy difference: {abs(test_accs[-1] - pytorch_test_accs[-1]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Takeaways and Reflections\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Automatic Differentiation:**\n",
    "   - Reverse-mode autodiff computes gradients in O(n) time by propagating vector-Jacobian products backward\n",
    "   - We never need to explicitly compute full Jacobian matrices\n",
    "   - Each layer caches its inputs during forward pass for use in backward pass\n",
    "\n",
    "2. **Layer Implementation:**\n",
    "   - Linear layer: Simple matrix multiplication with careful handling of batched inputs\n",
    "   - Activation functions: Element-wise operations with straightforward gradients\n",
    "   - Softmax + Cross-Entropy: Combined for numerical stability using log-sum-exp trick\n",
    "\n",
    "3. **Numerical Gradient Checking:**\n",
    "   - Essential for validating analytical gradients\n",
    "   - Finite differences provide O(ε²) accurate approximation\n",
    "   - Relative error < 1e-7 indicates correct implementation\n",
    "\n",
    "4. **Performance:**\n",
    "   - NumPy implementation is slower than PyTorch (no GPU acceleration, less optimized)\n",
    "   - But achieves comparable accuracy, validating our implementation\n",
    "   - PyTorch provides highly optimized BLAS operations and GPU support\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Understanding neural networks at this level allows you to:\n",
    "- Debug training issues (gradient vanishing/explosion, numerical instability)\n",
    "- Implement custom layers and operations\n",
    "- Understand what PyTorch/TensorFlow do under the hood\n",
    "- Optimize performance by knowing computational bottlenecks\n",
    "- Design novel architectures with confidence\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Optional Extensions:**\n",
    "1. Implement Batch Normalization layer\n",
    "2. Add Dropout layer\n",
    "3. Implement Convolutional layer using im2col\n",
    "4. Add Adam optimizer\n",
    "5. Implement learning rate scheduling\n",
    "6. Profile code to find bottlenecks\n",
    "\n",
    "**Moving Forward:**\n",
    "- Day 6: Transfer Learning with pretrained models\n",
    "- Day 7: Comprehensive model evaluation\n",
    "- You now have the foundation to understand any neural network architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: (Optional) Advanced Implementation - Batch Normalization\n",
    "\n",
    "If you have extra time, try implementing Batch Normalization from scratch.\n",
    "\n",
    "**Forward:**\n",
    "$$\n",
    "\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "$$\n",
    "$$\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "$$\n",
    "\n",
    "**Backward:** (Left as an exercise - derive the gradients!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    \"\"\"Batch Normalization layer (1D).\n",
    "    \n",
    "    Args:\n",
    "        num_features: Number of features (channels)\n",
    "        eps: Small constant for numerical stability\n",
    "        momentum: Momentum for running mean/var\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.params['gamma'] = np.ones(num_features)\n",
    "        self.params['beta'] = np.zeros(num_features)\n",
    "        \n",
    "        # Running statistics (for inference)\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.grads['gamma'] = np.zeros_like(self.params['gamma'])\n",
    "        self.grads['beta'] = np.zeros_like(self.params['beta'])\n",
    "        \n",
    "        self.training = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, num_features)\n",
    "        Returns:\n",
    "            y: (batch_size, num_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: Compute batch statistics, normalize, scale and shift\n",
    "        raise NotImplementedError(\"Exercise: Implement BatchNorm forward pass\")\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grad_output: dL/dy, shape (batch_size, num_features)\n",
    "        Returns:\n",
    "            grad_input: dL/dx, shape (batch_size, num_features)\n",
    "        \"\"\"\n",
    "        # TODO: Implement backward pass\n",
    "        # Hint: This is challenging! Derive gradients w.r.t. x, gamma, beta\n",
    "        raise NotImplementedError(\"Exercise: Implement BatchNorm backward pass\")\n",
    "\n",
    "# TODO: Test BatchNorm implementation\n",
    "# TODO: Add BatchNorm to the model and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've implemented a complete neural network library from scratch using only NumPy. You now understand:\n",
    "\n",
    "✅ Computational graphs and reverse-mode autodiff  \n",
    "✅ Forward and backward propagation  \n",
    "✅ Layer implementations (Linear, ReLU, Sigmoid, Softmax)  \n",
    "✅ Numerical gradient checking  \n",
    "✅ Training loops and optimization  \n",
    "✅ How PyTorch works under the hood  \n",
    "\n",
    "**Achievement:** >90% accuracy on MNIST with your own implementation!\n",
    "\n",
    "**Time spent:** ~4-5 hours (if you completed all sections)\n",
    "\n",
    "**Next:** Day 6 - Transfer Learning with Pretrained Models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
