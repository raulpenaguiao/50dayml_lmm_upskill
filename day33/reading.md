# Day 33: Advanced Fine-Tuning Techniques - Further Reading

This day covers parameter-efficient fine-tuning methods like LoRA (Low-Rank Adaptation) and Adapter layers, enabling adaptation of large models with minimal additional parameters.

## References

1. **LoRA: Low-Rank Adaptation of Large Language Models**
   - [Hu et al.: LoRA Paper](https://arxiv.org/abs/2106.09685)
   - Seminal paper introducing low-rank adaptation for efficient fine-tuning of large models.

2. **PEFT Library: Parameter-Efficient Fine-Tuning**
   - [HF PEFT Documentation](https://huggingface.co/docs/peft/)
   - Official library implementing LoRA, adapters, prompt tuning, and other parameter-efficient methods.

3. **Adapter Layers in Transformer Models**
   - [Houlsby et al.: Parameter-Efficient Adaptation](https://arxiv.org/abs/1902.00751)
   - Original paper introducing adapter layers for parameter-efficient transfer learning.

4. **Prefix Tuning for Prompt Learning**
   - [Li & Liang: Prefix Tuning](https://arxiv.org/abs/2101.00297)
   - Paper on prepending learnable vectors to achieve few-shot learning with frozen models.

5. **Comparing Parameter-Efficient Methods**
   - [He et al.: Effectiveness of Task-Agnostic PEFT](https://arxiv.org/abs/2301.12031)
   - Empirical comparison of different PEFT methods showing tradeoffs.

---

**Tip:** Start with reference #2 for practical usage, #1 for LoRA theory, #3 for adapters, and #5 for comparing methods.
