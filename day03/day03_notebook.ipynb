{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Day 03: Advanced CNNs and Data Augmentation\n",
    "\n",
    "**Goal:** Learn advanced CNN techniques including data augmentation, batch normalization, and learning rate scheduling.\n",
    "\n",
    "**Mathematical Focus:** Batch normalization as affine transformation, dropout as stochastic regularization, LR schedules from optimization theory.\n",
    "\n",
    "**Time estimate:** 3-4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "## Theory: Why These Techniques Matter\n",
    "\n",
    "### Data Augmentation\n",
    "- **Mathematical view:** Expanding training distribution by applying label-preserving transformations\n",
    "- **Effect:** Reduces overfitting by increasing effective dataset size\n",
    "- **Transformations:** Rotations, flips, crops preserve class labels\n",
    "\n",
    "### Batch Normalization\n",
    "- **Formula:** $\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$, then $y = \\gamma\\hat{x} + \\beta$\n",
    "- **Effect:** Normalizes layer inputs, stabilizes gradient flow\n",
    "- **Parameters:** $\\gamma$ (scale) and $\\beta$ (shift) are learned\n",
    "- **Benefit:** Smoother loss landscape, faster training\n",
    "\n",
    "### Dropout\n",
    "- **Mathematical view:** Stochastic regularization by randomly dropping neurons\n",
    "- **Effect:** Prevents co-adaptation of features, acts as ensemble\n",
    "- **Training:** Drop with probability $p$, scale by $1/(1-p)$\n",
    "- **Testing:** Use all neurons (scaling already handled)\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "- **Motivation:** Large LR early (fast convergence), small LR late (fine-tuning)\n",
    "- **StepLR:** Multiply LR by $\\gamma$ every $n$ steps\n",
    "- **ReduceLROnPlateau:** Adaptive reduction when validation plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Data Augmentation\n",
    "\n",
    "We'll create two datasets:\n",
    "1. **Standard:** Basic normalization only\n",
    "2. **Augmented:** Rotations, translations, affine transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-aug",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard transform (baseline from Day 2)\n",
    "transform_standard = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Augmented transform\n",
    "transform_augmented = transforms.Compose([\n",
    "    transforms.RandomRotation(15),              # Rotate ¬±15 degrees\n",
    "    transforms.RandomAffine(                    # Affine transformations\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),                   # Shift ¬±10%\n",
    "        scale=(0.9, 1.1)                        # Scale 90-110%\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset_standard = datasets.MNIST(root='./data', train=True, \n",
    "                                       download=True, transform=transform_standard)\n",
    "train_dataset_augmented = datasets.MNIST(root='./data', train=True,\n",
    "                                         download=True, transform=transform_augmented)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False,\n",
    "                             download=True, transform=transform_standard)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader_standard = DataLoader(train_dataset_standard, batch_size=64, shuffle=True)\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset_standard)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-aug",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation effects\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "# Get one image\n",
    "original_img, label = train_dataset_standard[0]\n",
    "\n",
    "# Show original\n",
    "axes[0, 0].imshow(original_img.squeeze(), cmap='gray')\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show 4 augmented versions\n",
    "for i in range(1, 5):\n",
    "    aug_img, _ = train_dataset_augmented[0]  # Same index, different transform\n",
    "    axes[0, i].imshow(aug_img.squeeze(), cmap='gray')\n",
    "    axes[0, i].set_title(f'Augmented {i}')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Show another digit\n",
    "original_img2, label2 = train_dataset_standard[1]\n",
    "axes[1, 0].imshow(original_img2.squeeze(), cmap='gray')\n",
    "axes[1, 0].set_title('Original')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "for i in range(1, 5):\n",
    "    aug_img2, _ = train_dataset_augmented[1]\n",
    "    axes[1, i].imshow(aug_img2.squeeze(), cmap='gray')\n",
    "    axes[1, i].set_title(f'Augmented {i}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Each time the augmented dataset is accessed, different random transforms are applied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "## 2. CNN with Batch Normalization and Dropout\n",
    "\n",
    "We'll build an improved CNN with:\n",
    "- Batch normalization after conv layers\n",
    "- Dropout for regularization\n",
    "- More capacity than Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Batch Normalization and Dropout\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1: 1 -> 32 channels, 3x3 kernel\n",
    "    - BatchNorm2d\n",
    "    - ReLU + MaxPool\n",
    "    - Conv2: 32 -> 64 channels, 3x3 kernel\n",
    "    - BatchNorm2d\n",
    "    - ReLU + MaxPool\n",
    "    - Flatten\n",
    "    - FC1: 1600 -> 128\n",
    "    - Dropout(0.5)\n",
    "    - FC2: 128 -> 10\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Normalize 32 channels\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Normalize 64 channels\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # After 2 pooling: 28 -> 14 -> 7\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.conv1(x)           # [N, 1, 28, 28] -> [N, 32, 28, 28]\n",
    "        x = self.bn1(x)             # Normalize\n",
    "        x = F.relu(x)               # Activation\n",
    "        x = self.pool(x)            # [N, 32, 28, 28] -> [N, 32, 14, 14]\n",
    "        \n",
    "        # Conv block 2\n",
    "        x = self.conv2(x)           # [N, 32, 14, 14] -> [N, 64, 14, 14]\n",
    "        x = self.bn2(x)             # Normalize\n",
    "        x = F.relu(x)               # Activation\n",
    "        x = self.pool(x)            # [N, 64, 14, 14] -> [N, 64, 7, 7]\n",
    "        \n",
    "        # Flatten and FC layers\n",
    "        x = x.view(-1, 64 * 7 * 7)  # [N, 64, 7, 7] -> [N, 3136]\n",
    "        x = F.relu(self.fc1(x))     # [N, 3136] -> [N, 128]\n",
    "        x = self.dropout(x)         # Dropout during training\n",
    "        x = self.fc2(x)             # [N, 128] -> [N, 10]\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and count parameters\n",
    "model = ImprovedCNN(dropout_rate=0.5).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "## 3. Training with Learning Rate Scheduling\n",
    "\n",
    "We'll implement:\n",
    "1. Model checkpointing (save best model)\n",
    "2. Learning rate scheduling (StepLR)\n",
    "3. Training with metrics tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=10, lr=0.001, use_scheduler=True):\n",
    "    \"\"\"\n",
    "    Train model with learning rate scheduling and checkpointing\n",
    "    \n",
    "    Returns:\n",
    "        history: dict with training metrics\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning rate scheduler: reduce LR by 0.1 every 3 epochs\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5) if use_scheduler else None\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100 * correct / total\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.2f}%, '\n",
    "              f'Test Acc: {test_acc:.2f}%, '\n",
    "              f'LR: {current_lr:.6f}')\n",
    "    \n",
    "    print(f'\\nBest Test Accuracy: {best_acc:.2f}%')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "## 4. Experiment: Standard vs Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training with STANDARD data (no augmentation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_standard = ImprovedCNN(dropout_rate=0.5).to(device)\n",
    "history_standard = train_model(model_standard, train_loader_standard, \n",
    "                               test_loader, num_epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exp2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training with AUGMENTED data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model_augmented = ImprovedCNN(dropout_rate=0.5).to(device)\n",
    "history_augmented = train_model(model_augmented, train_loader_augmented,\n",
    "                               test_loader, num_epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "## 5. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs = range(1, len(history_standard['train_loss']) + 1)\n",
    "\n",
    "# Training loss\n",
    "axes[0, 0].plot(epochs, history_standard['train_loss'], 'b-', label='Standard', marker='o')\n",
    "axes[0, 0].plot(epochs, history_augmented['train_loss'], 'r-', label='Augmented', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Training Loss')\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Training accuracy\n",
    "axes[0, 1].plot(epochs, history_standard['train_acc'], 'b-', label='Standard', marker='o')\n",
    "axes[0, 1].plot(epochs, history_augmented['train_acc'], 'r-', label='Augmented', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Training Accuracy (%)')\n",
    "axes[0, 1].set_title('Training Accuracy Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Test accuracy\n",
    "axes[1, 0].plot(epochs, history_standard['test_acc'], 'b-', label='Standard', marker='o')\n",
    "axes[1, 0].plot(epochs, history_augmented['test_acc'], 'r-', label='Augmented', marker='s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Test Accuracy (%)')\n",
    "axes[1, 0].set_title('Test Accuracy Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Learning rate schedule\n",
    "axes[1, 1].plot(epochs, history_standard['learning_rates'], 'g-', marker='o')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule (StepLR)')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Standard Data - Final Test Acc: {history_standard['test_acc'][-1]:.2f}%\")\n",
    "print(f\"Augmented Data - Final Test Acc: {history_augmented['test_acc'][-1]:.2f}%\")\n",
    "print(f\"\\nImprovement from augmentation: {history_augmented['test_acc'][-1] - history_standard['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "## 6. Experiment: Effect of Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dropout-exp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different dropout rates\n",
    "dropout_rates = [0.0, 0.3, 0.5, 0.7]\n",
    "dropout_results = {}\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with Dropout Rate: {dropout_rate}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = ImprovedCNN(dropout_rate=dropout_rate).to(device)\n",
    "    history = train_model(model, train_loader_standard, test_loader, \n",
    "                         num_epochs=5, lr=0.001)\n",
    "    dropout_results[dropout_rate] = history\n",
    "\n",
    "# Plot dropout comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for dropout_rate in dropout_rates:\n",
    "    plt.plot(dropout_results[dropout_rate]['train_acc'], \n",
    "             label=f'Dropout={dropout_rate}', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy (%)')\n",
    "plt.title('Effect of Dropout on Training Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for dropout_rate in dropout_rates:\n",
    "    plt.plot(dropout_results[dropout_rate]['test_acc'],\n",
    "             label=f'Dropout={dropout_rate}', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Effect of Dropout on Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal Test Accuracies:\")\n",
    "for dropout_rate in dropout_rates:\n",
    "    final_acc = dropout_results[dropout_rate]['test_acc'][-1]\n",
    "    print(f\"Dropout {dropout_rate}: {final_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "## 7. Model Checkpointing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model (architecture + weights)\n",
    "torch.save({\n",
    "    'model_state_dict': model_augmented.state_dict(),\n",
    "    'architecture': 'ImprovedCNN',\n",
    "    'dropout_rate': 0.5,\n",
    "    'final_test_acc': history_augmented['test_acc'][-1]\n",
    "}, 'checkpoint_day03.pth')\n",
    "\n",
    "print(\"Model checkpoint saved to 'checkpoint_day03.pth'\")\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('checkpoint_day03.pth')\n",
    "loaded_model = ImprovedCNN(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Saved test accuracy: {checkpoint['final_test_acc']:.2f}%\")\n",
    "\n",
    "# Verify loaded model\n",
    "loaded_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = loaded_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Verified accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### Data Augmentation\n",
    "- ‚úÖ Increases effective dataset size\n",
    "- ‚úÖ Reduces overfitting\n",
    "- ‚úÖ Improves generalization\n",
    "- ‚ö†Ô∏è May slightly slow training (more computation)\n",
    "- ‚ö†Ô∏è Can hurt performance if transforms are too aggressive\n",
    "\n",
    "### Batch Normalization\n",
    "- ‚úÖ Stabilizes training (smoother loss landscape)\n",
    "- ‚úÖ Allows higher learning rates\n",
    "- ‚úÖ Acts as mild regularization\n",
    "- ‚úÖ Reduces internal covariate shift\n",
    "- ‚ö†Ô∏è Different behavior in train vs test mode\n",
    "\n",
    "### Dropout\n",
    "- ‚úÖ Strong regularization technique\n",
    "- ‚úÖ Prevents overfitting\n",
    "- ‚úÖ Acts as ensemble of models\n",
    "- ‚ö†Ô∏è Too high dropout (>0.7) hurts training\n",
    "- ‚ö†Ô∏è Must disable during inference\n",
    "\n",
    "### Learning Rate Scheduling\n",
    "- ‚úÖ Improves final convergence\n",
    "- ‚úÖ Large LR early = fast training\n",
    "- ‚úÖ Small LR late = fine-tuning\n",
    "- üìä StepLR is simple and effective\n",
    "- üìä ReduceLROnPlateau is adaptive to validation\n",
    "\n",
    "### Best Practices Learned\n",
    "1. Always use data augmentation for image tasks\n",
    "2. Add batch norm after conv layers\n",
    "3. Use dropout (0.5) before final FC layers\n",
    "4. Start with higher LR, decay over time\n",
    "5. Save best model based on validation accuracy\n",
    "6. Monitor both training and validation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "1. **Try ReduceLROnPlateau scheduler** instead of StepLR\n",
    "2. **Experiment with batch norm placement**: Before or after ReLU?\n",
    "3. **Add more augmentation**: ColorJitter, RandomErasing\n",
    "4. **Compare BatchNorm vs LayerNorm vs GroupNorm**\n",
    "5. **Implement early stopping**: Stop if validation doesn't improve for N epochs\n",
    "6. **Try different dropout locations**: After each conv layer vs only FC layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
