# Day 03: Advanced CNNs and Data Augmentation - Further Reading

This day covers techniques to improve CNN performance through data augmentation, batch normalization, dropout, and learning rate schedulingâ€”essential tools for training practical deep learning models.

## References

1. **Data Augmentation Strategies**
   - [Albumentations Library Documentation](https://albumentations.ai/)
   - Comprehensive guide to image augmentation techniques with implementations and best practices for computer vision.

2. **Batch Normalization: How It Works**
   - [Batch Normalization Paper](https://arxiv.org/abs/1502.03167)
   - Original paper introducing batch normalization, explaining how it stabilizes training and enables higher learning rates.

3. **Dropout: A Simple Way to Prevent Overfitting**
   - [Dropout Paper](https://jmlr.org/papers/v15/srivastava14a.html)
   - Seminal paper introducing dropout as a regularization technique to prevent overfitting in neural networks.

4. **Learning Rate Scheduling and Optimization**
   - [PyTorch Learning Rate Scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)
   - Official PyTorch documentation on learning rate schedulers with examples of StepLR, ReduceLROnPlateau, and CosineAnnealingLR.

5. **Model Checkpointing and Early Stopping**
   - [Keras: Early Stopping Documentation](https://keras.io/api/callbacks/early_stopping/)
   - Best practices for saving the best model and stopping training early to avoid overfitting.

---

**Tip:** Start with reference #2 to understand batch normalization conceptually, then #3 for dropout, and #1 for practical augmentation recipes.
