{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 19: Character-Level Models and Text Generation\n",
    "\n",
    "**Goal:** Build character-level language models for text generation using sampling strategies.\n",
    "\n",
    "**Time estimate:** 3-4 hours\n",
    "\n",
    "**What you'll learn:**\n",
    "- Character-level vs word-level modeling\n",
    "- Autoregressive language modeling\n",
    "- Temperature sampling and top-k sampling\n",
    "- Beam search for generation\n",
    "- Perplexity evaluation metric\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Language Modeling Fundamentals\n",
    "\n",
    "### What is a Language Model?\n",
    "\n",
    "A language model assigns probabilities to sequences of tokens:\n",
    "\n",
    "$$P(\\text{\"The cat sat\"}) = P(\\text{The}) \\times P(\\text{cat}|\\text{The}) \\times P(\\text{sat}|\\text{The cat})$$\n",
    "\n",
    "**Autoregressive property:**\n",
    "- Predict next token given all previous tokens\n",
    "- $P(\\text{token}_i | \\text{token}_0, ..., \\text{token}_{i-1})$\n",
    "\n",
    "### Character-Level vs Word-Level\n",
    "\n",
    "#### Character-Level Modeling\n",
    "- **Vocabulary size:** ~100 (for English: 26 letters + punctuation + digits)\n",
    "- **Advantages:**\n",
    "  - Handles any word (no out-of-vocabulary)\n",
    "  - Can learn spelling, morphology\n",
    "  - Natural for code, structured text\n",
    "- **Disadvantages:**\n",
    "  - Longer sequences (more computation)\n",
    "  - Harder to learn long-range dependencies\n",
    "  - Slower generation (character by character)\n",
    "\n",
    "#### Word-Level Modeling\n",
    "- **Vocabulary size:** 10K-100K+ (or unlimited with subword tokenization)\n",
    "- **Advantages:**\n",
    "  - Shorter sequences\n",
    "  - Captures semantic relationships\n",
    "  - Faster to train and generate\n",
    "- **Disadvantages:**\n",
    "  - Out-of-vocabulary problem\n",
    "  - Requires tokenization preprocessing\n",
    "\n",
    "**Modern approach:** Use subword tokenization (BPE, WordPiece) - best of both worlds.\n",
    "\n",
    "Today we focus on **character-level** for simplicity and full understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Character Vocabulary and Preprocessing\n",
    "\n",
    "### Step 1: Create Character Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small Shakespeare text file for training\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filename = \"shakespeare.txt\"\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"✓ Downloaded {filename}\")\n",
    "except:\n",
    "    # Fallback: Create a small sample if download fails\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"The cat sat on the mat.\\n\" * 100)\n",
    "    print(f\"✓ Created sample {filename}\")\n",
    "\n",
    "# Read the text\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Text length: {len(text):,} characters\")\n",
    "print(f\"First 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create mappings\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"\\nCharacters in vocabulary:\")\n",
    "print(''.join(chars))\n",
    "print(f\"\\nExample mappings:\")\n",
    "for char in \"The\":\n",
    "    print(f\"  '{char}' → {char_to_idx[char]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to character indices\n",
    "data = torch.tensor([char_to_idx[char] for char in text], dtype=torch.long)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(f\"Sample: {data[:20]}\")\n",
    "print(f\"Decoded: {''.join([idx_to_char[idx.item()] for idx in data[:50]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for language modeling\n",
    "class CharacterDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Character-level language modeling dataset.\n",
    "    For each sequence, the target is the next token.\n",
    "    \n",
    "    Example:\n",
    "    Input:  \"Hello\" → [H, e, l, l, o]\n",
    "    Target:           [e, l, l, o, space]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Number of sequences we can create\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of length seq_len\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        # Target is the next character\n",
    "        y = self.data[idx + self.seq_len]\n",
    "        return x, y\n",
    "\n",
    "# Parameters\n",
    "seq_len = 64  # Context length\n",
    "batch_size = 32\n",
    "\n",
    "# Create train/val split\n",
    "train_size = int(0.9 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "train_dataset = CharacterDataset(train_data, seq_len)\n",
    "val_dataset = CharacterDataset(val_data, seq_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Example batch\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"\\nExample batch:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Target shape: {y.shape}\")\n",
    "print(f\"  First sequence: {''.join([idx_to_char[idx.item()] for idx in x[0]])}\")\n",
    "print(f\"  First target: '{idx_to_char[y[0].item()]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Character-Level RNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLevelRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level RNN language model.\n",
    "    \n",
    "    Architecture:\n",
    "    Input (batch_size, seq_len)\n",
    "      ↓ Embedding\n",
    "    (batch_size, seq_len, embed_dim)\n",
    "      ↓ LSTM\n",
    "    (batch_size, seq_len, hidden_dim)\n",
    "      ↓ Linear\n",
    "    (batch_size, seq_len, vocab_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Unnormalized probabilities, shape (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = self.fc(lstm_out)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return logits\n\n",
    "# Create model\n",
    "embed_dim = 64\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "model = CharLevelRNN(vocab_size, embed_dim, hidden_dim, num_layers, dropout=0.2).to(device)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training\n",
    "\n",
    "### Loss Function: Cross-Entropy\n",
    "\n",
    "We want to maximize the probability of the next character:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_i \\log P(\\text{next}_i | \\text{context}_i)$$\n",
    "\n",
    "This is the **cross-entropy loss** between predicted distribution and true next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(x)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        # Reshape for cross-entropy: (batch_size * seq_len, vocab_size)\n",
    "        loss = criterion(logits.reshape(-1, model.vocab_size), y.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (important for RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate on validation set.\n",
    "    \n",
    "    Returns:\n",
    "        loss: Cross-entropy loss\n",
    "        perplexity: exp(loss) - measure of model uncertainty\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.reshape(-1, model.vocab_size), y.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "perplexities = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Epoch':<8} {'Train Loss':<15} {'Val Loss':<15} {'Perplexity':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, perplexity = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"{epoch+1:<8} {train_loss:<15.4f} {val_loss:<15.4f} {perplexity:<15.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, label='Train Loss', linewidth=2, marker='o')\n",
    "ax.plot(val_losses, label='Val Loss', linewidth=2, marker='s')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training Progress', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "ax = axes[1]\n",
    "ax.plot(perplexities, label='Validation Perplexity', linewidth=2, marker='o', color='orange')\n",
    "ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Perplexity', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Language Model Perplexity', fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final validation perplexity: {perplexities[-1]:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Perplexity measures 'confusion' of the model\")\n",
    "print(f\"- Lower is better (best possible: 1.0)\")\n",
    "print(f\"- Uniform distribution over {vocab_size} chars: perplexity = {vocab_size:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Text Generation Strategies\n",
    "\n",
    "### Greedy Decoding (Baseline)\n",
    "\n",
    "Always pick the most likely next token:\n",
    "$$\\text{next} = \\arg\\max_i P(\\text{token}_i | \\text{context})$$\n",
    "\n",
    "**Problem:** Generates repetitive, boring text\n",
    "\n",
    "### Temperature Sampling\n",
    "\n",
    "Control diversity by modifying probability distribution:\n",
    "$$P'(\\text{token}_i) = \\frac{\\exp(\\text{logit}_i / T)}{\\sum_j \\exp(\\text{logit}_j / T)}$$\n",
    "\n",
    "Where:\n",
    "- $T = 1.0$: Original distribution (no change)\n",
    "- $T > 1.0$: Softer distribution (more diversity, more errors)\n",
    "- $T < 1.0$: Sharper distribution (less diversity, more confident)\n",
    "\n",
    "### Top-K Sampling\n",
    "\n",
    "Only sample from top-k most likely tokens, normalize:\n",
    "1. Sort probabilities in descending order\n",
    "2. Keep only top-k\n",
    "3. Renormalize remaining probabilities\n",
    "4. Sample from this distribution\n",
    "\n",
    "### Nucleus (Top-P) Sampling\n",
    "\n",
    "Keep smallest set of tokens with cumulative probability ≥ p:\n",
    "1. Sort by probability descending\n",
    "2. Keep tokens until cumulative sum ≥ p\n",
    "3. Sample from this nucleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_token(logits, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Sample next token from logits using various strategies.\n",
    "    \n",
    "    Args:\n",
    "        logits: Unnormalized scores, shape (vocab_size,)\n",
    "        temperature: Sampling temperature (1.0 = no change)\n",
    "        top_k: Keep only top-k tokens (None = use all)\n",
    "        top_p: Keep tokens with cumulative prob >= top_p (None = use all)\n",
    "    \n",
    "    Returns:\n",
    "        token_idx: Sampled token index\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    logits = logits / temperature\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Apply top-k\n",
    "    if top_k is not None:\n",
    "        # Set all but top-k to very negative value\n",
    "        topk_probs, topk_indices = torch.topk(probabilities, top_k)\n",
    "        # Create mask and apply\n",
    "        probabilities_masked = torch.zeros_like(probabilities)\n",
    "        probabilities_masked[topk_indices] = topk_probs\n",
    "        probabilities = probabilities_masked / probabilities_masked.sum()\n",
    "    \n",
    "    # Apply nucleus (top-p)\n",
    "    if top_p is not None:\n",
    "        sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        # Mask out tokens that exceed cumulative probability\n",
    "        sorted_indices_to_remove = cumsum_probs > top_p\n",
    "        sorted_indices_to_remove[0] = False  # Keep at least one token\n",
    "        sorted_probs[sorted_indices_to_remove] = 0\n",
    "        # Renormalize\n",
    "        probabilities = torch.zeros_like(probabilities)\n",
    "        probabilities[sorted_indices] = sorted_probs\n",
    "        probabilities = probabilities / probabilities.sum()\n",
    "    \n",
    "    # Sample\n",
    "    token_idx = torch.multinomial(probabilities, num_samples=1)\n",
    "    return token_idx.item()\n\n",
    "\n",
    "print(\"✓ Sampling functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, max_length, char_to_idx, idx_to_char, \n",
    "                  device, temperature=1.0, top_k=None, top_p=None):\n",
    "    \"\"\"\n",
    "    Generate text starting from start_text.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model\n",
    "        start_text: Starting text (string)\n",
    "        max_length: Maximum length to generate\n",
    "        char_to_idx, idx_to_char: Character mappings\n",
    "        device: Device to run on\n",
    "        temperature: Sampling temperature\n",
    "        top_k: Top-k sampling parameter\n",
    "        top_p: Nucleus sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        generated_text: Generated text string\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize with start text\n",
    "    current_text = start_text\n",
    "    context = torch.tensor([char_to_idx[c] for c in start_text], dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Keep only last seq_len tokens as context\n",
    "            context_trimmed = context[-seq_len:].unsqueeze(0)  # Add batch dimension\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(context_trimmed)  # (1, ≤seq_len, vocab_size)\n",
    "            \n",
    "            # Get logits for next token (last position)\n",
    "            next_logits = logits[0, -1, :]  # (vocab_size,)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_idx = sample_next_token(next_logits, temperature=temperature, \n",
    "                                        top_k=top_k, top_p=top_p)\n",
    "            \n",
    "            # Append to context and generated text\n",
    "            context = torch.cat([context, torch.tensor([next_idx]).to(device)])\n",
    "            current_text += idx_to_char[next_idx]\n",
    "    \n",
    "    return current_text\n\n",
    "\n",
    "print(\"✓ Generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with Different Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different temperature values\n",
    "start_text = \"The\"\n",
    "max_gen_length = 200\n",
    "\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"Text Generation with Different Temperatures\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(model, start_text, max_gen_length, \n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             temperature=temp)\n",
    "    print(f\"\\nTemperature = {temp} (Lower = more focused):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated[:300])  # Print first 300 chars\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with top-k sampling\n",
    "print(\"\\nText Generation with Top-K Sampling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for k in [5, 10, 20]:\n",
    "    generated = generate_text(model, start_text, max_gen_length,\n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             temperature=1.0, top_k=k)\n",
    "    print(f\"\\nTop-K = {k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with nucleus (top-p) sampling\n",
    "print(\"\\nText Generation with Nucleus (Top-P) Sampling\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for p in [0.5, 0.8, 0.95]:\n",
    "    generated = generate_text(model, start_text, max_gen_length,\n",
    "                             char_to_idx, idx_to_char, device,\n",
    "                             temperature=1.0, top_p=p)\n",
    "    print(f\"\\nTop-P = {p}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(generated[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analysis - Sampling Strategy Effects\n",
    "\n",
    "### Compare Diversity and Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generation_statistics(text, vocab_size):\n",
    "    \"\"\"\n",
    "    Analyze diversity and repetition in generated text.\n",
    "    \"\"\"\n",
    "    unique_chars = len(set(text))\n",
    "    char_counts = Counter(text)\n",
    "    entropy = -sum((count/len(text)) * np.log(count/len(text)) \n",
    "                   for count in char_counts.values())\n",
    "    \n",
    "    # Repetition: ratio of unique bigrams to total bigrams\n",
    "    bigrams = [text[i:i+2] for i in range(len(text)-1)]\n",
    "    unique_bigrams = len(set(bigrams))\n",
    "    bigram_diversity = unique_bigrams / len(bigrams) if bigrams else 0\n",
    "    \n",
    "    return {\n",
    "        'unique_chars': unique_chars,\n",
    "        'entropy': entropy,\n",
    "        'bigram_diversity': bigram_diversity,\n",
    "    }\n\n",
    "\n",
    "# Compare strategies\n",
    "strategies = [\n",
    "    ('Temperature=0.5', {'temperature': 0.5}),\n",
    "    ('Temperature=1.0', {'temperature': 1.0}),\n",
    "    ('Temperature=1.5', {'temperature': 1.5}),\n",
    "    ('Top-K=10', {'top_k': 10}),\n",
    "    ('Top-P=0.9', {'top_p': 0.9}),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, kwargs in strategies:\n",
    "    # Generate multiple samples\n",
    "    samples = [generate_text(model, start_text, 500, \n",
    "                            char_to_idx, idx_to_char, device, **kwargs)\n",
    "              for _ in range(3)]\n",
    "    combined = ''.join(samples)\n",
    "    stats = analyze_generation_statistics(combined, vocab_size)\n",
    "    results.append({'Strategy': name, **stats})\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nGeneration Statistics Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Entropy: Higher = more diverse (useful for text)\")\n",
    "print(\"- Bigram diversity: Higher = less repetition\")\n",
    "print(\"- Temperature controls entropy-quality tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Beam Search (Advanced Generation)\n",
    "\n",
    "Instead of greedy or random sampling, maintain multiple hypotheses:\n",
    "\n",
    "1. Keep top-k most likely sequences\n",
    "2. At each step, expand all k sequences\n",
    "3. Keep top-k sequences by cumulative probability\n",
    "4. Return best sequence\n",
    "\n",
    "**Trade-off:** More computation, but better quality\n",
    "\n",
    "Common in machine translation, but less common for character-level models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, start_text, max_length, char_to_idx, idx_to_char,\n",
    "               device, beam_width=5):\n",
    "    \"\"\"\n",
    "    Generate text using beam search.\n",
    "    \n",
    "    Args:\n",
    "        beam_width: Number of hypotheses to keep\n",
    "    \n",
    "    Returns:\n",
    "        best_text: Highest probability sequence found\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize beams: list of (text, context, log_prob)\n",
    "    beams = [(start_text, torch.tensor([char_to_idx[c] for c in start_text], \n",
    "                                       dtype=torch.long).to(device), 0.0)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            new_beams = []\n",
    "            \n",
    "            for text, context, log_prob in beams:\n",
    "                # Get next token distribution\n",
    "                context_trimmed = context[-seq_len:].unsqueeze(0)\n",
    "                logits = model(context_trimmed)\n",
    "                next_logits = logits[0, -1, :]\n",
    "                log_probs = torch.log_softmax(next_logits, dim=-1)\n",
    "                \n",
    "                # Get top-k next tokens\n",
    "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_width)\n",
    "                \n",
    "                for k in range(beam_width):\n",
    "                    next_idx = topk_indices[k].item()\n",
    "                    next_log_prob = topk_log_probs[k].item()\n",
    "                    new_context = torch.cat([context, torch.tensor([next_idx]).to(device)])\n",
    "                    new_text = text + idx_to_char[next_idx]\n",
    "                    new_log_prob = log_prob + next_log_prob\n",
    "                    new_beams.append((new_text, new_context, new_log_prob))\n",
    "            \n",
    "            # Keep top beam_width sequences\n",
    "            new_beams.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = new_beams[:beam_width]\n",
    "    \n",
    "    return beams[0][0]\n\n",
    "\n",
    "# Test beam search\n",
    "print(\"Beam Search Results:\")\n",
    "print(\"=\" * 80)\n",
    "for beam_width in [1, 5, 10]:\n",
    "    text = beam_search(model, start_text, 200, char_to_idx, idx_to_char, \n",
    "                      device, beam_width=beam_width)\n",
    "    print(f\"\\nBeam Width = {beam_width}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(text[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Considerations\n",
    "\n",
    "### Sequence Length Effects\n",
    "\n",
    "- **Too short (e.g., 4):** Model forgets context quickly\n",
    "- **Just right (e.g., 64):** Good balance\n",
    "- **Too long (e.g., 512+):** Slower training, diminishing returns\n",
    "\n",
    "### Character-Level Strengths\n",
    "✅ Handles any text (code, special characters, Unicode)  \n",
    "✅ No OOV problem  \n",
    "✅ Can learn orthography  \n",
    "\n",
    "### Character-Level Weaknesses\n",
    "❌ Long sequences (more computation)  \n",
    "❌ Harder to learn semantic relationships  \n",
    "❌ Slower generation (character by character)  \n",
    "\n",
    "### Modern Alternative: Subword Tokenization\n",
    "\n",
    "Best of both worlds:\n",
    "- Vocabulary: 5K-50K tokens (not 100 or 100K)\n",
    "- Handles OOV automatically\n",
    "- Shorter sequences\n",
    "- Used in all modern LLMs (BPE, WordPiece, SentencePiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "✅ **Language modeling:** Predicting next token given context  \n",
    "✅ **Character-level models:** When and why to use them  \n",
    "✅ **RNN architecture:** Embedding + LSTM + Linear layer  \n",
    "✅ **Perplexity metric:** Measuring model uncertainty  \n",
    "✅ **Sampling strategies:** Temperature, top-k, nucleus sampling  \n",
    "✅ **Beam search:** Finding better sequences  \n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "**Temperature:**\n",
    "- Controls diversity vs. quality\n",
    "- T=0.5: Focused, repetitive\n",
    "- T=1.0: Balanced\n",
    "- T=1.5: Diverse, random\n",
    "\n",
    "**Perplexity:**\n",
    "- Measures \"confusion\" of model\n",
    "- Lower is better\n",
    "- Uniform over n: perplexity = n\n",
    "\n",
    "**Trade-offs:**\n",
    "- Greedy: Fast, deterministic, boring\n",
    "- Sampling: Diverse, stochastic\n",
    "- Beam search: Higher quality, slower\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Text generation:** Creative writing, dialogue, code\n",
    "- **Code completion:** GitHub Copilot uses GPT-2 style models\n",
    "- **Machine translation:** Transformers with beam search\n",
    "- **Question answering:** RAG systems\n",
    "- **Chat:** ChatGPT, Claude use similar decoding\n",
    "\n",
    "### Coming Up (Day 20)\n",
    "\n",
    "**Project 2:** Implement full character-level RNN text generation project with your own dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Exercises\n",
    "\n",
    "### Basic\n",
    "1. **Experiment with context length:** Try seq_len = 32, 128, 256. How does it affect quality?\n",
    "2. **Different datasets:** Train on different text (Shakespeare, Wikipedia, code). How do results change?\n",
    "3. **Analyze attention:** Which characters does the model learn to group together?\n",
    "\n",
    "### Intermediate\n",
    "1. **Implement word-level model:** Change to word-level tokenization\n",
    "2. **Conditional generation:** Generate in specific style (e.g., \"Romeo and Juliet\" style)\n",
    "3. **Length control:** How to generate exactly N characters?\n",
    "\n",
    "### Advanced\n",
    "1. **Implement attention-based RNN:** Add attention to see what model focuses on\n",
    "2. **Mixture of experts:** Have multiple models generate, blend predictions\n",
    "3. **Fast sampling:** Implement sampling with top-p using bisection search\n",
    "4. **Evaluate text quality:** Use BLEU, ROUGE, or BERTScore on generated text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_name_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
