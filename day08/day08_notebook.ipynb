{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8: Hyperparameter Tuning and Experimentation\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "**Mathematical Prerequisites:**\n",
    "- Optimization theory (gradient descent, convergence)\n",
    "- Statistics (sampling, distributions)\n",
    "- Probability (Bayesian inference for advanced methods)\n",
    "- Understanding of search spaces and complexity\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Hyperparameters can make or break your model. Today we explore systematic approaches to hyperparameter optimization:\n",
    "1. Learning rate finder (critical first step)\n",
    "2. Grid search vs random search\n",
    "3. Bayesian optimization (advanced)\n",
    "4. Experiment tracking and management\n",
    "5. Visualization of hyperparameter effects\n",
    "6. Budget allocation strategies\n",
    "\n",
    "**Goal:** Build a systematic framework for finding optimal hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Theory - The Hyperparameter Optimization Problem\n",
    "\n",
    "### 1.1 Hyperparameters vs Parameters\n",
    "\n",
    "**Parameters ($\\theta$):** Learned from data via gradient descent\n",
    "- Weights, biases\n",
    "- Updated during training\n",
    "\n",
    "**Hyperparameters ($\\lambda$):** Set before training\n",
    "- Learning rate, batch size, architecture choices\n",
    "- Not directly optimized via gradient descent\n",
    "- Require expensive outer loop optimization\n",
    "\n",
    "### 1.2 The Optimization Problem\n",
    "\n",
    "**Inner loop (training):**\n",
    "$$\n",
    "\\theta^*(\\lambda) = \\arg\\min_{\\theta} L_{\\text{train}}(\\theta; \\lambda)\n",
    "$$\n",
    "\n",
    "**Outer loop (hyperparameter tuning):**\n",
    "$$\n",
    "\\lambda^* = \\arg\\min_{\\lambda} L_{\\text{val}}(\\theta^*(\\lambda))\n",
    "$$\n",
    "\n",
    "**Challenge:** \n",
    "- Each evaluation of $\\theta^*(\\lambda)$ requires full training run\n",
    "- High-dimensional search space\n",
    "- No gradient information for $\\lambda$\n",
    "\n",
    "### 1.3 Common Hyperparameters and Their Scales\n",
    "\n",
    "| Hyperparameter | Type | Search Scale | Typical Range |\n",
    "|----------------|------|--------------|---------------|\n",
    "| Learning rate | Continuous | Log | [1e-5, 1e-1] |\n",
    "| Batch size | Discrete | Linear/Log | [16, 512] |\n",
    "| Weight decay | Continuous | Log | [1e-6, 1e-2] |\n",
    "| Dropout rate | Continuous | Linear | [0.0, 0.5] |\n",
    "| Number of layers | Discrete | Linear | [2, 10] |\n",
    "| Hidden units | Discrete | Log | [32, 1024] |\n",
    "\n",
    "**Key Insight:** Learning rate, weight decay, and other scaling parameters should be searched on **log scale** because their effect is multiplicative.\n",
    "\n",
    "### 1.4 Search Strategies Comparison\n",
    "\n",
    "**Grid Search:**\n",
    "- Try all combinations on a grid\n",
    "- Complexity: $O(k^d)$ where $k$ = grid points per dimension, $d$ = dimensions\n",
    "- Good: Exhaustive, reproducible\n",
    "- Bad: Exponential in dimensions, wastes computation\n",
    "\n",
    "**Random Search:**\n",
    "- Sample hyperparameters randomly\n",
    "- Complexity: $O(n)$ where $n$ = number of trials\n",
    "- Good: More efficient than grid for high dimensions\n",
    "- Bad: May miss optimal region\n",
    "\n",
    "**Bayesian Optimization:**\n",
    "- Build probabilistic model of objective function\n",
    "- Use acquisition function to select next point\n",
    "- Good: Sample-efficient, intelligent exploration\n",
    "- Bad: Overhead for simple problems\n",
    "\n",
    "**Important Result (Bergstra & Bengio, 2012):**\n",
    "Random search is more efficient than grid search when only a few hyperparameters truly matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# For Bayesian optimization\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    SKOPT_AVAILABLE = True\n",
    "except:\n",
    "    SKOPT_AVAILABLE = False\n",
    "    print(\"scikit-optimize not available. Bayesian optimization will be skipped.\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create experiments directory\n",
    "experiments_dir = Path('./experiments')\n",
    "experiments_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset and Model Setup\n",
    "\n",
    "We'll use CIFAR-10 for hyperparameter tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Full datasets\n",
    "train_dataset_full = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Create train/val split\n",
    "train_size = int(0.8 * len(train_dataset_full))\n",
    "val_size = len(train_dataset_full) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset_full, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Simple CNN for Fast Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for hyperparameter tuning experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=64, dropout_rate=0.5, num_layers=2):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Flexible FC layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        input_size = 64 * 8 * 8\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.fc_layers.append(nn.Linear(input_size, hidden_size))\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        self.fc_final = nn.Linear(input_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for fc in self.fc_layers:\n",
    "            x = torch.relu(fc(x))\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning Rate Finder\n",
    "\n",
    "### 3.1 Theory: Learning Rate Range Test\n",
    "\n",
    "**Algorithm (Smith, 2017):**\n",
    "1. Start with very small learning rate (e.g., 1e-7)\n",
    "2. Exponentially increase LR after each batch\n",
    "3. Record loss at each LR\n",
    "4. Stop when loss explodes\n",
    "\n",
    "**Optimal LR Selection:**\n",
    "- **Method 1:** LR where loss decreases fastest (steepest gradient)\n",
    "- **Method 2:** One order of magnitude before minimum loss\n",
    "- **Method 3:** Middle of steepest decline region\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "At iteration $i$:\n",
    "$$\n",
    "\\text{LR}_i = \\text{LR}_{\\min} \\cdot \\left(\\frac{\\text{LR}_{\\max}}{\\text{LR}_{\\min}}\\right)^{i/n}\n",
    "$$\n",
    "where $n$ is total iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    \"\"\"Learning Rate Range Test.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        \n",
    "        # Save initial state\n",
    "        self.model_state = model.state_dict()\n",
    "        self.optimizer_state = optimizer.state_dict()\n",
    "    \n",
    "    def range_test(self, train_loader, start_lr=1e-7, end_lr=10, num_iter=100, \n",
    "                   smooth_f=0.05, diverge_th=5):\n",
    "        \"\"\"\n",
    "        Perform learning rate range test.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: Training data loader\n",
    "            start_lr: Starting learning rate\n",
    "            end_lr: Ending learning rate\n",
    "            num_iter: Number of iterations\n",
    "            smooth_f: Smoothing factor for loss\n",
    "            diverge_th: Threshold for divergence detection\n",
    "        \"\"\"\n",
    "        # Setup\n",
    "        self.model.train()\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Calculate LR multiplier\n",
    "        lr_mult = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        lr = start_lr\n",
    "        \n",
    "        # Update learning rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        # Iterate\n",
    "        iterator = iter(train_loader)\n",
    "        smoothed_loss = 0\n",
    "        \n",
    "        for iteration in tqdm(range(num_iter), desc='LR Finder'):\n",
    "            # Get batch\n",
    "            try:\n",
    "                inputs, labels = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(train_loader)\n",
    "                inputs, labels = next(iterator)\n",
    "            \n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Smooth loss\n",
    "            if iteration == 0:\n",
    "                smoothed_loss = loss.item()\n",
    "            else:\n",
    "                smoothed_loss = smooth_f * loss.item() + (1 - smooth_f) * smoothed_loss\n",
    "            \n",
    "            # Record\n",
    "            lrs.append(lr)\n",
    "            losses.append(smoothed_loss)\n",
    "            \n",
    "            # Check for divergence\n",
    "            if smoothed_loss > diverge_th * best_loss or torch.isnan(loss):\n",
    "                print(f\"\\nStopping early at iteration {iteration}. Loss diverged.\")\n",
    "                break\n",
    "            \n",
    "            if smoothed_loss < best_loss:\n",
    "                best_loss = smoothed_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update learning rate\n",
    "            lr *= lr_mult\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        \n",
    "        # Restore initial state\n",
    "        self.model.load_state_dict(self.model_state)\n",
    "        self.optimizer.load_state_dict(self.optimizer_state)\n",
    "        \n",
    "        return lrs, losses\n",
    "    \n",
    "    def plot(self, lrs, losses, skip_start=10, skip_end=5):\n",
    "        \"\"\"Plot learning rate vs loss.\"\"\"\n",
    "        if skip_start >= len(lrs):\n",
    "            skip_start = 0\n",
    "        if skip_end >= len(lrs):\n",
    "            skip_end = 0\n",
    "        \n",
    "        lrs = lrs[skip_start:-skip_end] if skip_end > 0 else lrs[skip_start:]\n",
    "        losses = losses[skip_start:-skip_end] if skip_end > 0 else losses[skip_start:]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Log scale plot\n",
    "        ax1.plot(lrs, losses, linewidth=2)\n",
    "        ax1.set_xscale('log')\n",
    "        ax1.set_xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "        ax1.set_ylabel('Loss', fontsize=12)\n",
    "        ax1.set_title('Learning Rate Finder', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Find steepest gradient\n",
    "        gradients = np.gradient(losses)\n",
    "        min_gradient_idx = np.argmin(gradients)\n",
    "        suggested_lr = lrs[min_gradient_idx]\n",
    "        \n",
    "        ax1.axvline(x=suggested_lr, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "        ax1.legend(fontsize=11)\n",
    "        \n",
    "        # Gradient plot\n",
    "        ax2.plot(lrs, gradients, linewidth=2, color='green')\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "        ax2.set_ylabel('Loss Gradient', fontsize=12)\n",
    "        ax2.set_title('Loss Gradient vs Learning Rate', fontsize=14, fontweight='bold')\n",
    "        ax2.axvline(x=suggested_lr, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Min gradient at LR: {suggested_lr:.2e}')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nSuggested learning rate: {suggested_lr:.2e}\")\n",
    "        print(f\"Consider using: {suggested_lr/10:.2e} to {suggested_lr:.2e}\")\n",
    "        \n",
    "        return suggested_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "lr_finder_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = SimpleCNN(hidden_size=64, dropout_rate=0.5, num_layers=2).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-7, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Run LR finder\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "lrs, losses = lr_finder.range_test(lr_finder_loader, start_lr=1e-6, end_lr=1, num_iter=100)\n",
    "suggested_lr = lr_finder.plot(lrs, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Grid Search\n",
    "\n",
    "### 4.1 Define Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search hyperparameters\n",
    "grid_search_space = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'weight_decay': [0.0, 1e-4, 1e-3],\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = np.prod([len(v) for v in grid_search_space.values()])\n",
    "print(f\"Grid Search: {total_combinations} total combinations\")\n",
    "print(f\"Search space: {grid_search_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training Function with Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(hyperparams, train_loader, val_loader, num_epochs=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Train model with given hyperparameters and return validation performance.\n",
    "    \n",
    "    Returns:\n",
    "        dict with results including val_acc, train_time, etc.\n",
    "    \"\"\"\n",
    "    # Unpack hyperparameters\n",
    "    lr = hyperparams.get('learning_rate', 0.01)\n",
    "    weight_decay = hyperparams.get('weight_decay', 0.0)\n",
    "    dropout_rate = hyperparams.get('dropout_rate', 0.5)\n",
    "    hidden_size = hyperparams.get('hidden_size', 64)\n",
    "    num_layers = hyperparams.get('num_layers', 2)\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleCNN(\n",
    "        hidden_size=hidden_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = correct / total\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss={epoch_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'hyperparams': hyperparams,\n",
    "        'val_acc': max(val_accs),\n",
    "        'final_val_acc': val_accs[-1],\n",
    "        'train_losses': train_losses,\n",
    "        'val_accs': val_accs,\n",
    "        'train_time': train_time,\n",
    "        'num_epochs': num_epochs\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(search_space, train_dataset, val_dataset, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Perform grid search over hyperparameter space.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate all combinations\n",
    "    keys = list(search_space.keys())\n",
    "    values = list(search_space.values())\n",
    "    combinations = list(product(*values))\n",
    "    \n",
    "    print(f\"\\nRunning Grid Search: {len(combinations)} combinations\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, combo in enumerate(combinations):\n",
    "        hyperparams = dict(zip(keys, combo))\n",
    "        \n",
    "        print(f\"\\nExperiment {idx+1}/{len(combinations)}\")\n",
    "        print(f\"Hyperparameters: {hyperparams}\")\n",
    "        \n",
    "        # Create data loaders with specified batch size\n",
    "        batch_size = hyperparams.get('batch_size', 64)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        result = train_and_evaluate(hyperparams, train_loader, val_loader, num_epochs=num_epochs)\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"Validation Accuracy: {result['val_acc']:.4f}\")\n",
    "        print(f\"Training Time: {result['train_time']:.2f}s\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Grid Search Complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run grid search (reduced space for demo)\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_results = grid_search(grid_search_space, train_dataset, val_dataset, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Analyze Grid Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "grid_df = pd.DataFrame([{\n",
    "    **r['hyperparams'],\n",
    "    'val_acc': r['val_acc'],\n",
    "    'train_time': r['train_time']\n",
    "} for r in grid_results])\n",
    "\n",
    "# Sort by validation accuracy\n",
    "grid_df_sorted = grid_df.sort_values('val_acc', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Configurations:\")\n",
    "print(\"=\"*100)\n",
    "print(grid_df_sorted.head().to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Learning rate effect\n",
    "lr_groups = grid_df.groupby('learning_rate')['val_acc'].agg(['mean', 'std'])\n",
    "axes[0].bar(range(len(lr_groups)), lr_groups['mean'], yerr=lr_groups['std'],\n",
    "           capsize=5, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xticks(range(len(lr_groups)))\n",
    "axes[0].set_xticklabels([f\"{lr:.3f}\" for lr in lr_groups.index])\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=12)\n",
    "axes[0].set_ylabel('Mean Validation Accuracy', fontsize=12)\n",
    "axes[0].set_title('Effect of Learning Rate', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Batch size effect\n",
    "bs_groups = grid_df.groupby('batch_size')['val_acc'].agg(['mean', 'std'])\n",
    "axes[1].bar(range(len(bs_groups)), bs_groups['mean'], yerr=bs_groups['std'],\n",
    "           capsize=5, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1].set_xticks(range(len(bs_groups)))\n",
    "axes[1].set_xticklabels([f\"{bs}\" for bs in bs_groups.index])\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Mean Validation Accuracy', fontsize=12)\n",
    "axes[1].set_title('Effect of Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Weight decay effect\n",
    "wd_groups = grid_df.groupby('weight_decay')['val_acc'].agg(['mean', 'std'])\n",
    "axes[2].bar(range(len(wd_groups)), wd_groups['mean'], yerr=wd_groups['std'],\n",
    "           capsize=5, alpha=0.7, edgecolor='black', color='green')\n",
    "axes[2].set_xticks(range(len(wd_groups)))\n",
    "axes[2].set_xticklabels([f\"{wd:.1e}\" for wd in wd_groups.index])\n",
    "axes[2].set_xlabel('Weight Decay', fontsize=12)\n",
    "axes[2].set_ylabel('Mean Validation Accuracy', fontsize=12)\n",
    "axes[2].set_title('Effect of Weight Decay', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best configuration\n",
    "best_idx = grid_df_sorted.index[0]\n",
    "best_config = grid_results[best_idx]\n",
    "print(f\"\\nBest Configuration:\")\n",
    "print(f\"Hyperparameters: {best_config['hyperparams']}\")\n",
    "print(f\"Validation Accuracy: {best_config['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Random Search\n",
    "\n",
    "### 5.1 Theory: Why Random Search?\n",
    "\n",
    "**Key Result (Bergstra & Bengio, 2012):**\n",
    "\n",
    "If only a few hyperparameters truly matter, random search explores those dimensions more thoroughly than grid search for the same budget.\n",
    "\n",
    "**Example:** 9 experiments, 2 hyperparameters\n",
    "- Grid: 3×3 grid → Only 3 unique values per dimension\n",
    "- Random: 9 random samples → 9 unique values per dimension\n",
    "\n",
    "**Search Space Definition:**\n",
    "- Continuous: Sample from distribution (uniform, log-uniform)\n",
    "- Discrete: Sample from set\n",
    "- Categorical: Sample from choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_hyperparams(search_space, n_samples=20):\n",
    "    \"\"\"\n",
    "    Sample hyperparameters randomly from search space.\n",
    "    \n",
    "    search_space format:\n",
    "    {\n",
    "        'param_name': ('type', min, max) or ('choice', [options])\n",
    "    }\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        sample = {}\n",
    "        for param, spec in search_space.items():\n",
    "            param_type = spec[0]\n",
    "            \n",
    "            if param_type == 'log_uniform':\n",
    "                # Sample on log scale (for LR, weight decay, etc.)\n",
    "                log_min, log_max = np.log10(spec[1]), np.log10(spec[2])\n",
    "                sample[param] = 10 ** np.random.uniform(log_min, log_max)\n",
    "            \n",
    "            elif param_type == 'uniform':\n",
    "                # Sample uniformly\n",
    "                sample[param] = np.random.uniform(spec[1], spec[2])\n",
    "            \n",
    "            elif param_type == 'int_uniform':\n",
    "                # Sample integer uniformly\n",
    "                sample[param] = np.random.randint(spec[1], spec[2] + 1)\n",
    "            \n",
    "            elif param_type == 'choice':\n",
    "                # Sample from discrete choices\n",
    "                sample[param] = np.random.choice(spec[1])\n",
    "        \n",
    "        samples.append(sample)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Define random search space\n",
    "random_search_space = {\n",
    "    'learning_rate': ('log_uniform', 1e-4, 1e-1),\n",
    "    'batch_size': ('choice', [32, 64, 128, 256]),\n",
    "    'weight_decay': ('log_uniform', 1e-6, 1e-2),\n",
    "    'dropout_rate': ('uniform', 0.0, 0.7),\n",
    "    'hidden_size': ('choice', [32, 64, 128, 256]),\n",
    "}\n",
    "\n",
    "# Sample hyperparameters\n",
    "n_random_samples = 20\n",
    "random_samples = sample_random_hyperparams(random_search_space, n_samples=n_random_samples)\n",
    "\n",
    "print(f\"Random Search: {n_random_samples} samples\")\n",
    "print(\"\\nFirst 5 samples:\")\n",
    "for i, sample in enumerate(random_samples[:5]):\n",
    "    print(f\"{i+1}. {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Run Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(hyperparams_list, train_dataset, val_dataset, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Perform random search.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nRunning Random Search: {len(hyperparams_list)} samples\\n\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, hyperparams in enumerate(hyperparams_list):\n",
    "        print(f\"\\nExperiment {idx+1}/{len(hyperparams_list)}\")\n",
    "        print(f\"Hyperparameters: {hyperparams}\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        batch_size = hyperparams.get('batch_size', 64)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        try:\n",
    "            result = train_and_evaluate(hyperparams, train_loader, val_loader, num_epochs=num_epochs)\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"Validation Accuracy: {result['val_acc']:.4f}\")\n",
    "            print(f\"Training Time: {result['train_time']:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Random Search Complete!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run random search\n",
    "print(\"Starting Random Search...\")\n",
    "random_results = random_search(random_samples, train_dataset, val_dataset, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Compare Grid Search vs Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "random_df = pd.DataFrame([{\n",
    "    **r['hyperparams'],\n",
    "    'val_acc': r['val_acc'],\n",
    "    'train_time': r['train_time'],\n",
    "    'method': 'Random Search'\n",
    "} for r in random_results])\n",
    "\n",
    "grid_df['method'] = 'Grid Search'\n",
    "\n",
    "# Combine\n",
    "combined_df = pd.concat([grid_df, random_df], ignore_index=True)\n",
    "\n",
    "# Compare best results\n",
    "print(\"\\nGrid Search vs Random Search:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Grid Search - Best Acc: {grid_df['val_acc'].max():.4f}, \"\n",
    "      f\"Mean Acc: {grid_df['val_acc'].mean():.4f}, \"\n",
    "      f\"Total Time: {grid_df['train_time'].sum():.2f}s\")\n",
    "print(f\"Random Search - Best Acc: {random_df['val_acc'].max():.4f}, \"\n",
    "      f\"Mean Acc: {random_df['val_acc'].mean():.4f}, \"\n",
    "      f\"Total Time: {random_df['train_time'].sum():.2f}s\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Distribution of accuracies\n",
    "grid_accs = grid_df['val_acc'].values\n",
    "random_accs = random_df['val_acc'].values\n",
    "\n",
    "axes[0].hist(grid_accs, bins=10, alpha=0.7, label='Grid Search', edgecolor='black')\n",
    "axes[0].hist(random_accs, bins=10, alpha=0.7, label='Random Search', edgecolor='black')\n",
    "axes[0].axvline(grid_accs.max(), color='blue', linestyle='--', linewidth=2, \n",
    "               label=f'Grid Best: {grid_accs.max():.4f}')\n",
    "axes[0].axvline(random_accs.max(), color='orange', linestyle='--', linewidth=2,\n",
    "               label=f'Random Best: {random_accs.max():.4f}')\n",
    "axes[0].set_xlabel('Validation Accuracy', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Validation Accuracies', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative best over time\n",
    "grid_cummax = pd.Series(grid_accs).cummax()\n",
    "random_cummax = pd.Series(random_accs).cummax()\n",
    "\n",
    "axes[1].plot(range(1, len(grid_cummax)+1), grid_cummax, 'o-', \n",
    "            linewidth=2, markersize=6, label='Grid Search')\n",
    "axes[1].plot(range(1, len(random_cummax)+1), random_cummax, 's-',\n",
    "            linewidth=2, markersize=6, label='Random Search')\n",
    "axes[1].set_xlabel('Number of Trials', fontsize=12)\n",
    "axes[1].set_ylabel('Best Validation Accuracy So Far', fontsize=12)\n",
    "axes[1].set_title('Cumulative Best Performance', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best from each\n",
    "print(\"\\nBest Configuration from Grid Search:\")\n",
    "best_grid = grid_df.loc[grid_df['val_acc'].idxmax()]\n",
    "print(best_grid.to_string())\n",
    "\n",
    "print(\"\\nBest Configuration from Random Search:\")\n",
    "best_random = random_df.loc[random_df['val_acc'].idxmax()]\n",
    "print(best_random.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualization of Hyperparameter Landscape\n",
    "\n",
    "### 6.1 Pairwise Hyperparameter Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use random search results for visualization (more diverse)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Learning rate vs Batch size\n",
    "scatter = axes[0, 0].scatter(random_df['learning_rate'], random_df['batch_size'],\n",
    "                            c=random_df['val_acc'], s=100, cmap='viridis',\n",
    "                            edgecolors='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Batch Size', fontsize=12)\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_title('Learning Rate vs Batch Size', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0, 0], label='Val Accuracy')\n",
    "\n",
    "# Learning rate vs Weight decay\n",
    "scatter = axes[0, 1].scatter(random_df['learning_rate'], random_df['weight_decay'],\n",
    "                            c=random_df['val_acc'], s=100, cmap='viridis',\n",
    "                            edgecolors='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Weight Decay (log scale)', fontsize=12)\n",
    "axes[0, 1].set_xscale('log')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].set_title('Learning Rate vs Weight Decay', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0, 1], label='Val Accuracy')\n",
    "\n",
    "# Dropout rate vs Hidden size\n",
    "scatter = axes[1, 0].scatter(random_df['dropout_rate'], random_df['hidden_size'],\n",
    "                            c=random_df['val_acc'], s=100, cmap='viridis',\n",
    "                            edgecolors='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Dropout Rate', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Hidden Size', fontsize=12)\n",
    "axes[1, 0].set_title('Dropout Rate vs Hidden Size', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='Val Accuracy')\n",
    "\n",
    "# Parallel coordinates plot\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Normalize columns for parallel coordinates\n",
    "plot_df = random_df[['learning_rate', 'batch_size', 'weight_decay', \n",
    "                     'dropout_rate', 'hidden_size', 'val_acc']].copy()\n",
    "\n",
    "# Add performance category\n",
    "plot_df['performance'] = pd.cut(plot_df['val_acc'], bins=3, \n",
    "                                labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Normalize numeric columns\n",
    "for col in ['learning_rate', 'batch_size', 'weight_decay', 'dropout_rate', 'hidden_size']:\n",
    "    plot_df[col] = (plot_df[col] - plot_df[col].min()) / (plot_df[col].max() - plot_df[col].min())\n",
    "\n",
    "parallel_coordinates(plot_df, 'performance', ax=axes[1, 1], \n",
    "                    colormap='viridis', alpha=0.5)\n",
    "axes[1, 1].set_title('Parallel Coordinates Plot', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Normalized Value', fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Experiment Tracking and Management\n",
    "\n",
    "### 7.1 Create Experiment Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentLogger:\n",
    "    \"\"\"Logger for tracking hyperparameter tuning experiments.\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.log_dir = experiments_dir / experiment_name\n",
    "        self.log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.log_file = self.log_dir / 'experiments.json'\n",
    "        self.experiments = self._load_experiments()\n",
    "    \n",
    "    def _load_experiments(self):\n",
    "        \"\"\"Load existing experiments.\"\"\"\n",
    "        if self.log_file.exists():\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return []\n",
    "    \n",
    "    def log_experiment(self, hyperparams, metrics, metadata=None):\n",
    "        \"\"\"Log a single experiment.\"\"\"\n",
    "        experiment = {\n",
    "            'id': len(self.experiments) + 1,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'hyperparams': hyperparams,\n",
    "            'metrics': metrics,\n",
    "            'metadata': metadata or {}\n",
    "        }\n",
    "        \n",
    "        self.experiments.append(experiment)\n",
    "        self._save_experiments()\n",
    "        \n",
    "        return experiment['id']\n",
    "    \n",
    "    def _save_experiments(self):\n",
    "        \"\"\"Save experiments to file.\"\"\"\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.experiments, f, indent=2)\n",
    "    \n",
    "    def get_best_experiment(self, metric='val_acc', maximize=True):\n",
    "        \"\"\"Get best experiment by metric.\"\"\"\n",
    "        if not self.experiments:\n",
    "            return None\n",
    "        \n",
    "        sorted_exps = sorted(\n",
    "            self.experiments,\n",
    "            key=lambda x: x['metrics'].get(metric, 0),\n",
    "            reverse=maximize\n",
    "        )\n",
    "        \n",
    "        return sorted_exps[0]\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print summary of experiments.\"\"\"\n",
    "        if not self.experiments:\n",
    "            print(\"No experiments logged.\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame([{\n",
    "            'id': exp['id'],\n",
    "            **exp['hyperparams'],\n",
    "            **exp['metrics']\n",
    "        } for exp in self.experiments])\n",
    "        \n",
    "        print(f\"\\nExperiment Summary: {self.experiment_name}\")\n",
    "        print(\"=\"*100)\n",
    "        print(f\"Total experiments: {len(self.experiments)}\")\n",
    "        print(f\"Best validation accuracy: {df['val_acc'].max():.4f}\")\n",
    "        print(f\"Mean validation accuracy: {df['val_acc'].mean():.4f}\")\n",
    "        print(f\"Std validation accuracy: {df['val_acc'].std():.4f}\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Create logger\n",
    "logger = ExperimentLogger('cifar10_tuning')\n",
    "\n",
    "# Log all random search experiments\n",
    "for result in random_results:\n",
    "    logger.log_experiment(\n",
    "        hyperparams=result['hyperparams'],\n",
    "        metrics={\n",
    "            'val_acc': result['val_acc'],\n",
    "            'train_time': result['train_time']\n",
    "        },\n",
    "        metadata={'method': 'random_search'}\n",
    "    )\n",
    "\n",
    "# Print summary\n",
    "summary_df = logger.summary()\n",
    "print(\"\\nTop 5 Experiments:\")\n",
    "print(summary_df.nlargest(5, 'val_acc').to_string(index=False))\n",
    "\n",
    "# Get best experiment\n",
    "best_exp = logger.get_best_experiment()\n",
    "print(f\"\\nBest Experiment ID: {best_exp['id']}\")\n",
    "print(f\"Hyperparameters: {best_exp['hyperparams']}\")\n",
    "print(f\"Validation Accuracy: {best_exp['metrics']['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Best Practices and Strategies\n",
    "\n",
    "### 8.1 Coarse-to-Fine Search\n",
    "\n",
    "**Strategy:**\n",
    "1. **Coarse search:** Wide range, few iterations\n",
    "2. **Identify promising region**\n",
    "3. **Fine search:** Narrow range, more iterations\n",
    "\n",
    "### 8.2 Budget Allocation\n",
    "\n",
    "**Successive Halving:**\n",
    "1. Train N configurations for k epochs\n",
    "2. Keep top N/2, train for 2k epochs\n",
    "3. Repeat until 1 configuration remains\n",
    "\n",
    "**Early Stopping:**\n",
    "- Monitor validation loss\n",
    "- Stop if no improvement for n epochs\n",
    "- Saves time on poor configurations\n",
    "\n",
    "### 8.3 Hyperparameter Importance\n",
    "\n",
    "**Generally most important (in order):**\n",
    "1. **Learning rate** - Most critical\n",
    "2. **Architecture** (depth, width)\n",
    "3. **Batch size**\n",
    "4. **Regularization** (dropout, weight decay)\n",
    "5. **Optimizer parameters** (momentum, betas)\n",
    "\n",
    "**Guideline:** Start with LR finder, then tune architecture, then regularization.\n",
    "\n",
    "### 8.4 Common Pitfalls\n",
    "\n",
    "1. **Not using log scale for LR, weight decay**\n",
    "   - Solution: Always use log scale for multiplicative parameters\n",
    "\n",
    "2. **Overfitting to validation set**\n",
    "   - Solution: Use separate test set, limit tuning iterations\n",
    "\n",
    "3. **Ignoring computational budget**\n",
    "   - Solution: Use early stopping, successive halving\n",
    "\n",
    "4. **Not tracking experiments**\n",
    "   - Solution: Always log hyperparameters and metrics\n",
    "\n",
    "5. **Optimizing too many hyperparameters**\n",
    "   - Solution: Focus on most important ones first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced - Bayesian Optimization (Optional)\n",
    "\n",
    "### 9.1 Theory\n",
    "\n",
    "**Idea:** Build probabilistic model of $f(\\lambda)$ (validation performance)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Train surrogate model (Gaussian Process) on observed $(\\lambda, f(\\lambda))$ pairs\n",
    "2. Use acquisition function to select next $\\lambda$ to try\n",
    "3. Evaluate $f(\\lambda)$, update surrogate\n",
    "4. Repeat\n",
    "\n",
    "**Acquisition Functions:**\n",
    "- **Expected Improvement (EI):** $EI(\\lambda) = \\mathbb{E}[\\max(f(\\lambda) - f(\\lambda^*), 0)]$\n",
    "- **Upper Confidence Bound (UCB):** $UCB(\\lambda) = \\mu(\\lambda) + \\kappa \\sigma(\\lambda)$\n",
    "\n",
    "**Trade-off:** Exploration (high uncertainty) vs Exploitation (high mean)\n",
    "\n",
    "**Note:** This section is optional as it requires additional dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered systematic hyperparameter optimization. You now understand:\n",
    "\n",
    "✅ The hyperparameter optimization problem (inner vs outer loop)  \n",
    "✅ Learning rate finder (critical first step)  \n",
    "✅ Grid search (exhaustive but expensive)  \n",
    "✅ Random search (more efficient for high dimensions)  \n",
    "✅ Hyperparameter landscape visualization  \n",
    "✅ Experiment tracking and management  \n",
    "✅ Best practices (coarse-to-fine, budget allocation)  \n",
    "✅ Bayesian optimization principles  \n",
    "\n",
    "**Key Insights:**\n",
    "1. **Always start with LR finder** - Most critical hyperparameter\n",
    "2. **Random search > Grid search** for high-dimensional spaces\n",
    "3. **Use log scale** for learning rate, weight decay, etc.\n",
    "4. **Track everything** - Reproducibility is crucial\n",
    "5. **Budget wisely** - Use early stopping, successive halving\n",
    "\n",
    "**Typical Workflow:**\n",
    "1. Run LR finder → Get LR range\n",
    "2. Coarse random search → Identify promising region\n",
    "3. Fine random/grid search → Refine best configuration\n",
    "4. (Optional) Bayesian optimization → Final tuning\n",
    "\n",
    "**Time spent:** ~3-4 hours\n",
    "\n",
    "**Next:** Day 9 - Advanced CNN Architectures (ResNet, VGG from papers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
