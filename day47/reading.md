# Day 47: Reinforcement Learning from Human Feedback (RLHF) - Further Reading

This day covers RLHF (Reinforcement Learning from Human Feedback), the technique used to align language models with human preferences and values.

## References

1. **InstructGPT: Training Language Models to Follow Instructions**
   - [Ouyang et al.: InstructGPT](https://arxiv.org/abs/2203.02155)
   - Paper introducing RLHF for aligning GPT models with human preferences through instruction following.

2. **Learning to Summarize with Human Feedback**
   - [Stiennon et al.: RL for Summarization](https://arxiv.org/abs/2009.01325)
   - Early application of RLHF to text summarization showing improvements over supervised learning.

3. **Direct Preference Optimization (DPO)**
   - [Rafailov et al.: DPO](https://arxiv.org/abs/2305.18290)
   - Simpler alternative to RLHF that directly optimizes for human preferences without reward models.

4. **TRL: Transformers Reinforcement Learning**
   - [HuggingFace TRL Library](https://huggingface.co/docs/trl/)
   - Library for implementing RLHF and DPO with pretrained language models.

5. **Constitutional AI for Model Alignment**
   - [Bai et al.: Constitutional AI](https://arxiv.org/abs/2212.08073)
   - Technique for aligning models with a set of principles without extensive human feedback.

---

**Tip:** Start with reference #1 for RLHF concepts, #4 for implementation, #3 for simpler alternatives, and #5 for constitutional AI approaches.
